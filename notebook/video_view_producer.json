{
	"name": "video_view_producer",
	"properties": {
		"folder": {
			"name": "Streaming"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "kafka303",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b9077cdd-cd66-4af9-b23e-42db455fc929"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "scala"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/kafka303",
				"name": "kafka303",
				"type": "Spark",
				"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/kafka303",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import org.apache.kafka.clients.producer._\r\n",
					"import java.sql.Timestamp\r\n",
					"import java.time.Instant\r\n",
					"import java.util.Properties\r\n",
					"\r\n",
					"import scala.util.Random\r\n",
					"import org.apache.kafka.clients.producer._"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"val bootstrapServers = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"confluent-cloud-brokers\")\r\n",
					"val props = new Properties()\r\n",
					"props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\r\n",
					"props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\r\n",
					"\r\n",
					"val kafkaAPIKey = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"confluent-cloud-user\")\r\n",
					"val kafkaAPISecret = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"confluent-cloud-password\")\r\n",
					"props.put(\"bootstrap.servers\", bootstrapServers)\r\n",
					"props.put(\"security.protocol\", \"SASL_SSL\")\r\n",
					"props.put(\"sasl.jaas.config\", s\"org.apache.kafka.common.security.plain.PlainLoginModule  required username='$kafkaAPIKey'   password='$kafkaAPISecret';\")\r\n",
					"props.put(\"sasl.mechanism\", \"PLAIN\")\r\n",
					"// Required for correctness in Apache Kafka clients prior to 2.6\r\n",
					"props.put(\"client.dns.lookup\", \"use_all_dns_ips\")\r\n",
					"// Best practice for Kafka producer to prevent data loss\r\n",
					"props.put(\"acks\", \"all\")\r\n",
					"\r\n",
					"val usageTopic = \"video_usage\"\r\n",
					"val producer = new KafkaProducer[String, String](props)\r\n",
					"\r\n",
					"val randomCompleted = () => { if (Random.nextInt(2) == 1) true else false }\r\n",
					"val randomDuration= () => { Random.nextInt(360) }\r\n",
					"val timestampNow = () => Timestamp.from(Instant.now)\r\n",
					"\r\n",
					"\r\n",
					"// val usageInfo = s\"\"\"{\"usageId\": $usageId, \"user\": \"user${user}\", \"completed\": ${randomCompleted()}, \"durationSeconds\": ${randomDuration()}, \"eventTimestamp\": \"${timestampNow()}\"}\"\"\"\r\n",
					"// val record = new ProducerRecord[String, String](usageTopic, usageId.toString, usageInfo)\r\n",
					"\r\n",
					"// //println(usageInfo)\r\n",
					"// produceRecord(producer, record)\r\n",
					""
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"\r\n",
					"val bootstrapServers = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"confluent-cloud-brokers\")\r\n",
					"val mode: String = \"confluent\"\r\n",
					"\r\n",
					"val props = new Properties()\r\n",
					"props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\r\n",
					"props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\r\n",
					"\r\n",
					"if (mode.toLowerCase == \"local\") {\r\n",
					"    println(\"Running producer in local mode\")\r\n",
					"    props.put(\"bootstrap.servers\", \"localhost:9092\")\r\n",
					"} else {\r\n",
					"    println(\"Running producer in Confluent Cloud mode\")\r\n",
					"    // If using Confluent\r\n",
					"    val kafkaAPIKey = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"confluent-cloud-user\")\r\n",
					"    val kafkaAPISecret = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"confluent-cloud-password\")\r\n",
					"    props.put(\"bootstrap.servers\", bootstrapServers)\r\n",
					"    props.put(\"security.protocol\", \"SASL_SSL\")\r\n",
					"    props.put(\"sasl.jaas.config\", s\"org.apache.kafka.common.security.plain.PlainLoginModule  required username='$kafkaAPIKey'   password='$kafkaAPISecret';\")\r\n",
					"    props.put(\"sasl.mechanism\", \"PLAIN\")\r\n",
					"    // Required for correctness in Apache Kafka clients prior to 2.6\r\n",
					"    props.put(\"client.dns.lookup\", \"use_all_dns_ips\")\r\n",
					"    // Best practice for Kafka producer to prevent data loss\r\n",
					"    props.put(\"acks\", \"all\")\r\n",
					"}\r\n",
					"\r\n",
					"\r\n",
					"def produceSingleVideoUsage(user: Int, usageId: Int): Unit = {\r\n",
					"    val usageTopic = \"video_usage\"\r\n",
					"\r\n",
					"    val producer = new KafkaProducer[String, String](props)\r\n",
					"\r\n",
					"    val randomCompleted = () => { if (Random.nextInt(2) == 1) true else false }\r\n",
					"    val randomDuration= () => { Random.nextInt(360) }\r\n",
					"    val timestampNow = () => Timestamp.from(Instant.now)\r\n",
					"\r\n",
					"    try {\r\n",
					"        val usageInfo = s\"\"\"{\"usageId\": $usageId, \"user\": \"user${user}\", \"completed\": ${randomCompleted()}, \"durationSeconds\": ${randomDuration()}, \"eventTimestamp\": \"${timestampNow()}\"}\"\"\"\r\n",
					"        val record = new ProducerRecord[String, String](usageTopic, usageId.toString, usageInfo)\r\n",
					"\r\n",
					"        //println(usageInfo)\r\n",
					"        produceRecord(producer, record)\r\n",
					"\r\n",
					"    }catch{\r\n",
					"        case e:Exception => e.printStackTrace()\r\n",
					"    }finally {\r\n",
					"        producer.close()\r\n",
					"    }\r\n",
					"}\r\n",
					""
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def produceRecord(producer: KafkaProducer[String, String], record: ProducerRecord[String, String]) = {\r\n",
					"    val metadata = producer.send(record)\r\n",
					"    printf(s\"sent to topic %s: record(key=%s value=%s) \" +\r\n",
					"        \"meta(partition=%d, offset=%d)\\n\",\r\n",
					"        record.topic(), record.key(), record.value(),\r\n",
					"        metadata.get().partition(),\r\n",
					"        metadata.get().offset()\r\n",
					"    )\r\n",
					"}"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def writeUsageWithPlan(): Unit = {\r\n",
					"    val usageTopic = \"video_usage\"\r\n",
					"\r\n",
					"    val producer = new KafkaProducer[String, String](props)\r\n",
					"\r\n",
					"    val randomCompleted = () => {\r\n",
					"        if (Random.nextInt(2) == 1) true else false\r\n",
					"    }\r\n",
					"    val randomUser = () => {\r\n",
					"        Random.nextInt(100)\r\n",
					"    }\r\n",
					"    val randomDuration = () => {\r\n",
					"        Random.nextInt(360)\r\n",
					"    }\r\n",
					"    val timestampNow = () => Timestamp.from(Instant.now)\r\n",
					"    val startInstant = Instant.now\r\n",
					"\r\n",
					"    val maxRecordId = 10000\r\n",
					"    val sleepMilliseconds = 500\r\n",
					"    val randomizeSleep = () => Random.nextInt(9) + 1\r\n",
					"    val pauseThreshold = 0 // at which record should pause start being enforced, 0 means all records\r\n",
					"\r\n",
					"    try {\r\n",
					"        for (i <- 0 to maxRecordId) {\r\n",
					"        val usageInfo = s\"\"\"{\"usageId\": $i, \"user\": \"user${randomUser()}\", \"completed\": ${randomCompleted()}, \"durationSeconds\": ${randomDuration()}, \"eventTimestamp\": \"${timestampNow()}\"}\"\"\"\r\n",
					"        val record = new ProducerRecord[String, String](usageTopic, i.toString, usageInfo)\r\n",
					"\r\n",
					"        println(usageInfo)\r\n",
					"        produceRecord(producer, record)\r\n",
					"\r\n",
					"        if (i > pauseThreshold) {\r\n",
					"            \r\n",
					"            Thread.sleep(sleepMilliseconds / randomizeSleep())\r\n",
					"        }\r\n",
					"        }\r\n",
					"    } catch {\r\n",
					"        case e: Exception => e.printStackTrace()\r\n",
					"    } finally {\r\n",
					"        producer.close()\r\n",
					"    }\r\n",
					"}\r\n",
					"\r\n",
					"\r\n",
					"writeUsageWithPlan()"
				]
			}
		]
	}
}