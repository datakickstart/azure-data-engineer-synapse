{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "datakickstart-synapse"
		},
		"AzureSqlDatabase1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSqlDatabase1'"
		},
		"AzureSqlStackoverflow_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSqlStackoverflow'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=sandbox-2-sqlserver.database.windows.net;Initial Catalog=@{linkedService().database_name};User ID=dv_admin"
		},
		"Snowflake1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'Snowflake1'"
		},
		"SnowflakeDemo_YellowTrips_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'SnowflakeDemo_YellowTrips'"
		},
		"StackOverflow2_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'StackOverflow2'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=sandbox-2-sqlserver.database.windows.net;Initial Catalog=@{linkedService().source_database};User ID=dv_admin"
		},
		"StackOverflowTest2_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'StackOverflowTest2'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=sandbox-2-sqlserver.database.windows.net;Initial Catalog=@{linkedService().database_name};User ID=dv_admin"
		},
		"datakickstart-synapse-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'datakickstart-synapse-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:datakickstart-synapse.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"datakickstart_databricks_ls_accessToken": {
			"type": "secureString",
			"metadata": "Secure string for 'accessToken' of 'datakickstart_databricks_ls'"
		},
		"dvtrainingadls_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'dvtrainingadls'"
		},
		"sandboxsqlserverless_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'sandboxsqlserverless'"
		},
		"synapse_serverless_datakickstart_ls_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synapse_serverless_datakickstart_ls'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=datakickstart-synapse-ondemand.sql.azuresynapse.net;Initial Catalog=demo"
		},
		"LogOutput_datakickstartadls_properties_typeProperties_serviceEndpoint": {
			"type": "string",
			"defaultValue": "https://datakickstartadls.blob.core.windows.net/"
		},
		"datakickstart-synapse-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datakickstartadls.dfs.core.windows.net"
		},
		"datakickstartadls2_ls_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datakickstartadls2.dfs.core.windows.net/"
		},
		"datakickstartadls_ls_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datakickstartadls.dfs.core.windows.net"
		},
		"demokv_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://dvtrainingkv.vault.azure.net/"
		},
		"dvtrainingadls_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://dvtrainingadls.dfs.core.windows.net"
		},
		"dvtrainingadls_blobsas_ls_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'dvtrainingadls_blobsas_ls'"
		},
		"nyc_tlc_yellow_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'nyc_tlc_yellow'"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSQL_VideoView_CDC_Pipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "AzureSQL_VideoView_CDC",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.2:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Dataflow1",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"AzureSQLVideos": {},
									"VideoViewCDCsink": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Coarse",
							"continuationSettings": {
								"customizedCheckpointKey": "4c9a1b6e-a137-4ae1-95ff-436f555aa8f0"
							}
						}
					},
					{
						"name": "AzureSQL_Member_CDC",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.2:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Dataflow2",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"membercdcsink": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Coarse"
						}
					},
					{
						"name": "AzureSQL_VideoView_JSON_CDC",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.2:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Dataflow1",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"AzureSQLVideos": {},
									"VideoViewCDCsink": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Coarse",
							"continuationSettings": {
								"customizedCheckpointKey": "4c9a1b6e-a137-4ae1-95ff-436f555aa8f0"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/Dataflow1')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/dataflows/Dataflow2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MetadataDrivenCopyTask_q6h_BottomLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will copy objects from one group. The objects belonging to this group will be copied parallelly.",
				"activities": [
					{
						"name": "ListObjectsFromOneGroup",
						"description": "List objects from one group and iterate each of them to downstream activities",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.ObjectsPerGroupToCopy",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "RouteJobsBasedOnLoadingBehavior",
									"description": "Check the loading behavior for each object if it requires full load or incremental load. If it is Default or FullLoad case, do full load. If it is DeltaLoad case, do incremental load.",
									"type": "Switch",
									"dependsOn": [
										{
											"activity": "GetSourceConnectionValues",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"on": {
											"value": "@json(item().DataLoadingBehaviorSettings).dataLoadingBehavior",
											"type": "Expression"
										},
										"cases": [
											{
												"value": "FullLoad",
												"activities": [
													{
														"name": "FullLoadOneObject",
														"description": "Take a full snapshot on this object and copy it to the destination",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [
															{
																"name": "Source",
																"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
															},
															{
																"name": "Destination",
																"value": "@{json(item().SinkObjectSettings).fileSystem}/@{json(item().SinkObjectSettings).folderPath}/@{json(item().SinkObjectSettings).fileName}"
															}
														],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "@json(item().CopySourceSettings).sqlReaderQuery",
																	"type": "Expression"
																},
																"partitionOption": {
																	"value": "@json(item().CopySourceSettings).partitionOption",
																	"type": "Expression"
																},
																"partitionSettings": {
																	"partitionColumnName": {
																		"value": "@json(item().CopySourceSettings).partitionColumnName",
																		"type": "Expression"
																	},
																	"partitionUpperBound": {
																		"value": "@json(item().CopySourceSettings).partitionUpperBound",
																		"type": "Expression"
																	},
																	"partitionLowerBound": {
																		"value": "@json(item().CopySourceSettings).partitionLowerBound",
																		"type": "Expression"
																	},
																	"partitionNames": "@json(item().CopySourceSettings).partitionNames"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"parallelCopies": 4,
															"validateDataConsistency": false,
															"dataIntegrationUnits": 16,
															"translator": {
																"value": "@json(item().CopyActivitySettings).translator",
																"type": "Expression"
															}
														},
														"inputs": [
															{
																"referenceName": "MetadataDrivenCopyTask_q6h_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	},
																	"cw_ls_database_name": {
																		"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "MetadataDrivenCopyTask_q6h_DestinationDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_fileName": {
																		"value": "@json(item().SinkObjectSettings).fileName",
																		"type": "Expression"
																	},
																	"cw_folderPath": {
																		"value": "@json(item().SinkObjectSettings).folderPath",
																		"type": "Expression"
																	},
																	"cw_fileSystem": {
																		"value": "@json(item().SinkObjectSettings).fileSystem",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "DeltaLoad",
												"activities": [
													{
														"name": "GetMaxWatermarkValue",
														"description": "Query the source object to get the max value from watermark column",
														"type": "Lookup",
														"dependsOn": [],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "select max([@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}]) as CurrentMaxWaterMarkColumnValue from [@{json(item().SourceObjectSettings).schema}].[@{json(item().SourceObjectSettings).table}]",
																	"type": "Expression"
																},
																"partitionOption": "None"
															},
															"dataset": {
																"referenceName": "MetadataDrivenCopyTask_q6h_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	},
																	"cw_ls_database_name": {
																		"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																		"type": "Expression"
																	}
																}
															}
														}
													},
													{
														"name": "DeltaLoadOneObject",
														"description": "Copy the changed data only from last time via comparing the value in watermark column to identify changes.",
														"type": "Copy",
														"dependsOn": [
															{
																"activity": "GetMaxWatermarkValue",
																"dependencyConditions": [
																	"Succeeded"
																]
															}
														],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [
															{
																"name": "Source",
																"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
															},
															{
																"name": "Destination",
																"value": "@{json(item().SinkObjectSettings).fileSystem}/@{json(item().SinkObjectSettings).folderPath}/@{json(item().SinkObjectSettings).fileName}"
															}
														],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "select * from [@{json(item().SourceObjectSettings).schema}].[@{json(item().SourceObjectSettings).table}] \n    where [@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}] > @{if(contains(json(item().DataLoadingBehaviorSettings).watermarkColumnType, 'Int'),\n    json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue, \n    concat('''', json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue, ''''))}\n    and [@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}] <= @{if(contains(json(item().DataLoadingBehaviorSettings).watermarkColumnType, 'Int'),\n    activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue, \n    concat('''', activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue, ''''))}",
																	"type": "Expression"
																},
																"partitionOption": {
																	"value": "@json(item().CopySourceSettings).partitionOption",
																	"type": "Expression"
																},
																"partitionSettings": {
																	"partitionColumnName": {
																		"value": "@json(item().CopySourceSettings).partitionColumnName",
																		"type": "Expression"
																	},
																	"partitionUpperBound": {
																		"value": "@json(item().CopySourceSettings).partitionUpperBound",
																		"type": "Expression"
																	},
																	"partitionLowerBound": {
																		"value": "@json(item().CopySourceSettings).partitionLowerBound",
																		"type": "Expression"
																	},
																	"partitionNames": "@json(item().CopySourceSettings).partitionNames"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"parallelCopies": 4,
															"validateDataConsistency": false,
															"dataIntegrationUnits": 16,
															"translator": {
																"value": "@json(item().CopyActivitySettings).translator",
																"type": "Expression"
															}
														},
														"inputs": [
															{
																"referenceName": "MetadataDrivenCopyTask_q6h_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	},
																	"cw_ls_database_name": {
																		"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "MetadataDrivenCopyTask_q6h_DestinationDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_fileName": {
																		"value": "@{json(item().SinkObjectSettings).fileName}-@{json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue}-@{activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue}",
																		"type": "Expression"
																	},
																	"cw_folderPath": {
																		"value": "@json(item().SinkObjectSettings).folderPath",
																		"type": "Expression"
																	},
																	"cw_fileSystem": {
																		"value": "@json(item().SinkObjectSettings).fileSystem",
																		"type": "Expression"
																	}
																}
															}
														]
													},
													{
														"name": "UpdateWatermarkColumnValue",
														"type": "SqlServerStoredProcedure",
														"dependsOn": [
															{
																"activity": "DeltaLoadOneObject",
																"dependencyConditions": [
																	"Succeeded"
																]
															}
														],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"storedProcedureName": "[dbo].[UpdateWatermarkColumnValue_q6h]",
															"storedProcedureParameters": {
																"Id": {
																	"value": {
																		"value": "@item().Id",
																		"type": "Expression"
																	},
																	"type": "Int32"
																},
																"watermarkColumnStartValue": {
																	"value": {
																		"value": "@activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue",
																		"type": "Expression"
																	},
																	"type": "String"
																}
															}
														},
														"linkedServiceName": {
															"referenceName": "sandboxsqlserverless",
															"type": "LinkedServiceReference"
														}
													}
												]
											}
										],
										"defaultActivities": [
											{
												"name": "DefaultFullLoadOneObject",
												"description": "Take a full snapshot on this object and copy it to the destination",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "7.00:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [
													{
														"name": "Source",
														"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
													},
													{
														"name": "Destination",
														"value": "@{json(item().SinkObjectSettings).fileSystem}/@{json(item().SinkObjectSettings).folderPath}/@{json(item().SinkObjectSettings).fileName}"
													}
												],
												"typeProperties": {
													"source": {
														"type": "AzureSqlSource",
														"sqlReaderQuery": {
															"value": "@json(item().CopySourceSettings).sqlReaderQuery",
															"type": "Expression"
														},
														"partitionOption": {
															"value": "@json(item().CopySourceSettings).partitionOption",
															"type": "Expression"
														},
														"partitionSettings": {
															"partitionColumnName": {
																"value": "@json(item().CopySourceSettings).partitionColumnName",
																"type": "Expression"
															},
															"partitionUpperBound": {
																"value": "@json(item().CopySourceSettings).partitionUpperBound",
																"type": "Expression"
															},
															"partitionLowerBound": {
																"value": "@json(item().CopySourceSettings).partitionLowerBound",
																"type": "Expression"
															},
															"partitionNames": "@json(item().CopySourceSettings).partitionNames"
														}
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"parallelCopies": 4,
													"validateDataConsistency": false,
													"dataIntegrationUnits": 16,
													"translator": {
														"value": "@json(item().CopyActivitySettings).translator",
														"type": "Expression"
													}
												},
												"inputs": [
													{
														"referenceName": "MetadataDrivenCopyTask_q6h_SourceDS",
														"type": "DatasetReference",
														"parameters": {
															"cw_schema": {
																"value": "@json(item().SourceObjectSettings).schema",
																"type": "Expression"
															},
															"cw_table": {
																"value": "@json(item().SourceObjectSettings).table",
																"type": "Expression"
															},
															"cw_ls_database_name": {
																"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "MetadataDrivenCopyTask_q6h_DestinationDS",
														"type": "DatasetReference",
														"parameters": {
															"cw_fileName": {
																"value": "@json(item().SinkObjectSettings).fileName",
																"type": "Expression"
															},
															"cw_folderPath": {
																"value": "@json(item().SinkObjectSettings).folderPath",
																"type": "Expression"
															},
															"cw_fileSystem": {
																"value": "@json(item().SinkObjectSettings).fileSystem",
																"type": "Expression"
															}
														}
													}
												]
											}
										]
									}
								},
								{
									"name": "GetSourceConnectionValues",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "AzureSqlSource",
											"sqlReaderQuery": {
												"value": "select ConnectionSettings from @{pipeline().parameters.ConnectionControlTableName}\n                                where Name = '@{item().SourceConnectionSettingsName}'",
												"type": "Expression"
											},
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "MetadataDrivenCopyTask_q6h_ControlDS",
											"type": "DatasetReference",
											"parameters": {}
										},
										"firstRowOnly": false
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"ObjectsPerGroupToCopy": {
						"type": "Array"
					},
					"ConnectionControlTableName": {
						"type": "String"
					}
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_q6h_20220818"
				},
				"annotations": [],
				"lastPublishTime": "2022-08-23T04:35:16Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/MetadataDrivenCopyTask_q6h_ControlDS')]",
				"[concat(variables('workspaceId'), '/datasets/MetadataDrivenCopyTask_q6h_SourceDS')]",
				"[concat(variables('workspaceId'), '/datasets/MetadataDrivenCopyTask_q6h_DestinationDS')]",
				"[concat(variables('workspaceId'), '/linkedServices/sandboxsqlserverless')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MetadataDrivenCopyTask_q6h_MiddleLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will copy one batch of objects. The objects belonging to this batch will be copied parallelly.",
				"activities": [
					{
						"name": "DivideOneBatchIntoMultipleGroups",
						"description": "Divide objects from single batch into multiple sub parallel groups to avoid reaching the output limit of lookup activity.",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0, add(div(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                    if(equals(mod(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), 0), 0, 1)))",
								"type": "Expression"
							},
							"isSequential": false,
							"batchCount": 50,
							"activities": [
								{
									"name": "GetObjectsPerGroupToCopy",
									"description": "Get objects (tables etc.) from control table required to be copied in this group. The order of objects to be copied following the TaskId in control table (ORDER BY [TaskId] DESC).",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "AzureSqlSource",
											"sqlReaderQuery": {
												"value": "WITH OrderedControlTable AS (\n                             SELECT *, ROW_NUMBER() OVER (ORDER BY [TaskId], [Id] DESC) AS RowNumber\n                             FROM @{pipeline().parameters.MainControlTableName}\n                             where TopLevelPipelineName = '@{pipeline().parameters.TopLevelPipelineName}'\n                             and TriggerName like '%@{pipeline().parameters.TriggerName}%' and CopyEnabled = 1)\n                             SELECT * FROM OrderedControlTable WHERE RowNumber BETWEEN @{add(mul(int(item()),pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                             add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch), 1))}\n                             AND @{min(add(mul(int(item()), pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch),\n                             pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity)),\n                            mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, add(pipeline().parameters.CurrentSequentialNumberOfBatch,1)), pipeline().parameters.SumOfObjectsToCopy)}",
												"type": "Expression"
											},
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "MetadataDrivenCopyTask_q6h_ControlDS",
											"type": "DatasetReference",
											"parameters": {}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "CopyObjectsInOneGroup",
									"description": "Execute another pipeline to copy objects from one group. The objects belonging to this group will be copied parallelly.",
									"type": "ExecutePipeline",
									"dependsOn": [
										{
											"activity": "GetObjectsPerGroupToCopy",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "MetadataDrivenCopyTask_q6h_BottomLevel",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"ObjectsPerGroupToCopy": {
												"value": "@activity('GetObjectsPerGroupToCopy').output.value",
												"type": "Expression"
											},
											"ConnectionControlTableName": {
												"value": "@pipeline().parameters.ConnectionControlTableName",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"MaxNumberOfObjectsReturnedFromLookupActivity": {
						"type": "Int"
					},
					"TopLevelPipelineName": {
						"type": "String"
					},
					"TriggerName": {
						"type": "String"
					},
					"CurrentSequentialNumberOfBatch": {
						"type": "Int"
					},
					"SumOfObjectsToCopy": {
						"type": "Int"
					},
					"SumOfObjectsToCopyForCurrentBatch": {
						"type": "Int"
					},
					"MainControlTableName": {
						"type": "String"
					},
					"ConnectionControlTableName": {
						"type": "String"
					}
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_q6h_20220818"
				},
				"annotations": [],
				"lastPublishTime": "2022-08-23T04:35:26Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/MetadataDrivenCopyTask_q6h_ControlDS')]",
				"[concat(variables('workspaceId'), '/pipelines/MetadataDrivenCopyTask_q6h_BottomLevel')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MetadataDrivenCopyTask_q6h_TopLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will count the total number of objects (tables etc.) required to be copied in this run, come up with the number of sequential batches based on the max allowed concurrent copy task, and then execute another pipeline to copy different batches sequentially.",
				"activities": [
					{
						"name": "GetSumOfObjectsToCopy",
						"description": "Count the total number of objects (tables etc.) required to be copied in this run.",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": {
									"value": "SELECT count(*) as count FROM @{pipeline().parameters.MainControlTableName} where TopLevelPipelineName='@{pipeline().Pipeline}' and TriggerName like '%@{pipeline().TriggerName}%' and CopyEnabled = 1",
									"type": "Expression"
								},
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "MetadataDrivenCopyTask_q6h_ControlDS",
								"type": "DatasetReference",
								"parameters": {}
							}
						}
					},
					{
						"name": "CopyBatchesOfObjectsSequentially",
						"description": "Come up with the number of sequential batches based on the max allowed concurrent copy tasks, and then execute another pipeline to copy different batches sequentially.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "GetSumOfObjectsToCopy",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0, add(div(activity('GetSumOfObjectsToCopy').output.firstRow.count,\n                    pipeline().parameters.MaxNumberOfConcurrentTasks),\n                    if(equals(mod(activity('GetSumOfObjectsToCopy').output.firstRow.count,\n                    pipeline().parameters.MaxNumberOfConcurrentTasks), 0), 0, 1)))",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "CopyObjectsInOneBatch",
									"description": "Execute another pipeline to copy one batch of objects. The objects belonging to this batch will be copied parallelly.",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "MetadataDrivenCopyTask_q6h_MiddleLevel",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"MaxNumberOfObjectsReturnedFromLookupActivity": {
												"value": "@pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity",
												"type": "Expression"
											},
											"TopLevelPipelineName": {
												"value": "@{pipeline().Pipeline}",
												"type": "Expression"
											},
											"TriggerName": {
												"value": "@{pipeline().TriggerName}",
												"type": "Expression"
											},
											"CurrentSequentialNumberOfBatch": {
												"value": "@item()",
												"type": "Expression"
											},
											"SumOfObjectsToCopy": {
												"value": "@activity('GetSumOfObjectsToCopy').output.firstRow.count",
												"type": "Expression"
											},
											"SumOfObjectsToCopyForCurrentBatch": {
												"value": "@min(pipeline().parameters.MaxNumberOfConcurrentTasks, activity('GetSumOfObjectsToCopy').output.firstRow.count)",
												"type": "Expression"
											},
											"MainControlTableName": {
												"value": "@pipeline().parameters.MainControlTableName",
												"type": "Expression"
											},
											"ConnectionControlTableName": {
												"value": "@pipeline().parameters.ConnectionControlTableName",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"MaxNumberOfObjectsReturnedFromLookupActivity": {
						"type": "Int",
						"defaultValue": 5000
					},
					"MaxNumberOfConcurrentTasks": {
						"type": "Int",
						"defaultValue": 4
					},
					"MainControlTableName": {
						"type": "String",
						"defaultValue": "dbo.MainControlTable_q6h"
					},
					"ConnectionControlTableName": {
						"type": "String",
						"defaultValue": "dbo.ConnectionControlTable_q6h"
					}
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_q6h_20220818"
				},
				"annotations": [
					"MetadataDrivenSolution"
				],
				"lastPublishTime": "2022-08-23T04:35:38Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/MetadataDrivenCopyTask_q6h_ControlDS')]",
				"[concat(variables('workspaceId'), '/pipelines/MetadataDrivenCopyTask_q6h_MiddleLevel')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MetadataDrivenCopyTask_skn_BottomLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will copy objects from one group. The objects belonging to this group will be copied parallelly.",
				"activities": [
					{
						"name": "ListObjectsFromOneGroup",
						"description": "List objects from one group and iterate each of them to downstream activities",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.ObjectsPerGroupToCopy",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "RouteJobsBasedOnLoadingBehavior",
									"description": "Check the loading behavior for each object if it requires full load or incremental load. If it is Default or FullLoad case, do full load. If it is DeltaLoad case, do incremental load.",
									"type": "Switch",
									"dependsOn": [
										{
											"activity": "GetSourceConnectionValues",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"on": {
											"value": "@json(item().DataLoadingBehaviorSettings).dataLoadingBehavior",
											"type": "Expression"
										},
										"cases": [
											{
												"value": "FullLoad",
												"activities": [
													{
														"name": "FullLoadOneObject",
														"description": "Take a full snapshot on this object and copy it to the destination",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "0.12:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [
															{
																"name": "Source",
																"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
															},
															{
																"name": "Destination",
																"value": "@{json(item().SinkObjectSettings).fileSystem}/@{json(item().SinkObjectSettings).folderPath}/@{json(item().SinkObjectSettings).fileName}"
															}
														],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "@json(item().CopySourceSettings).sqlReaderQuery",
																	"type": "Expression"
																},
																"partitionOption": {
																	"value": "@json(item().CopySourceSettings).partitionOption",
																	"type": "Expression"
																},
																"partitionSettings": {
																	"partitionColumnName": {
																		"value": "@json(item().CopySourceSettings).partitionColumnName",
																		"type": "Expression"
																	},
																	"partitionUpperBound": {
																		"value": "@json(item().CopySourceSettings).partitionUpperBound",
																		"type": "Expression"
																	},
																	"partitionLowerBound": {
																		"value": "@json(item().CopySourceSettings).partitionLowerBound",
																		"type": "Expression"
																	},
																	"partitionNames": "@json(item().CopySourceSettings).partitionNames"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"value": "@json(item().CopyActivitySettings).translator",
																"type": "Expression"
															}
														},
														"inputs": [
															{
																"referenceName": "MetadataDrivenCopyTask_skn_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	},
																	"cw_ls_database_name": {
																		"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "MetadataDrivenCopyTask_skn_DestinationDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_fileName": {
																		"value": "@json(item().SinkObjectSettings).fileName",
																		"type": "Expression"
																	},
																	"cw_folderPath": {
																		"value": "@json(item().SinkObjectSettings).folderPath",
																		"type": "Expression"
																	},
																	"cw_fileSystem": {
																		"value": "@json(item().SinkObjectSettings).fileSystem",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "DeltaLoad",
												"activities": [
													{
														"name": "GetMaxWatermarkValue",
														"description": "Query the source object to get the max value from watermark column",
														"type": "Lookup",
														"dependsOn": [],
														"policy": {
															"timeout": "0.12:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "select max([@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}]) as CurrentMaxWaterMarkColumnValue from [@{json(item().SourceObjectSettings).schema}].[@{json(item().SourceObjectSettings).table}]",
																	"type": "Expression"
																},
																"partitionOption": "None"
															},
															"dataset": {
																"referenceName": "MetadataDrivenCopyTask_skn_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	},
																	"cw_ls_database_name": {
																		"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																		"type": "Expression"
																	}
																}
															}
														}
													},
													{
														"name": "DeltaLoadOneObject",
														"description": "Copy the changed data only from last time via comparing the value in watermark column to identify changes.",
														"type": "Copy",
														"dependsOn": [
															{
																"activity": "GetMaxWatermarkValue",
																"dependencyConditions": [
																	"Succeeded"
																]
															}
														],
														"policy": {
															"timeout": "0.12:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [
															{
																"name": "Source",
																"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
															},
															{
																"name": "Destination",
																"value": "@{json(item().SinkObjectSettings).fileSystem}/@{json(item().SinkObjectSettings).folderPath}/@{json(item().SinkObjectSettings).fileName}"
															}
														],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "select * from [@{json(item().SourceObjectSettings).schema}].[@{json(item().SourceObjectSettings).table}] \n    where [@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}] > @{if(contains(json(item().DataLoadingBehaviorSettings).watermarkColumnType, 'Int'),\n    json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue, \n    concat('''', json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue, ''''))}\n    and [@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}] <= @{if(contains(json(item().DataLoadingBehaviorSettings).watermarkColumnType, 'Int'),\n    activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue, \n    concat('''', activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue, ''''))}",
																	"type": "Expression"
																},
																"partitionOption": {
																	"value": "@json(item().CopySourceSettings).partitionOption",
																	"type": "Expression"
																},
																"partitionSettings": {
																	"partitionColumnName": {
																		"value": "@json(item().CopySourceSettings).partitionColumnName",
																		"type": "Expression"
																	},
																	"partitionUpperBound": {
																		"value": "@json(item().CopySourceSettings).partitionUpperBound",
																		"type": "Expression"
																	},
																	"partitionLowerBound": {
																		"value": "@json(item().CopySourceSettings).partitionLowerBound",
																		"type": "Expression"
																	},
																	"partitionNames": "@json(item().CopySourceSettings).partitionNames"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"value": "@json(item().CopyActivitySettings).translator",
																"type": "Expression"
															}
														},
														"inputs": [
															{
																"referenceName": "MetadataDrivenCopyTask_skn_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	},
																	"cw_ls_database_name": {
																		"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "MetadataDrivenCopyTask_skn_DestinationDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_fileName": {
																		"value": "@{json(item().SinkObjectSettings).fileName}-@{json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue}-@{activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue}",
																		"type": "Expression"
																	},
																	"cw_folderPath": {
																		"value": "@json(item().SinkObjectSettings).folderPath",
																		"type": "Expression"
																	},
																	"cw_fileSystem": {
																		"value": "@json(item().SinkObjectSettings).fileSystem",
																		"type": "Expression"
																	}
																}
															}
														]
													},
													{
														"name": "UpdateWatermarkColumnValue",
														"type": "SqlServerStoredProcedure",
														"dependsOn": [
															{
																"activity": "DeltaLoadOneObject",
																"dependencyConditions": [
																	"Succeeded"
																]
															}
														],
														"policy": {
															"timeout": "0.12:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"storedProcedureName": "[UpdateWatermarkColumnValue_skn]",
															"storedProcedureParameters": {
																"Id": {
																	"value": {
																		"value": "@item().Id",
																		"type": "Expression"
																	},
																	"type": "Int32"
																},
																"watermarkColumnStartValue": {
																	"value": {
																		"value": "@activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue",
																		"type": "Expression"
																	},
																	"type": "String"
																}
															}
														},
														"linkedServiceName": {
															"referenceName": "AzureSqlDatabase1",
															"type": "LinkedServiceReference"
														}
													}
												]
											}
										],
										"defaultActivities": [
											{
												"name": "DefaultFullLoadOneObject",
												"description": "Take a full snapshot on this object and copy it to the destination",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [
													{
														"name": "Source",
														"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
													},
													{
														"name": "Destination",
														"value": "@{json(item().SinkObjectSettings).fileSystem}/@{json(item().SinkObjectSettings).folderPath}/@{json(item().SinkObjectSettings).fileName}"
													}
												],
												"typeProperties": {
													"source": {
														"type": "AzureSqlSource",
														"sqlReaderQuery": {
															"value": "@json(item().CopySourceSettings).sqlReaderQuery",
															"type": "Expression"
														},
														"partitionOption": {
															"value": "@json(item().CopySourceSettings).partitionOption",
															"type": "Expression"
														},
														"partitionSettings": {
															"partitionColumnName": {
																"value": "@json(item().CopySourceSettings).partitionColumnName",
																"type": "Expression"
															},
															"partitionUpperBound": {
																"value": "@json(item().CopySourceSettings).partitionUpperBound",
																"type": "Expression"
															},
															"partitionLowerBound": {
																"value": "@json(item().CopySourceSettings).partitionLowerBound",
																"type": "Expression"
															},
															"partitionNames": "@json(item().CopySourceSettings).partitionNames"
														}
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"validateDataConsistency": false,
													"translator": {
														"value": "@json(item().CopyActivitySettings).translator",
														"type": "Expression"
													}
												},
												"inputs": [
													{
														"referenceName": "MetadataDrivenCopyTask_skn_SourceDS",
														"type": "DatasetReference",
														"parameters": {
															"cw_schema": {
																"value": "@json(item().SourceObjectSettings).schema",
																"type": "Expression"
															},
															"cw_table": {
																"value": "@json(item().SourceObjectSettings).table",
																"type": "Expression"
															},
															"cw_ls_database_name": {
																"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "MetadataDrivenCopyTask_skn_DestinationDS",
														"type": "DatasetReference",
														"parameters": {
															"cw_fileName": {
																"value": "@json(item().SinkObjectSettings).fileName",
																"type": "Expression"
															},
															"cw_folderPath": {
																"value": "@json(item().SinkObjectSettings).folderPath",
																"type": "Expression"
															},
															"cw_fileSystem": {
																"value": "@json(item().SinkObjectSettings).fileSystem",
																"type": "Expression"
															}
														}
													}
												]
											}
										]
									}
								},
								{
									"name": "GetSourceConnectionValues",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "AzureSqlSource",
											"sqlReaderQuery": {
												"value": "select ConnectionSettings from @{pipeline().parameters.ConnectionControlTableName}\n                                where Name = '@{item().SourceConnectionSettingsName}'",
												"type": "Expression"
											},
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "MetadataDrivenCopyTask_skn_ControlDS",
											"type": "DatasetReference",
											"parameters": {}
										},
										"firstRowOnly": false
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"ObjectsPerGroupToCopy": {
						"type": "Array"
					},
					"ConnectionControlTableName": {
						"type": "String"
					}
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_skn_20221208"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/MetadataDrivenCopyTask_skn_ControlDS')]",
				"[concat(variables('workspaceId'), '/datasets/MetadataDrivenCopyTask_skn_SourceDS')]",
				"[concat(variables('workspaceId'), '/datasets/MetadataDrivenCopyTask_skn_DestinationDS')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlDatabase1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MetadataDrivenCopyTask_skn_MiddleLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will copy one batch of objects. The objects belonging to this batch will be copied parallelly.",
				"activities": [
					{
						"name": "DivideOneBatchIntoMultipleGroups",
						"description": "Divide objects from single batch into multiple sub parallel groups to avoid reaching the output limit of lookup activity.",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0, add(div(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                    if(equals(mod(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), 0), 0, 1)))",
								"type": "Expression"
							},
							"isSequential": false,
							"batchCount": 50,
							"activities": [
								{
									"name": "GetObjectsPerGroupToCopy",
									"description": "Get objects (tables etc.) from control table required to be copied in this group. The order of objects to be copied following the TaskId in control table (ORDER BY [TaskId] DESC).",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "AzureSqlSource",
											"sqlReaderQuery": {
												"value": "WITH OrderedControlTable AS (\n                             SELECT *, ROW_NUMBER() OVER (ORDER BY [TaskId], [Id] DESC) AS RowNumber\n                             FROM @{pipeline().parameters.MainControlTableName}\n                             where TopLevelPipelineName = '@{pipeline().parameters.TopLevelPipelineName}'\n                             and TriggerName like '%@{pipeline().parameters.TriggerName}%' and CopyEnabled = 1)\n                             SELECT * FROM OrderedControlTable WHERE RowNumber BETWEEN @{add(mul(int(item()),pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                             add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch), 1))}\n                             AND @{min(add(mul(int(item()), pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch),\n                             pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity)),\n                            mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, add(pipeline().parameters.CurrentSequentialNumberOfBatch,1)), pipeline().parameters.SumOfObjectsToCopy)}",
												"type": "Expression"
											},
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "MetadataDrivenCopyTask_skn_ControlDS",
											"type": "DatasetReference",
											"parameters": {}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "CopyObjectsInOneGroup",
									"description": "Execute another pipeline to copy objects from one group. The objects belonging to this group will be copied parallelly.",
									"type": "ExecutePipeline",
									"dependsOn": [
										{
											"activity": "GetObjectsPerGroupToCopy",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "MetadataDrivenCopyTask_skn_BottomLevel",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"ObjectsPerGroupToCopy": {
												"value": "@activity('GetObjectsPerGroupToCopy').output.value",
												"type": "Expression"
											},
											"ConnectionControlTableName": {
												"value": "@pipeline().parameters.ConnectionControlTableName",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"MaxNumberOfObjectsReturnedFromLookupActivity": {
						"type": "Int"
					},
					"TopLevelPipelineName": {
						"type": "String"
					},
					"TriggerName": {
						"type": "String"
					},
					"CurrentSequentialNumberOfBatch": {
						"type": "Int"
					},
					"SumOfObjectsToCopy": {
						"type": "Int"
					},
					"SumOfObjectsToCopyForCurrentBatch": {
						"type": "Int"
					},
					"MainControlTableName": {
						"type": "String"
					},
					"ConnectionControlTableName": {
						"type": "String"
					}
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_skn_20221208"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/MetadataDrivenCopyTask_skn_ControlDS')]",
				"[concat(variables('workspaceId'), '/pipelines/MetadataDrivenCopyTask_skn_BottomLevel')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MetadataDrivenCopyTask_skn_TopLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will count the total number of objects (tables etc.) required to be copied in this run, come up with the number of sequential batches based on the max allowed concurrent copy task, and then execute another pipeline to copy different batches sequentially.",
				"activities": [
					{
						"name": "GetSumOfObjectsToCopy",
						"description": "Count the total number of objects (tables etc.) required to be copied in this run.",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": {
									"value": "SELECT count(*) as count FROM @{pipeline().parameters.MainControlTableName} where TopLevelPipelineName='@{pipeline().Pipeline}' and TriggerName like '%@{pipeline().TriggerName}%' and CopyEnabled = 1",
									"type": "Expression"
								},
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "MetadataDrivenCopyTask_skn_ControlDS",
								"type": "DatasetReference",
								"parameters": {}
							}
						}
					},
					{
						"name": "CopyBatchesOfObjectsSequentially",
						"description": "Come up with the number of sequential batches based on the max allowed concurrent copy tasks, and then execute another pipeline to copy different batches sequentially.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "GetSumOfObjectsToCopy",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0, add(div(activity('GetSumOfObjectsToCopy').output.firstRow.count,\n                    pipeline().parameters.MaxNumberOfConcurrentTasks),\n                    if(equals(mod(activity('GetSumOfObjectsToCopy').output.firstRow.count,\n                    pipeline().parameters.MaxNumberOfConcurrentTasks), 0), 0, 1)))",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "CopyObjectsInOneBatch",
									"description": "Execute another pipeline to copy one batch of objects. The objects belonging to this batch will be copied parallelly.",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "MetadataDrivenCopyTask_skn_MiddleLevel",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"MaxNumberOfObjectsReturnedFromLookupActivity": {
												"value": "@pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity",
												"type": "Expression"
											},
											"TopLevelPipelineName": {
												"value": "@{pipeline().Pipeline}",
												"type": "Expression"
											},
											"TriggerName": {
												"value": "@{pipeline().TriggerName}",
												"type": "Expression"
											},
											"CurrentSequentialNumberOfBatch": {
												"value": "@item()",
												"type": "Expression"
											},
											"SumOfObjectsToCopy": {
												"value": "@activity('GetSumOfObjectsToCopy').output.firstRow.count",
												"type": "Expression"
											},
											"SumOfObjectsToCopyForCurrentBatch": {
												"value": "@min(pipeline().parameters.MaxNumberOfConcurrentTasks, activity('GetSumOfObjectsToCopy').output.firstRow.count)",
												"type": "Expression"
											},
											"MainControlTableName": {
												"value": "@pipeline().parameters.MainControlTableName",
												"type": "Expression"
											},
											"ConnectionControlTableName": {
												"value": "@pipeline().parameters.ConnectionControlTableName",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"MaxNumberOfObjectsReturnedFromLookupActivity": {
						"type": "Int",
						"defaultValue": 5000
					},
					"MaxNumberOfConcurrentTasks": {
						"type": "Int",
						"defaultValue": 20
					},
					"MainControlTableName": {
						"type": "String",
						"defaultValue": "MainControlTable_skn"
					},
					"ConnectionControlTableName": {
						"type": "String",
						"defaultValue": "ConnectionControlTable_skn"
					}
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_skn_20221208"
				},
				"annotations": [
					"MetadataDrivenSolution"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/MetadataDrivenCopyTask_skn_ControlDS')]",
				"[concat(variables('workspaceId'), '/pipelines/MetadataDrivenCopyTask_skn_MiddleLevel')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline_with_Logging')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "data_lake_load",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "pyspark_logging",
								"type": "NotebookReference"
							},
							"parameters": {
								"location": {
									"value": {
										"value": "@pipeline().parameters.location",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkLogging2",
								"type": "BigDataPoolReference"
							},
							"executorSize": null,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": null,
							"numExecutors": null
						}
					},
					{
						"name": "Refined_table_create",
						"type": "Script",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"linkedServiceName": {
							"referenceName": "synapse_serverless_datakickstart_ls",
							"type": "LinkedServiceReference"
						},
						"typeProperties": {
							"scripts": [
								{
									"type": "Query",
									"text": {
										"value": "/* IF  EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[dbo].[fact_stackoverflow_post]') AND type in (N'U'))\nDROP EXTERNAL TABLE demo.dbo.fact_stackoverflow_post;\n\nCREATE EXTERNAL TABLE demo.dbo.fact_stackoverflow_post \nWITH (\n    LOCATION = 'stackoverflow/fact_stackoverflow_@{pipeline().TriggerTime}',  \n    DATA_SOURCE = refined_datakickstartadls,  \n    FILE_FORMAT = ParquetFormat  \n)\nAS\n*/\nselect top 20 * from raw_stackoverflow.dbo.posts;",
										"type": "Expression"
									}
								}
							],
							"logSettings": {
								"logDestination": "ExternalStore",
								"logLocationSettings": {
									"linkedServiceName": {
										"referenceName": "LogOutput_datakickstartadls",
										"type": "LinkedServiceReference"
									},
									"path": "demo/synapse_logs"
								}
							}
						}
					},
					{
						"name": "LogAnalyticts_post",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "Refined_table_create",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.1:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "https://@{pipeline().parameters.workspace}.ods.opinsights.azure.com/api/logs?api-version=2016-04-01",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {
								"content-type": "application/json",
								"Authorization": {
									"value": "@pipeline().parameters.auth_signature",
									"type": "Expression"
								},
								"Log-Type": "DataKickstartPipeline",
								"x-ms-date": {
									"value": "@{formatDateTime(utcnow(), 'ddd, dd MMM yyyy HH:mm:ss G\\MT')}",
									"type": "Expression"
								}
							},
							"body": {
								"value": "[{\"TimeGeneratedPipeline\": \"2022-12-07 17:10:01\", \"message\": \"Script completed successfully.\", \"notebook\": \"stackoverflow_fact_create\", \"platform\": \"Synapse Pipelines\", \"version\": 1.0}]",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"location": {
						"type": "string",
						"defaultValue": "dev"
					},
					"workspace": {
						"type": "string",
						"defaultValue": "041a7d31-3f9e-4b27-989c-114d98731f26"
					},
					"auth_signature": {
						"type": "securestring"
					}
				},
				"annotations": [],
				"lastPublishTime": "2022-02-21T23:25:41Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/pyspark_logging')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sparkLogging2')]",
				"[concat(variables('workspaceId'), '/linkedServices/synapse_serverless_datakickstart_ls')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/LogOutput_datakickstartadls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflowCopyTask_g91_BottomLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will copy objects from one group. The objects belonging to this group will be copied parallelly.",
				"activities": [
					{
						"name": "ListObjectsFromOneGroup",
						"description": "List objects from one group and iterate each of them to downstream activities",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.ObjectsPerGroupToCopy",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "RouteJobsBasedOnLoadingBehavior",
									"description": "Check the loading behavior for each object if it requires full load or incremental load. If it is Default or FullLoad case, do full load. If it is DeltaLoad case, do incremental load.",
									"type": "Switch",
									"dependsOn": [
										{
											"activity": "GetSourceConnectionValues",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"on": {
											"value": "@json(item().DataLoadingBehaviorSettings).dataLoadingBehavior",
											"type": "Expression"
										},
										"cases": [
											{
												"value": "FullLoad",
												"activities": [
													{
														"name": "FullLoadOneObject",
														"description": "Take a full snapshot on this object and copy it to the destination",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [
															{
																"name": "Source",
																"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
															},
															{
																"name": "Destination",
																"value": "@{json(item().SinkObjectSettings).fileSystem}/@{json(item().SinkObjectSettings).folderPath}/@{json(item().SinkObjectSettings).fileName}"
															}
														],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "@json(item().CopySourceSettings).sqlReaderQuery",
																	"type": "Expression"
																},
																"partitionOption": {
																	"value": "@json(item().CopySourceSettings).partitionOption",
																	"type": "Expression"
																},
																"partitionSettings": {
																	"partitionColumnName": {
																		"value": "@json(item().CopySourceSettings).partitionColumnName",
																		"type": "Expression"
																	},
																	"partitionUpperBound": {
																		"value": "@json(item().CopySourceSettings).partitionUpperBound",
																		"type": "Expression"
																	},
																	"partitionLowerBound": {
																		"value": "@json(item().CopySourceSettings).partitionLowerBound",
																		"type": "Expression"
																	},
																	"partitionNames": "@json(item().CopySourceSettings).partitionNames"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"parallelCopies": 4,
															"validateDataConsistency": false,
															"logSettings": {
																"enableCopyActivityLog": true,
																"copyActivityLogSettings": {
																	"logLevel": "Info",
																	"enableReliableLogging": false
																},
																"logLocationSettings": {
																	"linkedServiceName": {
																		"referenceName": "LogOutput_datakickstartadls",
																		"type": "LinkedServiceReference"
																	},
																	"path": "demo/synapse_logs"
																}
															},
															"dataIntegrationUnits": 16,
															"translator": {
																"value": "@json(item().CopyActivitySettings).translator",
																"type": "Expression"
															}
														},
														"inputs": [
															{
																"referenceName": "StackOverflowCopyTask_g91_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	},
																	"cw_ls_database_name": {
																		"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "StackOverflowCopyTask_g91_DestinationDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_fileName": {
																		"value": "@json(item().SinkObjectSettings).fileName",
																		"type": "Expression"
																	},
																	"cw_folderPath": {
																		"value": "@json(item().SinkObjectSettings).folderPath",
																		"type": "Expression"
																	},
																	"cw_fileSystem": {
																		"value": "@json(item().SinkObjectSettings).fileSystem",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "DeltaLoad",
												"activities": [
													{
														"name": "GetMaxWatermarkValue",
														"description": "Query the source object to get the max value from watermark column",
														"type": "Lookup",
														"dependsOn": [],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "select max([@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}]) as CurrentMaxWaterMarkColumnValue from [@{json(item().SourceObjectSettings).schema}].[@{json(item().SourceObjectSettings).table}]",
																	"type": "Expression"
																},
																"partitionOption": "None"
															},
															"dataset": {
																"referenceName": "StackOverflowCopyTask_g91_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	},
																	"cw_ls_database_name": {
																		"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																		"type": "Expression"
																	}
																}
															}
														}
													},
													{
														"name": "DeltaLoadOneObject",
														"description": "Copy the changed data only from last time via comparing the value in watermark column to identify changes.",
														"type": "Copy",
														"dependsOn": [
															{
																"activity": "GetMaxWatermarkValue",
																"dependencyConditions": [
																	"Succeeded"
																]
															}
														],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [
															{
																"name": "Source",
																"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
															},
															{
																"name": "Destination",
																"value": "@{json(item().SinkObjectSettings).fileSystem}/@{json(item().SinkObjectSettings).folderPath}/@{json(item().SinkObjectSettings).fileName}"
															}
														],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "select * from [@{json(item().SourceObjectSettings).schema}].[@{json(item().SourceObjectSettings).table}] \n    where [@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}] > @{if(contains(json(item().DataLoadingBehaviorSettings).watermarkColumnType, 'Int'),\n    json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue, \n    concat('''', json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue, ''''))}\n    and [@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}] <= @{if(contains(json(item().DataLoadingBehaviorSettings).watermarkColumnType, 'Int'),\n    activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue, \n    concat('''', activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue, ''''))}",
																	"type": "Expression"
																},
																"partitionOption": {
																	"value": "@json(item().CopySourceSettings).partitionOption",
																	"type": "Expression"
																},
																"partitionSettings": {
																	"partitionColumnName": {
																		"value": "@json(item().CopySourceSettings).partitionColumnName",
																		"type": "Expression"
																	},
																	"partitionUpperBound": {
																		"value": "@json(item().CopySourceSettings).partitionUpperBound",
																		"type": "Expression"
																	},
																	"partitionLowerBound": {
																		"value": "@json(item().CopySourceSettings).partitionLowerBound",
																		"type": "Expression"
																	},
																	"partitionNames": "@json(item().CopySourceSettings).partitionNames"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"parallelCopies": 4,
															"validateDataConsistency": false,
															"dataIntegrationUnits": 16,
															"translator": {
																"value": "@json(item().CopyActivitySettings).translator",
																"type": "Expression"
															}
														},
														"inputs": [
															{
																"referenceName": "StackOverflowCopyTask_g91_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	},
																	"cw_ls_database_name": {
																		"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "StackOverflowCopyTask_g91_DestinationDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_fileName": {
																		"value": "@{json(item().SinkObjectSettings).fileName}-@{json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue}-@{activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue}",
																		"type": "Expression"
																	},
																	"cw_folderPath": {
																		"value": "@json(item().SinkObjectSettings).folderPath",
																		"type": "Expression"
																	},
																	"cw_fileSystem": {
																		"value": "@json(item().SinkObjectSettings).fileSystem",
																		"type": "Expression"
																	}
																}
															}
														]
													},
													{
														"name": "UpdateWatermarkColumnValue",
														"type": "SqlServerStoredProcedure",
														"dependsOn": [
															{
																"activity": "DeltaLoadOneObject",
																"dependencyConditions": [
																	"Succeeded"
																]
															}
														],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"storedProcedureName": "[dbo].[UpdateWatermarkColumnValue_g91]",
															"storedProcedureParameters": {
																"Id": {
																	"value": {
																		"value": "@item().Id",
																		"type": "Expression"
																	},
																	"type": "Int32"
																},
																"watermarkColumnStartValue": {
																	"value": {
																		"value": "@activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue",
																		"type": "Expression"
																	},
																	"type": "String"
																}
															}
														},
														"linkedServiceName": {
															"referenceName": "sandboxsqlserverless",
															"type": "LinkedServiceReference"
														}
													}
												]
											}
										],
										"defaultActivities": [
											{
												"name": "DefaultFullLoadOneObject",
												"description": "Take a full snapshot on this object and copy it to the destination",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "7.00:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [
													{
														"name": "Source",
														"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
													},
													{
														"name": "Destination",
														"value": "@{json(item().SinkObjectSettings).fileSystem}/@{json(item().SinkObjectSettings).folderPath}/@{json(item().SinkObjectSettings).fileName}"
													}
												],
												"typeProperties": {
													"source": {
														"type": "AzureSqlSource",
														"sqlReaderQuery": {
															"value": "@json(item().CopySourceSettings).sqlReaderQuery",
															"type": "Expression"
														},
														"partitionOption": {
															"value": "@json(item().CopySourceSettings).partitionOption",
															"type": "Expression"
														},
														"partitionSettings": {
															"partitionColumnName": {
																"value": "@json(item().CopySourceSettings).partitionColumnName",
																"type": "Expression"
															},
															"partitionUpperBound": {
																"value": "@json(item().CopySourceSettings).partitionUpperBound",
																"type": "Expression"
															},
															"partitionLowerBound": {
																"value": "@json(item().CopySourceSettings).partitionLowerBound",
																"type": "Expression"
															},
															"partitionNames": "@json(item().CopySourceSettings).partitionNames"
														}
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"parallelCopies": 4,
													"validateDataConsistency": false,
													"dataIntegrationUnits": 16,
													"translator": {
														"value": "@json(item().CopyActivitySettings).translator",
														"type": "Expression"
													}
												},
												"inputs": [
													{
														"referenceName": "StackOverflowCopyTask_g91_SourceDS",
														"type": "DatasetReference",
														"parameters": {
															"cw_schema": {
																"value": "@json(item().SourceObjectSettings).schema",
																"type": "Expression"
															},
															"cw_table": {
																"value": "@json(item().SourceObjectSettings).table",
																"type": "Expression"
															},
															"cw_ls_database_name": {
																"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "StackOverflowCopyTask_g91_DestinationDS",
														"type": "DatasetReference",
														"parameters": {
															"cw_fileName": {
																"value": "@json(item().SinkObjectSettings).fileName",
																"type": "Expression"
															},
															"cw_folderPath": {
																"value": "@json(item().SinkObjectSettings).folderPath",
																"type": "Expression"
															},
															"cw_fileSystem": {
																"value": "@json(item().SinkObjectSettings).fileSystem",
																"type": "Expression"
															}
														}
													}
												]
											}
										]
									}
								},
								{
									"name": "GetSourceConnectionValues",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "AzureSqlSource",
											"sqlReaderQuery": {
												"value": "select ConnectionSettings from @{pipeline().parameters.ConnectionControlTableName}\n                                where Name = '@{item().SourceConnectionSettingsName}'",
												"type": "Expression"
											},
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "StackOverflowCopyTask_g91_ControlDS",
											"type": "DatasetReference",
											"parameters": {}
										},
										"firstRowOnly": false
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"ObjectsPerGroupToCopy": {
						"type": "Array"
					},
					"ConnectionControlTableName": {
						"type": "String"
					}
				},
				"folder": {
					"name": "StackOverflowCopyTask_g91_20220729"
				},
				"annotations": [],
				"lastPublishTime": "2022-08-23T04:35:32Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/StackOverflowCopyTask_g91_ControlDS')]",
				"[concat(variables('workspaceId'), '/datasets/StackOverflowCopyTask_g91_SourceDS')]",
				"[concat(variables('workspaceId'), '/datasets/StackOverflowCopyTask_g91_DestinationDS')]",
				"[concat(variables('workspaceId'), '/linkedServices/sandboxsqlserverless')]",
				"[concat(variables('workspaceId'), '/linkedServices/LogOutput_datakickstartadls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflowCopyTask_g91_MiddleLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will copy one batch of objects. The objects belonging to this batch will be copied parallelly.",
				"activities": [
					{
						"name": "DivideOneBatchIntoMultipleGroups",
						"description": "Divide objects from single batch into multiple sub parallel groups to avoid reaching the output limit of lookup activity.",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0, add(div(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                    if(equals(mod(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), 0), 0, 1)))",
								"type": "Expression"
							},
							"isSequential": false,
							"batchCount": 50,
							"activities": [
								{
									"name": "GetObjectsPerGroupToCopy",
									"description": "Get objects (tables etc.) from control table required to be copied in this group. The order of objects to be copied following the TaskId in control table (ORDER BY [TaskId] DESC).",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "AzureSqlSource",
											"sqlReaderQuery": {
												"value": "WITH OrderedControlTable AS (\n                             SELECT *, ROW_NUMBER() OVER (ORDER BY [TaskId], [Id] DESC) AS RowNumber\n                             FROM @{pipeline().parameters.MainControlTableName}\n                             where TopLevelPipelineName = '@{pipeline().parameters.TopLevelPipelineName}'\n                             and TriggerName like '%@{pipeline().parameters.TriggerName}%' and CopyEnabled = 1)\n                             SELECT * FROM OrderedControlTable WHERE RowNumber BETWEEN @{add(mul(int(item()),pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                             add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch), 1))}\n                             AND @{min(add(mul(int(item()), pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch),\n                             pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity)),\n                            mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, add(pipeline().parameters.CurrentSequentialNumberOfBatch,1)), pipeline().parameters.SumOfObjectsToCopy)}",
												"type": "Expression"
											},
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "StackOverflowCopyTask_g91_ControlDS",
											"type": "DatasetReference",
											"parameters": {}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "CopyObjectsInOneGroup",
									"description": "Execute another pipeline to copy objects from one group. The objects belonging to this group will be copied parallelly.",
									"type": "ExecutePipeline",
									"dependsOn": [
										{
											"activity": "GetObjectsPerGroupToCopy",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "StackOverflowCopyTask_g91_BottomLevel",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"ObjectsPerGroupToCopy": {
												"value": "@activity('GetObjectsPerGroupToCopy').output.value",
												"type": "Expression"
											},
											"ConnectionControlTableName": {
												"value": "@pipeline().parameters.ConnectionControlTableName",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"MaxNumberOfObjectsReturnedFromLookupActivity": {
						"type": "Int"
					},
					"TopLevelPipelineName": {
						"type": "String"
					},
					"TriggerName": {
						"type": "String"
					},
					"CurrentSequentialNumberOfBatch": {
						"type": "Int"
					},
					"SumOfObjectsToCopy": {
						"type": "Int"
					},
					"SumOfObjectsToCopyForCurrentBatch": {
						"type": "Int"
					},
					"MainControlTableName": {
						"type": "String"
					},
					"ConnectionControlTableName": {
						"type": "String"
					}
				},
				"folder": {
					"name": "StackOverflowCopyTask_g91_20220729"
				},
				"annotations": [],
				"lastPublishTime": "2022-08-23T04:35:42Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/StackOverflowCopyTask_g91_ControlDS')]",
				"[concat(variables('workspaceId'), '/pipelines/StackOverflowCopyTask_g91_BottomLevel')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflowCopyTask_g91_TopLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will count the total number of objects (tables etc.) required to be copied in this run, come up with the number of sequential batches based on the max allowed concurrent copy task, and then execute another pipeline to copy different batches sequentially.",
				"activities": [
					{
						"name": "GetSumOfObjectsToCopy",
						"description": "Count the total number of objects (tables etc.) required to be copied in this run.",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": {
									"value": "SELECT count(*) as count FROM @{pipeline().parameters.MainControlTableName} where TopLevelPipelineName='@{pipeline().Pipeline}' and TriggerName like '%@{pipeline().TriggerName}%' and CopyEnabled = 1",
									"type": "Expression"
								},
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "StackOverflowCopyTask_g91_ControlDS",
								"type": "DatasetReference",
								"parameters": {}
							}
						}
					},
					{
						"name": "CopyBatchesOfObjectsSequentially",
						"description": "Come up with the number of sequential batches based on the max allowed concurrent copy tasks, and then execute another pipeline to copy different batches sequentially.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "GetSumOfObjectsToCopy",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0, add(div(activity('GetSumOfObjectsToCopy').output.firstRow.count,\n                    pipeline().parameters.MaxNumberOfConcurrentTasks),\n                    if(equals(mod(activity('GetSumOfObjectsToCopy').output.firstRow.count,\n                    pipeline().parameters.MaxNumberOfConcurrentTasks), 0), 0, 1)))",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "CopyObjectsInOneBatch",
									"description": "Execute another pipeline to copy one batch of objects. The objects belonging to this batch will be copied parallelly.",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "StackOverflowCopyTask_g91_MiddleLevel",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"MaxNumberOfObjectsReturnedFromLookupActivity": {
												"value": "@pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity",
												"type": "Expression"
											},
											"TopLevelPipelineName": {
												"value": "@{pipeline().Pipeline}",
												"type": "Expression"
											},
											"TriggerName": {
												"value": "@{pipeline().TriggerName}",
												"type": "Expression"
											},
											"CurrentSequentialNumberOfBatch": {
												"value": "@item()",
												"type": "Expression"
											},
											"SumOfObjectsToCopy": {
												"value": "@activity('GetSumOfObjectsToCopy').output.firstRow.count",
												"type": "Expression"
											},
											"SumOfObjectsToCopyForCurrentBatch": {
												"value": "@min(pipeline().parameters.MaxNumberOfConcurrentTasks, activity('GetSumOfObjectsToCopy').output.firstRow.count)",
												"type": "Expression"
											},
											"MainControlTableName": {
												"value": "@pipeline().parameters.MainControlTableName",
												"type": "Expression"
											},
											"ConnectionControlTableName": {
												"value": "@pipeline().parameters.ConnectionControlTableName",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"MaxNumberOfObjectsReturnedFromLookupActivity": {
						"type": "Int",
						"defaultValue": 5000
					},
					"MaxNumberOfConcurrentTasks": {
						"type": "Int",
						"defaultValue": 4
					},
					"MainControlTableName": {
						"type": "String",
						"defaultValue": "dbo.MainControlTable_g91"
					},
					"ConnectionControlTableName": {
						"type": "String",
						"defaultValue": "dbo.ConnectionControlTable_g91"
					}
				},
				"folder": {
					"name": "StackOverflowCopyTask_g91_20220729"
				},
				"annotations": [
					"MetadataDrivenSolution"
				],
				"lastPublishTime": "2022-08-23T04:35:50Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/StackOverflowCopyTask_g91_ControlDS')]",
				"[concat(variables('workspaceId'), '/pipelines/StackOverflowCopyTask_g91_MiddleLevel')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflow_MetadataDrivenCopyTask1_cc8_BottomLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will copy objects from one group. The objects belonging to this group will be copied parallelly.",
				"activities": [
					{
						"name": "ListObjectsFromOneGroup",
						"description": "List objects from one group and iterate each of them to downstream activities",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.ObjectsPerGroupToCopy",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "RouteJobsBasedOnLoadingBehavior",
									"description": "Check the loading behavior for each object if it requires full load or incremental load. If it is Default or FullLoad case, do full load. If it is DeltaLoad case, do incremental load.",
									"type": "Switch",
									"dependsOn": [
										{
											"activity": "GetSourceConnectionValues",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"on": {
											"value": "@json(item().DataLoadingBehaviorSettings).dataLoadingBehavior",
											"type": "Expression"
										},
										"cases": [
											{
												"value": "FullLoad",
												"activities": [
													{
														"name": "FullLoadOneObject",
														"description": "Take a full snapshot on this object and copy it to the destination",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "0.12:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [
															{
																"name": "Source",
																"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
															},
															{
																"name": "Destination",
																"value": "@{json(item().SinkObjectSettings).fileSystem}/@{json(item().SinkObjectSettings).folderPath}/@{json(item().SinkObjectSettings).fileName}"
															}
														],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "@json(item().CopySourceSettings).sqlReaderQuery",
																	"type": "Expression"
																},
																"partitionOption": {
																	"value": "@json(item().CopySourceSettings).partitionOption",
																	"type": "Expression"
																},
																"partitionSettings": {
																	"partitionColumnName": {
																		"value": "@json(item().CopySourceSettings).partitionColumnName",
																		"type": "Expression"
																	},
																	"partitionUpperBound": {
																		"value": "@json(item().CopySourceSettings).partitionUpperBound",
																		"type": "Expression"
																	},
																	"partitionLowerBound": {
																		"value": "@json(item().CopySourceSettings).partitionLowerBound",
																		"type": "Expression"
																	},
																	"partitionNames": "@json(item().CopySourceSettings).partitionNames"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"value": "@json(item().CopyActivitySettings).translator",
																"type": "Expression"
															}
														},
														"inputs": [
															{
																"referenceName": "StackOverflow_MetadataDrivenCopyTask1_cc8_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	},
																	"cw_ls_database_name": {
																		"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "StackOverflow_MetadataDrivenCopyTask1_cc8_DestinationDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_fileName": {
																		"value": "@json(item().SinkObjectSettings).fileName",
																		"type": "Expression"
																	},
																	"cw_folderPath": {
																		"value": "@json(item().SinkObjectSettings).folderPath",
																		"type": "Expression"
																	},
																	"cw_fileSystem": {
																		"value": "@json(item().SinkObjectSettings).fileSystem",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "DeltaLoad",
												"activities": [
													{
														"name": "GetMaxWatermarkValue",
														"description": "Query the source object to get the max value from watermark column",
														"type": "Lookup",
														"dependsOn": [],
														"policy": {
															"timeout": "0.12:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "select max([@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}]) as CurrentMaxWaterMarkColumnValue from [@{json(item().SourceObjectSettings).schema}].[@{json(item().SourceObjectSettings).table}]",
																	"type": "Expression"
																},
																"partitionOption": "None"
															},
															"dataset": {
																"referenceName": "StackOverflow_MetadataDrivenCopyTask1_cc8_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	},
																	"cw_ls_database_name": {
																		"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																		"type": "Expression"
																	}
																}
															}
														}
													},
													{
														"name": "DeltaLoadOneObject",
														"description": "Copy the changed data only from last time via comparing the value in watermark column to identify changes.",
														"type": "Copy",
														"dependsOn": [
															{
																"activity": "GetMaxWatermarkValue",
																"dependencyConditions": [
																	"Succeeded"
																]
															}
														],
														"policy": {
															"timeout": "0.12:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [
															{
																"name": "Source",
																"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
															},
															{
																"name": "Destination",
																"value": "@{json(item().SinkObjectSettings).fileSystem}/@{json(item().SinkObjectSettings).folderPath}/@{json(item().SinkObjectSettings).fileName}"
															}
														],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "select * from [@{json(item().SourceObjectSettings).schema}].[@{json(item().SourceObjectSettings).table}] \n    where [@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}] > @{if(contains(json(item().DataLoadingBehaviorSettings).watermarkColumnType, 'Int'),\n    json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue, \n    concat('''', json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue, ''''))}\n    and [@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}] <= @{if(contains(json(item().DataLoadingBehaviorSettings).watermarkColumnType, 'Int'),\n    activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue, \n    concat('''', activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue, ''''))}",
																	"type": "Expression"
																},
																"partitionOption": {
																	"value": "@json(item().CopySourceSettings).partitionOption",
																	"type": "Expression"
																},
																"partitionSettings": {
																	"partitionColumnName": {
																		"value": "@json(item().CopySourceSettings).partitionColumnName",
																		"type": "Expression"
																	},
																	"partitionUpperBound": {
																		"value": "@json(item().CopySourceSettings).partitionUpperBound",
																		"type": "Expression"
																	},
																	"partitionLowerBound": {
																		"value": "@json(item().CopySourceSettings).partitionLowerBound",
																		"type": "Expression"
																	},
																	"partitionNames": "@json(item().CopySourceSettings).partitionNames"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"value": "@json(item().CopyActivitySettings).translator",
																"type": "Expression"
															}
														},
														"inputs": [
															{
																"referenceName": "StackOverflow_MetadataDrivenCopyTask1_cc8_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	},
																	"cw_ls_database_name": {
																		"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "StackOverflow_MetadataDrivenCopyTask1_cc8_DestinationDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_fileName": {
																		"value": "@{json(item().SinkObjectSettings).fileName}-@{json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue}-@{activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue}",
																		"type": "Expression"
																	},
																	"cw_folderPath": {
																		"value": "@json(item().SinkObjectSettings).folderPath",
																		"type": "Expression"
																	},
																	"cw_fileSystem": {
																		"value": "@json(item().SinkObjectSettings).fileSystem",
																		"type": "Expression"
																	}
																}
															}
														]
													},
													{
														"name": "UpdateWatermarkColumnValue",
														"type": "SqlServerStoredProcedure",
														"dependsOn": [
															{
																"activity": "DeltaLoadOneObject",
																"dependencyConditions": [
																	"Succeeded"
																]
															}
														],
														"policy": {
															"timeout": "0.12:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"storedProcedureName": "[dbo].[UpdateWatermarkColumnValue_cc8]",
															"storedProcedureParameters": {
																"Id": {
																	"value": {
																		"value": "@item().Id",
																		"type": "Expression"
																	},
																	"type": "Int32"
																},
																"watermarkColumnStartValue": {
																	"value": {
																		"value": "@activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue",
																		"type": "Expression"
																	},
																	"type": "String"
																}
															}
														},
														"linkedServiceName": {
															"referenceName": "AzureSqlDatabase1",
															"type": "LinkedServiceReference"
														}
													}
												]
											}
										],
										"defaultActivities": [
											{
												"name": "DefaultFullLoadOneObject",
												"description": "Take a full snapshot on this object and copy it to the destination",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "0.12:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [
													{
														"name": "Source",
														"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
													},
													{
														"name": "Destination",
														"value": "@{json(item().SinkObjectSettings).fileSystem}/@{json(item().SinkObjectSettings).folderPath}/@{json(item().SinkObjectSettings).fileName}"
													}
												],
												"typeProperties": {
													"source": {
														"type": "AzureSqlSource",
														"sqlReaderQuery": {
															"value": "@json(item().CopySourceSettings).sqlReaderQuery",
															"type": "Expression"
														},
														"partitionOption": {
															"value": "@json(item().CopySourceSettings).partitionOption",
															"type": "Expression"
														},
														"partitionSettings": {
															"partitionColumnName": {
																"value": "@json(item().CopySourceSettings).partitionColumnName",
																"type": "Expression"
															},
															"partitionUpperBound": {
																"value": "@json(item().CopySourceSettings).partitionUpperBound",
																"type": "Expression"
															},
															"partitionLowerBound": {
																"value": "@json(item().CopySourceSettings).partitionLowerBound",
																"type": "Expression"
															},
															"partitionNames": "@json(item().CopySourceSettings).partitionNames"
														}
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"validateDataConsistency": false,
													"translator": {
														"value": "@json(item().CopyActivitySettings).translator",
														"type": "Expression"
													}
												},
												"inputs": [
													{
														"referenceName": "StackOverflow_MetadataDrivenCopyTask1_cc8_SourceDS",
														"type": "DatasetReference",
														"parameters": {
															"cw_schema": {
																"value": "@json(item().SourceObjectSettings).schema",
																"type": "Expression"
															},
															"cw_table": {
																"value": "@json(item().SourceObjectSettings).table",
																"type": "Expression"
															},
															"cw_ls_database_name": {
																"value": "@json(activity('GetSourceConnectionValues').output.value[0].ConnectionSettings).database_name",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "StackOverflow_MetadataDrivenCopyTask1_cc8_DestinationDS",
														"type": "DatasetReference",
														"parameters": {
															"cw_fileName": {
																"value": "@json(item().SinkObjectSettings).fileName",
																"type": "Expression"
															},
															"cw_folderPath": {
																"value": "@json(item().SinkObjectSettings).folderPath",
																"type": "Expression"
															},
															"cw_fileSystem": {
																"value": "@json(item().SinkObjectSettings).fileSystem",
																"type": "Expression"
															}
														}
													}
												]
											}
										]
									}
								},
								{
									"name": "GetSourceConnectionValues",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "AzureSqlSource",
											"sqlReaderQuery": {
												"value": "select ConnectionSettings from @{pipeline().parameters.ConnectionControlTableName}\n                                where Name = '@{item().SourceConnectionSettingsName}'",
												"type": "Expression"
											},
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "StackOverflow_MetadataDrivenCopyTask1_cc8_ControlDS",
											"type": "DatasetReference",
											"parameters": {}
										},
										"firstRowOnly": false
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"ObjectsPerGroupToCopy": {
						"type": "Array"
					},
					"ConnectionControlTableName": {
						"type": "String"
					}
				},
				"folder": {
					"name": "StackOverflow_MetadataDrivenCopyTask1_cc8_20221208"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/StackOverflow_MetadataDrivenCopyTask1_cc8_ControlDS')]",
				"[concat(variables('workspaceId'), '/datasets/StackOverflow_MetadataDrivenCopyTask1_cc8_SourceDS')]",
				"[concat(variables('workspaceId'), '/datasets/StackOverflow_MetadataDrivenCopyTask1_cc8_DestinationDS')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlDatabase1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflow_MetadataDrivenCopyTask1_cc8_MiddleLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will copy one batch of objects. The objects belonging to this batch will be copied parallelly.",
				"activities": [
					{
						"name": "DivideOneBatchIntoMultipleGroups",
						"description": "Divide objects from single batch into multiple sub parallel groups to avoid reaching the output limit of lookup activity.",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0, add(div(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                    if(equals(mod(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), 0), 0, 1)))",
								"type": "Expression"
							},
							"isSequential": false,
							"batchCount": 50,
							"activities": [
								{
									"name": "GetObjectsPerGroupToCopy",
									"description": "Get objects (tables etc.) from control table required to be copied in this group. The order of objects to be copied following the TaskId in control table (ORDER BY [TaskId] DESC).",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "AzureSqlSource",
											"sqlReaderQuery": {
												"value": "WITH OrderedControlTable AS (\n                             SELECT *, ROW_NUMBER() OVER (ORDER BY [TaskId], [Id] DESC) AS RowNumber\n                             FROM @{pipeline().parameters.MainControlTableName}\n                             where TopLevelPipelineName = '@{pipeline().parameters.TopLevelPipelineName}'\n                             and TriggerName like '%@{pipeline().parameters.TriggerName}%' and CopyEnabled = 1)\n                             SELECT * FROM OrderedControlTable WHERE RowNumber BETWEEN @{add(mul(int(item()),pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                             add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch), 1))}\n                             AND @{min(add(mul(int(item()), pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch),\n                             pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity)),\n                            mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, add(pipeline().parameters.CurrentSequentialNumberOfBatch,1)), pipeline().parameters.SumOfObjectsToCopy)}",
												"type": "Expression"
											},
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "StackOverflow_MetadataDrivenCopyTask1_cc8_ControlDS",
											"type": "DatasetReference",
											"parameters": {}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "CopyObjectsInOneGroup",
									"description": "Execute another pipeline to copy objects from one group. The objects belonging to this group will be copied parallelly.",
									"type": "ExecutePipeline",
									"dependsOn": [
										{
											"activity": "GetObjectsPerGroupToCopy",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "StackOverflow_MetadataDrivenCopyTask1_cc8_BottomLevel",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"ObjectsPerGroupToCopy": {
												"value": "@activity('GetObjectsPerGroupToCopy').output.value",
												"type": "Expression"
											},
											"ConnectionControlTableName": {
												"value": "@pipeline().parameters.ConnectionControlTableName",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"MaxNumberOfObjectsReturnedFromLookupActivity": {
						"type": "Int"
					},
					"TopLevelPipelineName": {
						"type": "String"
					},
					"TriggerName": {
						"type": "String"
					},
					"CurrentSequentialNumberOfBatch": {
						"type": "Int"
					},
					"SumOfObjectsToCopy": {
						"type": "Int"
					},
					"SumOfObjectsToCopyForCurrentBatch": {
						"type": "Int"
					},
					"MainControlTableName": {
						"type": "String"
					},
					"ConnectionControlTableName": {
						"type": "String"
					}
				},
				"folder": {
					"name": "StackOverflow_MetadataDrivenCopyTask1_cc8_20221208"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/StackOverflow_MetadataDrivenCopyTask1_cc8_ControlDS')]",
				"[concat(variables('workspaceId'), '/pipelines/StackOverflow_MetadataDrivenCopyTask1_cc8_BottomLevel')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflow_MetadataDrivenCopyTask1_cc8_TopLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will count the total number of objects (tables etc.) required to be copied in this run, come up with the number of sequential batches based on the max allowed concurrent copy task, and then execute another pipeline to copy different batches sequentially.",
				"activities": [
					{
						"name": "GetSumOfObjectsToCopy",
						"description": "Count the total number of objects (tables etc.) required to be copied in this run.",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": {
									"value": "SELECT count(*) as count FROM @{pipeline().parameters.MainControlTableName} where TopLevelPipelineName='@{pipeline().Pipeline}' and TriggerName like '%@{pipeline().TriggerName}%' and CopyEnabled = 1",
									"type": "Expression"
								},
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "StackOverflow_MetadataDrivenCopyTask1_cc8_ControlDS",
								"type": "DatasetReference",
								"parameters": {}
							}
						}
					},
					{
						"name": "CopyBatchesOfObjectsSequentially",
						"description": "Come up with the number of sequential batches based on the max allowed concurrent copy tasks, and then execute another pipeline to copy different batches sequentially.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "GetSumOfObjectsToCopy",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0, add(div(activity('GetSumOfObjectsToCopy').output.firstRow.count,\n                    pipeline().parameters.MaxNumberOfConcurrentTasks),\n                    if(equals(mod(activity('GetSumOfObjectsToCopy').output.firstRow.count,\n                    pipeline().parameters.MaxNumberOfConcurrentTasks), 0), 0, 1)))",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "CopyObjectsInOneBatch",
									"description": "Execute another pipeline to copy one batch of objects. The objects belonging to this batch will be copied parallelly.",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "StackOverflow_MetadataDrivenCopyTask1_cc8_MiddleLevel",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"MaxNumberOfObjectsReturnedFromLookupActivity": {
												"value": "@pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity",
												"type": "Expression"
											},
											"TopLevelPipelineName": {
												"value": "@{pipeline().Pipeline}",
												"type": "Expression"
											},
											"TriggerName": {
												"value": "@{pipeline().TriggerName}",
												"type": "Expression"
											},
											"CurrentSequentialNumberOfBatch": {
												"value": "@item()",
												"type": "Expression"
											},
											"SumOfObjectsToCopy": {
												"value": "@activity('GetSumOfObjectsToCopy').output.firstRow.count",
												"type": "Expression"
											},
											"SumOfObjectsToCopyForCurrentBatch": {
												"value": "@min(pipeline().parameters.MaxNumberOfConcurrentTasks, activity('GetSumOfObjectsToCopy').output.firstRow.count)",
												"type": "Expression"
											},
											"MainControlTableName": {
												"value": "@pipeline().parameters.MainControlTableName",
												"type": "Expression"
											},
											"ConnectionControlTableName": {
												"value": "@pipeline().parameters.ConnectionControlTableName",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"MaxNumberOfObjectsReturnedFromLookupActivity": {
						"type": "Int",
						"defaultValue": 5000
					},
					"MaxNumberOfConcurrentTasks": {
						"type": "Int",
						"defaultValue": 20
					},
					"MainControlTableName": {
						"type": "String",
						"defaultValue": "dbo.MainControlTable_cc8"
					},
					"ConnectionControlTableName": {
						"type": "String",
						"defaultValue": "dbo.ConnectionControlTable_cc8"
					}
				},
				"folder": {
					"name": "StackOverflow_MetadataDrivenCopyTask1_cc8_20221208"
				},
				"annotations": [
					"MetadataDrivenSolution"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/StackOverflow_MetadataDrivenCopyTask1_cc8_ControlDS')]",
				"[concat(variables('workspaceId'), '/pipelines/StackOverflow_MetadataDrivenCopyTask1_cc8_MiddleLevel')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/stackoverflow_main')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "StackOverflow_API_Ingest",
						"type": "DatabricksNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebookPath": "/Repos/training@dustinvannoy.com/datakickstart-databricks-workspace/stackoverflow/stackoverflow_ingest_api"
						},
						"linkedServiceName": {
							"referenceName": "datakickstart_databricks_ls",
							"type": "LinkedServiceReference"
						}
					},
					{
						"name": "StackOverflow_File_Ingest",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "stackoverflow_ingest_adls",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkLogging",
								"type": "BigDataPoolReference"
							},
							"executorSize": null,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": null,
							"numExecutors": null
						}
					},
					{
						"name": "StackOverflowCopyTask_g91_TopLevel",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "StackOverflow_API_Ingest",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "StackOverflow_File_Ingest",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "StackOverflowCopyTask_g91_TopLevel",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "Refined_table_create",
						"type": "Script",
						"dependsOn": [
							{
								"activity": "StackOverflowCopyTask_g91_TopLevel",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"linkedServiceName": {
							"referenceName": "synapse_serverless_datakickstart_ls",
							"type": "LinkedServiceReference"
						},
						"typeProperties": {
							"scripts": [
								{
									"type": "Query",
									"text": {
										"value": "IF  EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[dbo].[fact_stackoverflow_post]') AND type in (N'U'))\nDROP EXTERNAL TABLE demo.dbo.fact_stackoverflow_post;\n\nCREATE EXTERNAL TABLE demo.dbo.fact_stackoverflow_post \nWITH (\n    LOCATION = 'stackoverflow/fact_stackoverflow_@{pipeline().TriggerTime}',  \n    DATA_SOURCE = refined_datakickstartadls,  \n    FILE_FORMAT = ParquetFormat  \n)\nAS\nselect * from raw_stackoverflow.dbo.posts;",
										"type": "Expression"
									}
								}
							],
							"logSettings": {
								"logDestination": "ExternalStore",
								"logLocationSettings": {
									"linkedServiceName": {
										"referenceName": "LogOutput_datakickstartadls",
										"type": "LinkedServiceReference"
									},
									"path": "demo/synapse_logs"
								}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-08-23T04:35:54Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/datakickstart_databricks_ls')]",
				"[concat(variables('workspaceId'), '/notebooks/stackoverflow_ingest_adls')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sparkLogging')]",
				"[concat(variables('workspaceId'), '/pipelines/StackOverflowCopyTask_g91_TopLevel')]",
				"[concat(variables('workspaceId'), '/linkedServices/synapse_serverless_datakickstart_ls')]",
				"[concat(variables('workspaceId'), '/linkedServices/LogOutput_datakickstartadls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlDatabase1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "LocationId",
						"type": "int",
						"precision": 10
					},
					{
						"name": "Borough",
						"type": "varchar"
					},
					{
						"name": "Zone",
						"type": "varchar"
					},
					{
						"name": "service_zone",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "taxi_zone"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlDatabase1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlTable2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "sandboxsqlserverless",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "key",
						"type": "nvarchar"
					},
					{
						"name": "offset",
						"type": "bigint",
						"precision": 19
					},
					{
						"name": "timestamp",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "usageId",
						"type": "int",
						"precision": 10
					},
					{
						"name": "user",
						"type": "nvarchar"
					},
					{
						"name": "completed",
						"type": "bit"
					},
					{
						"name": "durationSeconds",
						"type": "int",
						"precision": 10
					},
					{
						"name": "eventTimestamp",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "video_view"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/sandboxsqlserverless')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlTable3')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "sandboxsqlserverless",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "key",
						"type": "nvarchar"
					},
					{
						"name": "offset",
						"type": "bigint",
						"precision": 19
					},
					{
						"name": "timestamp",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "membershipId",
						"type": "int",
						"precision": 10
					},
					{
						"name": "user",
						"type": "nvarchar"
					},
					{
						"name": "planId",
						"type": "nvarchar"
					},
					{
						"name": "startDate",
						"type": "nvarchar"
					},
					{
						"name": "endDate",
						"type": "nvarchar"
					},
					{
						"name": "updatedAt",
						"type": "nvarchar"
					},
					{
						"name": "eventTimestamp",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "member"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/sandboxsqlserverless')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "dvtrainingadls",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "taxi_zone_lookup.csv",
						"folderPath": "nyctaxi/lookups",
						"fileSystem": "demo"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "LocationID",
						"type": "String"
					},
					{
						"name": "Borough",
						"type": "String"
					},
					{
						"name": "Zone",
						"type": "String"
					},
					{
						"name": "service_zone",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/dvtrainingadls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_69u')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SnowflakeDemo_YellowTrips",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "SnowflakeTable",
				"schema": [
					{
						"name": "YEARMONTH",
						"type": "VARCHAR",
						"precision": 0,
						"scale": 0
					},
					{
						"name": "VENDORID",
						"type": "VARCHAR",
						"precision": 0,
						"scale": 0
					},
					{
						"name": "TPEPPICKUPDATETIME",
						"type": "TIMESTAMP_NTZ",
						"precision": 29,
						"scale": 9
					},
					{
						"name": "TPEPDROPOFFDATETIME",
						"type": "TIMESTAMP_NTZ",
						"precision": 29,
						"scale": 9
					},
					{
						"name": "PASSENGERCOUNT",
						"type": "NUMBER",
						"precision": 38,
						"scale": 0
					},
					{
						"name": "TRIPDISTANCE",
						"type": "FLOAT",
						"precision": 15,
						"scale": 0
					},
					{
						"name": "RATECODEID",
						"type": "NUMBER",
						"precision": 38,
						"scale": 0
					},
					{
						"name": "STOREANDFWDFLAG",
						"type": "VARCHAR",
						"precision": 0,
						"scale": 0
					},
					{
						"name": "PULOCATIONID",
						"type": "NUMBER",
						"precision": 38,
						"scale": 0
					},
					{
						"name": "DOLOCATIONID",
						"type": "NUMBER",
						"precision": 38,
						"scale": 0
					},
					{
						"name": "PAYMENTTYPE",
						"type": "NUMBER",
						"precision": 38,
						"scale": 0
					},
					{
						"name": "FAREAMOUNT",
						"type": "FLOAT",
						"precision": 15,
						"scale": 0
					},
					{
						"name": "EXTRA",
						"type": "FLOAT",
						"precision": 15,
						"scale": 0
					},
					{
						"name": "MTATAX",
						"type": "FLOAT",
						"precision": 15,
						"scale": 0
					},
					{
						"name": "TOLLSAMOUNT",
						"type": "FLOAT",
						"precision": 15,
						"scale": 0
					},
					{
						"name": "IMPROVEMENTSURCHARGE",
						"type": "VARCHAR",
						"precision": 0,
						"scale": 0
					},
					{
						"name": "TOTALAMOUNT",
						"type": "FLOAT",
						"precision": 15,
						"scale": 0
					},
					{
						"name": "PICKUPDT",
						"type": "DATE",
						"precision": 10,
						"scale": 0
					},
					{
						"name": "DROPOFFDT",
						"type": "DATE",
						"precision": 10,
						"scale": 0
					}
				],
				"typeProperties": {
					"schema": "PUBLIC",
					"table": "TRIPS_YELLOW"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SnowflakeDemo_YellowTrips')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_avi')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SnowflakeDemo_YellowTrips",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "SnowflakeTable",
				"schema": [],
				"typeProperties": {
					"schema": "PUBLIC",
					"table": "TRIPS_YELLOW_PARQUET"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SnowflakeDemo_YellowTrips')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_w2k')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SnowflakeDemo_YellowTrips",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "SnowflakeTable",
				"schema": [],
				"typeProperties": {
					"schema": "PUBLIC",
					"table": "TRIPS_YELLOW_VARIANT"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SnowflakeDemo_YellowTrips')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MetadataDrivenCopyTask_q6h_ControlDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "sandboxsqlserverless",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_q6h_20220818"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "MainControlTable_q6h"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/sandboxsqlserverless')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MetadataDrivenCopyTask_q6h_DestinationDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "datakickstartadls_ls",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_fileName": {
						"type": "String"
					},
					"cw_folderPath": {
						"type": "String"
					},
					"cw_fileSystem": {
						"type": "String"
					}
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_q6h_20220818"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().cw_fileName",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().cw_folderPath",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().cw_fileSystem",
							"type": "Expression"
						}
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/datakickstartadls_ls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MetadataDrivenCopyTask_q6h_SourceDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlStackoverflow",
					"type": "LinkedServiceReference",
					"parameters": {
						"database_name": {
							"value": "@dataset().cw_ls_database_name",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"cw_schema": {
						"type": "String"
					},
					"cw_table": {
						"type": "String"
					},
					"cw_ls_database_name": {
						"type": "String"
					}
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_q6h_20220818"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().cw_schema",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().cw_table",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlStackoverflow')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MetadataDrivenCopyTask_skn_ControlDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlDatabase1",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_skn_20221208"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"table": "MainControlTable_skn"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlDatabase1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MetadataDrivenCopyTask_skn_DestinationDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "datakickstart-synapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_fileName": {
						"type": "String"
					},
					"cw_folderPath": {
						"type": "String"
					},
					"cw_fileSystem": {
						"type": "String"
					}
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_skn_20221208"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().cw_fileName",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().cw_folderPath",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().cw_fileSystem",
							"type": "Expression"
						}
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/datakickstart-synapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MetadataDrivenCopyTask_skn_SourceDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlStackoverflow",
					"type": "LinkedServiceReference",
					"parameters": {
						"database_name": {
							"value": "@dataset().cw_ls_database_name",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"cw_schema": {
						"type": "String"
					},
					"cw_table": {
						"type": "String"
					},
					"cw_ls_database_name": {
						"type": "String"
					}
				},
				"folder": {
					"name": "MetadataDrivenCopyTask_skn_20221208"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().cw_schema",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().cw_table",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlStackoverflow')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_69u')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "datakickstartadls_ls",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "nyctaxi/tripdata/yellow_parquet",
						"fileSystem": "demo"
					},
					"compressionCodec": "snappy"
				},
				"schema": [
					{
						"name": "vendorID",
						"type": "UTF8"
					},
					{
						"name": "tpepPickupDateTime",
						"type": "INT96"
					},
					{
						"name": "tpepDropoffDateTime",
						"type": "INT96"
					},
					{
						"name": "passengerCount",
						"type": "INT32"
					},
					{
						"name": "tripDistance",
						"type": "DOUBLE"
					},
					{
						"name": "puLocationId",
						"type": "UTF8"
					},
					{
						"name": "doLocationId",
						"type": "UTF8"
					},
					{
						"name": "startLon",
						"type": "DOUBLE"
					},
					{
						"name": "startLat",
						"type": "DOUBLE"
					},
					{
						"name": "endLon",
						"type": "DOUBLE"
					},
					{
						"name": "endLat",
						"type": "DOUBLE"
					},
					{
						"name": "rateCodeId",
						"type": "INT32"
					},
					{
						"name": "storeAndFwdFlag",
						"type": "UTF8"
					},
					{
						"name": "paymentType",
						"type": "UTF8"
					},
					{
						"name": "fareAmount",
						"type": "DOUBLE"
					},
					{
						"name": "extra",
						"type": "DOUBLE"
					},
					{
						"name": "mtaTax",
						"type": "DOUBLE"
					},
					{
						"name": "improvementSurcharge",
						"type": "UTF8"
					},
					{
						"name": "tipAmount",
						"type": "DOUBLE"
					},
					{
						"name": "tollsAmount",
						"type": "DOUBLE"
					},
					{
						"name": "totalAmount",
						"type": "DOUBLE"
					},
					{
						"name": "pickupDt",
						"type": "DATE"
					},
					{
						"name": "dropoffDt",
						"type": "DATE"
					},
					{
						"name": "tipPct",
						"type": "DOUBLE"
					},
					{
						"name": "Borough",
						"type": "UTF8"
					},
					{
						"name": "PickupZone",
						"type": "UTF8"
					},
					{
						"name": "PickupServiceZone",
						"type": "UTF8"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/datakickstartadls_ls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_avi')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "dvtrainingadls_blobsas_ls",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "nyctaxi/tripdata/yellow_parquet",
						"container": "demo"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/dvtrainingadls_blobsas_ls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_w2k')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "dvtrainingadls_blobsas_ls",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "nyctaxi/tripdata/yellow_parquet",
						"container": "demo"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/dvtrainingadls_blobsas_ls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflowCopyTask_g91_ControlDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "sandboxsqlserverless",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "StackOverflowCopyTask_g91_20220729"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "MainControlTable_g91"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/sandboxsqlserverless')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflowCopyTask_g91_DestinationDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "datakickstartadls_ls",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_fileName": {
						"type": "String"
					},
					"cw_folderPath": {
						"type": "String"
					},
					"cw_fileSystem": {
						"type": "String"
					}
				},
				"folder": {
					"name": "StackOverflowCopyTask_g91_20220729"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().cw_fileName",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().cw_folderPath",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().cw_fileSystem",
							"type": "Expression"
						}
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/datakickstartadls_ls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflowCopyTask_g91_SourceDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "StackOverflowTest2",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_schema": {
						"type": "String"
					},
					"cw_table": {
						"type": "String"
					},
					"cw_ls_database_name": {
						"type": "String"
					}
				},
				"folder": {
					"name": "StackOverflowCopyTask_g91_20220729"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().cw_schema",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().cw_table",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/StackOverflowTest2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflowCopy_Dynamic_bnd_ControlDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "sandboxsqlserverless",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "StackOverflowCopy_Dynamic_bnd_20220728"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "StackOverflowControlTable"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/sandboxsqlserverless')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflowCopy_Dynamic_bnd_DestinationDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "datakickstartadls_ls",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_fileName": {
						"type": "String"
					},
					"cw_folderPath": {
						"type": "String"
					},
					"cw_fileSystem": {
						"type": "String"
					}
				},
				"folder": {
					"name": "StackOverflowCopy_Dynamic_bnd_20220728"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().cw_fileName",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().cw_folderPath",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().cw_fileSystem",
							"type": "Expression"
						}
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/datakickstartadls_ls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflowCopy_Dynamic_bnd_SourceDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "StackOverflow2",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_schema": {
						"type": "String"
					},
					"cw_table": {
						"type": "String"
					},
					"cw_ls_source_database": {
						"type": "String"
					}
				},
				"folder": {
					"name": "StackOverflowCopy_Dynamic_bnd_20220728"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().cw_schema",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().cw_table",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/StackOverflow2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflow_Dynamic_test2_813_ControlDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "sandboxsqlserverless",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "StackOverflow_Dynamic_test2_813_20220726"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "ControlStackOverflow_813"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/sandboxsqlserverless')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflow_Dynamic_test2_813_DestinationDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "datakickstartadls_ls",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_fileName": {
						"type": "String"
					},
					"cw_folderPath": {
						"type": "String"
					},
					"cw_fileSystem": {
						"type": "String"
					}
				},
				"folder": {
					"name": "StackOverflow_Dynamic_test2_813_20220726"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().cw_fileName",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().cw_folderPath",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().cw_fileSystem",
							"type": "Expression"
						}
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/datakickstartadls_ls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflow_Dynamic_test2_813_SourceDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlStackoverflow",
					"type": "LinkedServiceReference",
					"parameters": {
						"database_name": {
							"value": "@dataset().cw_ls_database_name",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"cw_schema": {
						"type": "String"
					},
					"cw_table": {
						"type": "String"
					},
					"cw_ls_database_name": {
						"type": "String"
					}
				},
				"folder": {
					"name": "StackOverflow_Dynamic_test2_813_20220726"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().cw_schema",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().cw_table",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlStackoverflow')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflow_MetadataDrivenCopyTask1_cc8_ControlDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlDatabase1",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "StackOverflow_MetadataDrivenCopyTask1_cc8_20221208"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "MainControlTable_cc8"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlDatabase1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflow_MetadataDrivenCopyTask1_cc8_DestinationDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "datakickstart-synapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_fileName": {
						"type": "String"
					},
					"cw_folderPath": {
						"type": "String"
					},
					"cw_fileSystem": {
						"type": "String"
					}
				},
				"folder": {
					"name": "StackOverflow_MetadataDrivenCopyTask1_cc8_20221208"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().cw_fileName",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().cw_folderPath",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().cw_fileSystem",
							"type": "Expression"
						}
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/datakickstart-synapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflow_MetadataDrivenCopyTask1_cc8_SourceDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlStackoverflow",
					"type": "LinkedServiceReference",
					"parameters": {
						"database_name": {
							"value": "@dataset().cw_ls_database_name",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"cw_schema": {
						"type": "String"
					},
					"cw_table": {
						"type": "String"
					},
					"cw_ls_database_name": {
						"type": "String"
					}
				},
				"folder": {
					"name": "StackOverflow_MetadataDrivenCopyTask1_cc8_20221208"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().cw_schema",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().cw_table",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlStackoverflow')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackoverflowSql_Dynamic_xg8_ControlDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "sandboxsqlserverless",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "StackoverflowSql_Dynamic_xg8_20220726"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "ControlStackoverflowTest1"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/sandboxsqlserverless')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackoverflowSql_Dynamic_xg8_DestinationDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "datakickstartadls_ls",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_fileName": {
						"type": "String"
					},
					"cw_folderPath": {
						"type": "String"
					},
					"cw_fileSystem": {
						"type": "String"
					}
				},
				"folder": {
					"name": "StackoverflowSql_Dynamic_xg8_20220726"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().cw_fileName",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().cw_folderPath",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().cw_fileSystem",
							"type": "Expression"
						}
					},
					"compressionCodec": "snappy",
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/datakickstartadls_ls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackoverflowSql_Dynamic_xg8_SourceDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlStackoverflow",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_schema": {
						"type": "String"
					},
					"cw_table": {
						"type": "String"
					},
					"cw_ls_database_name": {
						"type": "String"
					}
				},
				"folder": {
					"name": "StackoverflowSql_Dynamic_xg8_20220726"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().cw_schema",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().cw_table",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlStackoverflow')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SynapseServerless_datakickstart')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "synapse_serverless_datakickstart_ls",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "vendorID",
						"type": "nvarchar"
					},
					{
						"name": "tpepPickupDateTime",
						"type": "datetime2",
						"scale": 7
					},
					{
						"name": "tpepDropoffDateTime",
						"type": "datetime2",
						"scale": 7
					},
					{
						"name": "passengerCount",
						"type": "int",
						"precision": 10
					},
					{
						"name": "tripDistance",
						"type": "float",
						"precision": 15
					},
					{
						"name": "puLocationId",
						"type": "nvarchar"
					},
					{
						"name": "doLocationId",
						"type": "nvarchar"
					},
					{
						"name": "startLon",
						"type": "float",
						"precision": 15
					},
					{
						"name": "startLat",
						"type": "float",
						"precision": 15
					},
					{
						"name": "endLon",
						"type": "float",
						"precision": 15
					},
					{
						"name": "endLat",
						"type": "float",
						"precision": 15
					},
					{
						"name": "rateCodeId",
						"type": "int",
						"precision": 10
					},
					{
						"name": "storeAndFwdFlag",
						"type": "nvarchar"
					},
					{
						"name": "paymentType",
						"type": "nvarchar"
					},
					{
						"name": "fareAmount",
						"type": "float",
						"precision": 15
					},
					{
						"name": "extra",
						"type": "float",
						"precision": 15
					},
					{
						"name": "mtaTax",
						"type": "float",
						"precision": 15
					},
					{
						"name": "improvementSurcharge",
						"type": "nvarchar"
					},
					{
						"name": "tipAmount",
						"type": "float",
						"precision": 15
					},
					{
						"name": "tollsAmount",
						"type": "float",
						"precision": 15
					},
					{
						"name": "totalAmount",
						"type": "float",
						"precision": 15
					},
					{
						"name": "pickupDt",
						"type": "date"
					},
					{
						"name": "dropoffDt",
						"type": "date"
					},
					{
						"name": "tipPct",
						"type": "float",
						"precision": 15
					},
					{
						"name": "Borough",
						"type": "nvarchar"
					},
					{
						"name": "PickupZone",
						"type": "nvarchar"
					},
					{
						"name": "PickupServiceZone",
						"type": "nvarchar"
					}
				],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/synapse_serverless_datakickstart_ls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/adls_datakickstart_parquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "datakickstart-synapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "output",
						"fileSystem": "demo"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/datakickstart-synapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlDatabase1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AzureSqlDatabase1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlStackoverflow')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"database_name": {
						"type": "string",
						"defaultValue": "StackOverflow2010"
					}
				},
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AzureSqlStackoverflow_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "demokv",
							"type": "LinkedServiceReference"
						},
						"secretName": "dustin-demo-sqldb-password"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/demokv')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LogOutput_datakickstartadls')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"serviceEndpoint": "[parameters('LogOutput_datakickstartadls_properties_typeProperties_serviceEndpoint')]",
					"accountKind": "StorageV2"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Snowflake1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "Snowflake",
				"typeProperties": {
					"authenticationType": "Basic",
					"connectionString": "[parameters('Snowflake1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SnowflakeDemo_YellowTrips')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "Snowflake",
				"typeProperties": {
					"authenticationType": "Basic",
					"connectionString": "[parameters('SnowflakeDemo_YellowTrips_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflow2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"source_database": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('StackOverflow2_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "demokv",
							"type": "LinkedServiceReference"
						},
						"secretName": "dustin-demo-sqldb-password"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/demokv')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StackOverflowTest2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"database_name": {
						"type": "string",
						"defaultValue": "StackOverflow2010"
					}
				},
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('StackOverflowTest2_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "demokv",
							"type": "LinkedServiceReference"
						},
						"secretName": "dustin-demo-sqldb-password"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/demokv')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/datakickstart-synapse-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('datakickstart-synapse-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/datakickstart-synapse-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('datakickstart-synapse-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/datakickstart_databricks_ls')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDatabricks",
				"typeProperties": {
					"domain": "https://adb-1125862267992199.19.azuredatabricks.net",
					"accessToken": {
						"type": "SecureString",
						"value": "[parameters('datakickstart_databricks_ls_accessToken')]"
					},
					"newClusterNodeType": "Standard_DS3_v2",
					"newClusterNumOfWorker": "2",
					"newClusterSparkEnvVars": {
						"PYSPARK_PYTHON": "/databricks/python3/bin/python3"
					},
					"newClusterVersion": "10.4.x-scala2.12",
					"clusterOption": "Fixed",
					"newClusterInitScripts": []
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/datakickstartadls2_ls')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('datakickstartadls2_ls_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/datakickstartadls_ls')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('datakickstartadls_ls_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/demokv')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('demokv_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dvtrainingadls')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('dvtrainingadls_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('dvtrainingadls_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dvtrainingadls_blobsas_ls')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "[parameters('dvtrainingadls_blobsas_ls_sasUri')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nyc_tlc_yellow')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "[parameters('nyc_tlc_yellow_sasUri')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sandboxsqlserverless')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('sandboxsqlserverless_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapse_serverless_datakickstart_ls')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('synapse_serverless_datakickstart_ls_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DatacenterIntegrationRuntime1')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow1')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable2",
								"type": "DatasetReference"
							},
							"name": "AzureSQLVideos"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "datakickstartadls2_ls",
								"type": "LinkedServiceReference"
							},
							"name": "VideoViewCDCsink"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          key as string,",
						"          offset as long,",
						"          timestamp as timestamp,",
						"          usageId as integer,",
						"          user as string,",
						"          completed as boolean,",
						"          durationSeconds as integer,",
						"          eventTimestamp as timestamp",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     enableCdc: true,",
						"     mode: 'read',",
						"     skipInitialLoad: false,",
						"     waterMarkColumn: 'offset',",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> AzureSQLVideos",
						"AzureSQLVideos sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'delta',",
						"     fileSystem: 'sources',",
						"     folderPath: 'video_usage/video_view_cdc',",
						"     mergeSchema: false,",
						"     autoCompact: false,",
						"     optimizedWrite: false,",
						"     vacuum: 0,",
						"     deletable: false,",
						"     insertable: true,",
						"     updateable: false,",
						"     upsertable: false,",
						"     umask: 0666,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> VideoViewCDCsink"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/AzureSqlTable2')]",
				"[concat(variables('workspaceId'), '/linkedServices/datakickstartadls2_ls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow2')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable3",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "datakickstartadls2_ls",
								"type": "LinkedServiceReference"
							},
							"name": "membercdcsink"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          key as string,",
						"          offset as long,",
						"          timestamp as timestamp,",
						"          membershipId as integer,",
						"          user as string,",
						"          planId as string,",
						"          startDate as string,",
						"          endDate as string,",
						"          updatedAt as string,",
						"          eventTimestamp as timestamp",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> source1",
						"source1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'delta',",
						"     fileSystem: 'sources',",
						"     folderPath: 'video_usage/member_cdc',",
						"     mergeSchema: false,",
						"     autoCompact: false,",
						"     optimizedWrite: false,",
						"     vacuum: 0,",
						"     deletable: false,",
						"     insertable: true,",
						"     updateable: false,",
						"     upsertable: false,",
						"     umask: 0666,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> membercdcsink"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/AzureSqlTable3')]",
				"[concat(variables('workspaceId'), '/linkedServices/datakickstartadls2_ls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CU_ServerlessExternal_Raw')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "USE demo\nGO\n\nCREATE LOGIN [test@trainingdustinvannoy.onmicrosoft.com] FROM EXTERNAL PROVIDER;\nALTER SERVER ROLE sysadmin ADD MEMBER [test1];\n\n-- In your database\nCREATE USER test1 FROM LOGIN [test@trainingdustinvannoy.onmicrosoft.com];\nALTER ROLE db_datareader ADD member test1; \n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'raw_datakickstartadls') \n\tCREATE EXTERNAL DATA SOURCE [raw_datakickstartadls] \n\tWITH (\n\t\tLOCATION = 'abfss://raw@datakickstartadls.dfs.core.windows.net' \n\t)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'ParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT ParquetFormat WITH (  FORMAT_TYPE = PARQUET );\nGO\nIF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'DeltaFormat') \n\tCREATE EXTERNAL FILE FORMAT DeltaLakeFormat WITH (  FORMAT_TYPE = DELTA );\nGO\n\nDROP EXTERNAL TABLE area_raw;\n\nCREATE EXTERNAL TABLE area_raw (\n\t[area_code] nvarchar(4000),\n\t[area_name] nvarchar(4000),\n\t[display_level] bigint\n\t)\n\tWITH (\n\tLOCATION = 'cu/area/*.parquet',\n\tDATA_SOURCE = raw_datakickstartadls,\n\tFILE_FORMAT = ParquetFormat\n\t)\nGO\n\nDROP EXTERNAL TABLE current_raw;\n\nCREATE EXTERNAL TABLE current_raw (\n\t[series_id] nvarchar(4000),\n\t[year] bigint,\n\t[period] nvarchar(4000),\n\t[value] float\n\t)\n\tWITH (\n\tLOCATION = 'cu/current/**',\n\tDATA_SOURCE = raw_datakickstartadls,\n\tFILE_FORMAT = ParquetFormat\n\t)\nGO\n\n\nCREATE EXTERNAL TABLE series_raw \n\tWITH (\n\tLOCATION = 'cu/series/**',\n\tDATA_SOURCE = raw_datakickstartadls,\n\tFILE_FORMAT = ParquetFormat\n\t)\nGO\n\nSELECT TOP 100 * FROM dbo.area_raw\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "demo",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Grant access to a user to a single serverless SQL pool database')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--1.a Create LOGIN for an Azure AD account.To create a login, you must be connected to the master database.\n\nCREATE USER [alias@domain.com] FROM EXTERNAL PROVIDER;\n\n\n--1.b Create a Login for a SQL authuser. To create a login, you must be connected to the master database.\nCREATE LOGIN <login_alias> WITH PASSWORD = 'enter_your_password';\nCREATE USER <user_alias> FROM LOGIN <login_alias>\n\n--2.a Create USER for the Azure AD account\n\nuse yourdb -- Use your DB name\nCREATE USER <user_alias> FROM LOGIN [alias@domain.com]\n\n--2.b Create USER for the SQL Auth account\nuse yourdb -- Use your DB name\nCREATE USER <user_alias> FROM LOGIN <login_alias>\n\n--3.Add USER to members of the specified role\n\nuse yourdb -- Use your DB name\ngo\nalter role db_owner Add MEMBER <user_alias> -- Type user alias from step 2\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ServerlessViews_CREATE_PROC')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "USE demo\nGO\n\nCREATE OR ALTER PROC CreateView @db_folder nvarchar(50), @name nvarchar(50)\nAS\nBEGIN\n\nDECLARE @statement VARCHAR(MAX)\nSET @statement = N'CREATE OR ALTER VIEW [' + @name + '] AS\n        SELECT *\n        FROM OPENROWSET(\n            BULK ''https://datakickstartadls.dfs.core.windows.net/raw/' + @db_folder + '/' + @name + '/*.parquet'',\n            FORMAT=''Parquet''\n        ) AS [result]\n'\n\nEXEC (@statement)\n\nEND\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "demo",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ServerlessViews_EXEC')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "USE demo\nGO\n\nEXEC CreateView 'cu', 'current';",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "demo",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/data_lake_serverless_analytics')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "USE demo\nGO\n\nSELECT \n  paymentType,\n  count(1) recordCount,\n  avg(tipAmount) avgTip\nFROM yellow_trips_delta\nWHERE paymentType is not null\n--and year_month='2018_12'\nGROUP BY paymentType\nORDER BY recordCount desc\n\n\nSELECT \n  cast(tpepPickupDatetime as date) PickupDate,\n  PickupZone,\n  Borough,\n  count(1) recordCount,\n  avg(tipAmount) avgTip\nFROM yellow_trips_delta\nWHERE paymentType is not null\nGROUP BY \n  cast(tpepPickupDatetime as date),\n  PickupZone,\n  Borough\n\nSELECT \n  pickupDt,\n  count(1) recordCount,\n  avg(tipAmount) avgTip\nFROM yellow_trips_parquet\nWHERE paymentType is not null\nGROUP BY \n  pickupDt\n\n\nSELECT TOP 10 *\nFROM yellow_trips_delta\nWHERE tpepPickupDatetime between '2018-12-01' and '2018-12-02'\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "demo",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/data_lake_serverless_setup')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "USE demo\nGO\n\n-- Select data using OPENROWSET (from Synapse autogenerated script)\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datakickstartadls.dfs.core.windows.net/demo/nyctaxi/tripdata/yellow_delta/',\n        FORMAT = 'DELTA'\n    ) AS [result]\n\n\n-- Create external table for easy use. Data source and formats must be created the first time (from Synapse autogenerated script)\nIF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'demo_datakickstartadls_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [demo_datakickstartadls_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://demo@datakickstartadls.dfs.core.windows.net' \n\t)\nGO\n\nIF EXISTS (SELECT * FROM sys.external_tables WHERE name = 'yellow_trips_delta')\n\tDROP EXTERNAL TABLE yellow_trips_delta;\nGO\n\n\nCREATE EXTERNAL TABLE yellow_trips_delta (\n\t[vendorID] nvarchar(4000),\n\t[tpepPickupDateTime] datetime2(7),\n\t[tpepDropoffDateTime] datetime2(7),\n\t[passengerCount] int,\n\t[tripDistance] float,\n\t[puLocationId] nvarchar(4000),\n\t[doLocationId] nvarchar(4000),\n\t[startLon] float,\n\t[startLat] float,\n\t[endLon] float,\n\t[endLat] float,\n\t[rateCodeId] int,\n\t[storeAndFwdFlag] nvarchar(4000),\n\t[paymentType] nvarchar(4000),\n\t[fareAmount] float,\n\t[extra] float,\n\t[mtaTax] float,\n\t[improvementSurcharge] nvarchar(4000),\n\t[tipAmount] float,\n\t[tollsAmount] float,\n\t[totalAmount] float,\n\t[pickupDt] date,\n\t[dropoffDt] date,\n\t[tipPct] float,\n\t[Borough] nvarchar(4000),\n\t[PickupZone] nvarchar(4000),\n\t[PickupServiceZone] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'nyctaxi/tripdata/yellow_delta/**',\n\tDATA_SOURCE = [demo_datakickstartadls_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.yellow_trips_delta\nGO\n\n\nIF EXISTS (SELECT * FROM sys.external_tables WHERE name = 'yellow_trips_parquet')\n\tDROP EXTERNAL TABLE yellow_trips_parquet;\nGO\n\n\nCREATE EXTERNAL TABLE yellow_trips_parquet (\n\t[vendorID] nvarchar(4000),\n\t[tpepPickupDateTime] datetime2(7),\n\t[tpepDropoffDateTime] datetime2(7),\n\t[passengerCount] int,\n\t[tripDistance] float,\n\t[puLocationId] nvarchar(4000),\n\t[doLocationId] nvarchar(4000),\n\t[startLon] float,\n\t[startLat] float,\n\t[endLon] float,\n\t[endLat] float,\n\t[rateCodeId] int,\n\t[storeAndFwdFlag] nvarchar(4000),\n\t[paymentType] nvarchar(4000),\n\t[fareAmount] float,\n\t[extra] float,\n\t[mtaTax] float,\n\t[improvementSurcharge] nvarchar(4000),\n\t[tipAmount] float,\n\t[tollsAmount] float,\n\t[totalAmount] float,\n\t[pickupDt] date,\n\t[dropoffDt] date,\n\t[tipPct] float,\n\t[Borough] nvarchar(4000),\n\t[PickupZone] nvarchar(4000),\n\t[PickupServiceZone] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'nyctaxi/tripdata/yellow_parquet/**',\n\tDATA_SOURCE = [demo_datakickstartadls_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\nSELECT TOP 100 * FROM dbo.yellow_trips_parquet\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "demo",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dedicated_sql_copy')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "ALTER DATABASE sql01\nSET AUTO_CREATE_STATISTICS ON;\n\nCREATE TABLE yellow_trips (\n\t[vendorID] nvarchar(40),\n\t[tpepPickupDateTime] datetime2(7),\n\t[tpepDropoffDateTime] datetime2(7),\n\t[passengerCount] int,\n\t[tripDistance] float,\n\t[puLocationId] nvarchar(40),\n\t[doLocationId] nvarchar(40),\n\t[startLon] float,\n\t[startLat] float,\n\t[endLon] float,\n\t[endLat] float,\n\t[rateCodeId] int,\n\t[storeAndFwdFlag] nvarchar(40),\n\t[paymentType] nvarchar(400),\n\t[fareAmount] float,\n\t[extra] float,\n\t[mtaTax] float,\n\t[improvementSurcharge] nvarchar(400),\n\t[tipAmount] float,\n\t[tollsAmount] float,\n\t[totalAmount] float,\n\t[pickupDt] date,\n\t[dropoffDt] date,\n\t[tipPct] float,\n\t[Borough] nvarchar(400),\n\t[PickupZone] nvarchar(400),\n\t[PickupServiceZone] nvarchar(400)\n\t)\nWITH (\n    CLUSTERED COLUMNSTORE INDEX,\n    DISTRIBUTION = HASH (pickupDt)\n) \n\nCOPY INTO yellow_trips FROM 'https://datakickstartadls.dfs.core.windows.net/demo/nyctaxi/tripdata/yellow_parquet/yearMonth=2018_02/*.parquet' WITH (FILE_TYPE = 'PARQUET') -- , AUTO_CREATE_TABLE = 'ON'\n\n\n\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sql01",
						"poolName": "sql01"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dedicated_sql_query')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT \n  pickupDt,\n  count(1) recordCount,\n  avg(tipAmount) avgTip\nFROM yellow_trips\nWHERE paymentType is not null\nGROUP BY \n  pickupDt\n\n  SELECT \n  pickupDt,\n  PickupZone,\n  Borough,\n  count(1) recordCount,\n  avg(tipAmount) avgTip\nFROM yellow_trips\nWHERE paymentType is not null\nGROUP BY \n  pickupDt\n  ,PickupZone\n  ,Borough\nHAVING count(1) > 1000",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sql01",
						"poolName": "sql01"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sql_permissions')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is an example of setting permissions on serverless SQL database\nCREATE DATABASE severless_db\n\nuse master\ngo\n\nCREATE LOGIN [test@trainingdustinvannoy.onmicrosoft.com] FROM EXTERNAL PROVIDER;\ngo\nCREATE LOGIN [synapse_reader] FROM EXTERNAL PROVIDER;\ngo\n\nALTER SERVER ROLE sysadmin ADD MEMBER [synapse_reader];\n--ALTER SERVER ROLE public ADD MEMBER [synapse_reader];\n\nalter role db_datareader add MEMBER [synapse_reader]\nGO\n\nalter role db_datareader add MEMBER [synapse_reader]\nGO\n\n\n\n\nuse [taxi]\nGO\nCREATE LOGIN [synapse_reader] FROM EXTERNAL PROVIDER;\ngo\n\nuse severless_db\ngo\nCREATE USER reader FROM LOGIN [synapse_reader];\ngo\nalter role db_datareader Add member [reader]\ngo\n\nCREATE VIEW test2 as SELECT * FROM taxi.dbo.yourtablename ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "severless_db",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/view_role_members')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "Select top 100 *\nFROM\n\tsys.database_role_members rm\n    JOIN sys.database_principals AS r ON rm.[role_principal_id] = r.[principal_id]\n\tJOIN sys.database_principals AS m ON rm.[member_principal_id] = m.[principal_id]\n-- WHERE\n-- \tr.[type_desc] = 'DATABASE_ROLE';\n\nSelect top 100 *\nFROM\n\tsys.server_role_members rm\n    JOIN sys.server_principals AS r ON rm.[member_principal_id] = r.[principal_id]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Confluent_Batch_Synapse')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Streaming"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "kafkatest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "36583788-27b7-4746-894c-584eb18e46b9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/kafkatest",
						"name": "kafkatest",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/kafkatest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"##Define Variables\r\n",
							"\r\n",
							"confluentBootstrapServers = \"pkc-57jzz.southcentralus.azure.confluent.cloud:9092\"\r\n",
							"\r\n",
							"confluentApiKey = \"57SIALDON7M7P3BM\"\r\n",
							"\r\n",
							"confluentSecret = \"meYS4nx3FtyZ2MHEb8vYE0QQxIjinhcROx+k7hBA38fEgwmTMqz3LtgeXenMtkTD\"\r\n",
							"\r\n",
							"confluentRegistryApiKey = \"K7ATMG3SO7XDORJB\"\r\n",
							"\r\n",
							"confluentRegistrySecret = \"mp/hWGvfrFI4lY1rKLzqqnbrbL1pIbf7sEbahShc8rQnV0qzaKY3NsIBb9NXu5SO\"\r\n",
							"\r\n",
							"confluentTopicName = \"product\"\r\n",
							"\r\n",
							"schemaRegistryUrl = \"https://psrc-pg3n2.westus2.azure.confluent.cloud\"\r\n",
							"\r\n",
							" \r\n",
							"\r\n",
							"##Define Schema Registry\r\n",
							"\r\n",
							"# from confluent_kafka.schema_registry import SchemaRegistryClient\r\n",
							"\r\n",
							"# schema_registry_conf = {\r\n",
							"\r\n",
							"#     'url': schemaRegistryUrl,\r\n",
							"\r\n",
							"#     'basic.auth.user.info': '{}:{}'.format(confluentRegistryApiKey, confluentRegistrySecret)}\r\n",
							"\r\n",
							"# schema_registry_client = SchemaRegistryClient(schema_registry_conf)\r\n",
							"\r\n",
							" \r\n",
							"\r\n",
							"##Import Library\r\n",
							"\r\n",
							"import pyspark.sql.functions as fn\r\n",
							"\r\n",
							"from pyspark.sql.avro.functions import from_avro\r\n",
							"\r\n",
							"from pyspark.sql.types import StringType\r\n",
							"\r\n",
							"binary_to_string = fn.udf(lambda x: str(int.from_bytes(x, byteorder='big')), StringType())\r\n",
							"\r\n",
							" \r\n",
							"\r\n",
							"##Create Spark Readstream\r\n",
							"\r\n",
							"# clickstreamTestDf = (\r\n",
							"\r\n",
							"#   spark\r\n",
							"\r\n",
							"#   .readStream\r\n",
							"\r\n",
							"#   .format(\"delta\")\r\n",
							"\r\n",
							"#   .load(\"abfss://demo@datakickstartadls.dfs.core.windows.net/confluent_test_source\")\r\n",
							"# )\r\n",
							"\r\n",
							"clickstreamTestDf = (\r\n",
							"\r\n",
							"  spark\r\n",
							"\r\n",
							"  .readStream\r\n",
							"\r\n",
							"  .format(\"kafka\")\r\n",
							"\r\n",
							"  .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\r\n",
							"\r\n",
							"  .option(\"kafka.security.protocol\", \"SASL_SSL\")\r\n",
							"\r\n",
							"  .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\r\n",
							"  .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(confluentApiKey, confluentSecret))\r\n",
							"\r\n",
							"\r\n",
							"  .option(\"kafka.sasl.mechanism\", \"PLAIN\")\r\n",
							"\r\n",
							"  .option(\"subscribe\", confluentTopicName)\r\n",
							"\r\n",
							"  .option(\"startingOffsets\", \"earliest\")\r\n",
							"\r\n",
							"  .option(\"failOnDataLoss\", \"false\")\r\n",
							"\r\n",
							"  .load()\r\n",
							"\r\n",
							")\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"#Data within the Spark stream can be displayed dynamically from within the notebook.\r\n",
							"\r\n",
							"##Display \r\n",
							"\r\n",
							"# display(clickstreamTestDf) \r\n",
							"# clickstreamTestDf = (clickstreamTestDf\r\n",
							"# .withColumn('key', fn.col(\"key\").cast(StringType()))\r\n",
							"\r\n",
							"#   .withColumn('fixedValue', fn.expr(\"substring(value, 6, length(value)-5)\"))\r\n",
							"\r\n",
							"#   .withColumn('valueSchemaId', binary_to_string(fn.expr(\"substring(value, 2, 4)\")))\r\n",
							"\r\n",
							"#   .select('topic', 'partition', 'offset', 'timestamp', 'timestampType', 'key', 'valueSchemaId','fixedValue')\r\n",
							"# )\r\n",
							"\r\n",
							"clickstreamTestDf.writeStream.option(\"checkpointLocation\", \"abfss://demo@datakickstartadls.dfs.core.windows.net/checkpoints/confluent1\").format(\"delta\").start(\"abfss://demo@datakickstartadls.dfs.core.windows.net/confluent_test\")\r\n",
							"\r\n",
							"print(\"Streaming query\")\r\n",
							"\r\n",
							"# Caused by: java.lang.IllegalArgumentException: Could not find a 'KafkaClient' entry in the JAAS configuration. System property 'java.security.auth.login.config' is not set"
						],
						"outputs": [],
						"execution_count": 12
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Confluent_Synapse')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Streaming"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "kafkatest",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6e4a4882-1698-4506-aec4-7ee5dcd7516e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/kafkatest",
						"name": "kafkatest",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/kafkatest",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"##Define Variables\r\n",
							"\r\n",
							"confluentBootstrapServers = \"pkc-57jzz.southcentralus.azure.confluent.cloud:9092\"\r\n",
							"\r\n",
							"confluentApiKey = \"57SIALDON7M7P3BM\"\r\n",
							"\r\n",
							"confluentSecret = \"meYS4nx3FtyZ2MHEb8vYE0QQxIjinhcROx+k7hBA38fEgwmTMqz3LtgeXenMtkTD\"\r\n",
							"\r\n",
							"confluentRegistryApiKey = \"K7ATMG3SO7XDORJB\"\r\n",
							"\r\n",
							"confluentRegistrySecret = \"mp/hWGvfrFI4lY1rKLzqqnbrbL1pIbf7sEbahShc8rQnV0qzaKY3NsIBb9NXu5SO\"\r\n",
							"\r\n",
							"confluentTopicName = \"product\"\r\n",
							"\r\n",
							"schemaRegistryUrl = \"https://psrc-pg3n2.westus2.azure.confluent.cloud\"\r\n",
							"\r\n",
							" \r\n",
							"\r\n",
							"##Define Schema Registry\r\n",
							"\r\n",
							"# from confluent_kafka.schema_registry import SchemaRegistryClient\r\n",
							"\r\n",
							"# schema_registry_conf = {\r\n",
							"\r\n",
							"#     'url': schemaRegistryUrl,\r\n",
							"\r\n",
							"#     'basic.auth.user.info': '{}:{}'.format(confluentRegistryApiKey, confluentRegistrySecret)}\r\n",
							"\r\n",
							"# schema_registry_client = SchemaRegistryClient(schema_registry_conf)\r\n",
							"\r\n",
							" \r\n",
							"\r\n",
							"##Import Library\r\n",
							"\r\n",
							"import pyspark.sql.functions as fn\r\n",
							"\r\n",
							"from pyspark.sql.avro.functions import from_avro\r\n",
							"\r\n",
							"from pyspark.sql.types import StringType\r\n",
							"\r\n",
							"binary_to_string = fn.udf(lambda x: str(int.from_bytes(x, byteorder='big')), StringType())\r\n",
							"\r\n",
							" \r\n",
							"\r\n",
							"##Create Spark Readstream\r\n",
							"\r\n",
							"# clickstreamTestDf = (\r\n",
							"\r\n",
							"#   spark\r\n",
							"\r\n",
							"#   .readStream\r\n",
							"\r\n",
							"#   .format(\"delta\")\r\n",
							"\r\n",
							"#   .load(\"abfss://demo@datakickstartadls.dfs.core.windows.net/confluent_test_source\")\r\n",
							"# )\r\n",
							"\r\n",
							"clickstreamTestDf = (\r\n",
							"\r\n",
							"  spark\r\n",
							"\r\n",
							"  .readStream\r\n",
							"\r\n",
							"  .format(\"kafka\")\r\n",
							"\r\n",
							"  .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\r\n",
							"\r\n",
							"  .option(\"kafka.security.protocol\", \"SASL_SSL\")\r\n",
							"\r\n",
							"  .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(confluentApiKey, confluentSecret))\r\n",
							"\r\n",
							"  .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\r\n",
							"\r\n",
							"  .option(\"kafka.sasl.mechanism\", \"PLAIN\")\r\n",
							"\r\n",
							"  .option(\"subscribe\", confluentTopicName)\r\n",
							"\r\n",
							"  .option(\"startingOffsets\", \"earliest\")\r\n",
							"\r\n",
							"  .option(\"failOnDataLoss\", \"false\")\r\n",
							"\r\n",
							"  .load()\r\n",
							"\r\n",
							"  .withColumn('key', fn.col(\"key\").cast(StringType()))\r\n",
							"\r\n",
							"  .withColumn('fixedValue', fn.expr(\"substring(value, 6, length(value)-5)\"))\r\n",
							"\r\n",
							"  .withColumn('valueSchemaId', binary_to_string(fn.expr(\"substring(value, 2, 4)\")))\r\n",
							"\r\n",
							"  .select('topic', 'partition', 'offset', 'timestamp', 'timestampType', 'key', 'valueSchemaId','fixedValue')\r\n",
							"\r\n",
							")\r\n",
							"\r\n",
							"#Data within the Spark stream can be displayed dynamically from within the notebook.\r\n",
							"\r\n",
							"##Display \r\n",
							"\r\n",
							"# display(clickstreamTestDf) \r\n",
							"clickstreamTestDf.writeStream.option(\"checkpointLocation\", \"abfss://demo@datakickstartadls.dfs.core.windows.net/checkpoints/confluent1\").format(\"delta\").start(\"abfss://demo@datakickstartadls.dfs.core.windows.net/confluent_test\")\r\n",
							"\r\n",
							"print(\"Streaming query\")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Confluent_v1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Streaming"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "confluent1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5441b500-6219-4e6f-862e-0879be0b9c33"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/confluent1",
						"name": "confluent1",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/confluent1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import pkg_resources\r\n",
							"# for d in pkg_resources.working_set:\r\n",
							"#      print(d)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %%configure -f\r\n",
							"# {\r\n",
							"#     \"conf\": {\r\n",
							"#         \"spark.jars\": \"abfss://demo@datakickstartadls.dfs.core.windows.net/jars/*\",\r\n",
							"\r\n",
							"#     }\r\n",
							"# }"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							"##Define Variables\r\n",
							"\r\n",
							"confluentBootstrapServers = \"pkc-41973.westus2.azure.confluent.cloud:9092\"\r\n",
							"\r\n",
							"confluentApiKey = \"HBZAZ3BKC57KW5XI\"\r\n",
							"\r\n",
							"confluentSecret = \"tvEWW4vI+X1M81Y0BU7knPqlgllBthCs+sU7Zsiv27CyJlII2qRTF208kVkf0q4/\"\r\n",
							"\r\n",
							"confluentRegistryApiKey = \"PYSZJLGMTTVXUFNH\"\r\n",
							"\r\n",
							"confluentRegistrySecret = \"DVUSBKSeXQO533EDfowOkHL1ncmkuNHee48NOK82D1rY/0NqFqofxgyMpZb1pA0Y\"\r\n",
							"\r\n",
							"confluentTopicName = \"video_usage\"\r\n",
							"\r\n",
							"schemaRegistryUrl = \"https://psrc-4r0k9.westus2.azure.confluent.cloud\"\r\n",
							"\r\n",
							" \r\n",
							"\r\n",
							"#Define Schema Registry\r\n",
							"\r\n",
							"# from confluent_kafka.schema_registry import SchemaRegistryClient\r\n",
							"\r\n",
							"# schema_registry_conf = {\r\n",
							"\r\n",
							"#     'url': schemaRegistryUrl,\r\n",
							"\r\n",
							"#     'basic.auth.user.info': '{}:{}'.format(confluentRegistryApiKey, confluentRegistrySecret)}\r\n",
							"\r\n",
							"# schema_registry_client = SchemaRegistryClient(schema_registry_conf)\r\n",
							"\r\n",
							"\r\n",
							"##Import Library\r\n",
							"\r\n",
							"import pyspark.sql.functions as fn\r\n",
							"\r\n",
							"from pyspark.sql.avro.functions import from_avro\r\n",
							"\r\n",
							"from pyspark.sql.types import StringType\r\n",
							"\r\n",
							"binary_to_string = fn.udf(lambda x: str(int.from_bytes(x, byteorder='big')), StringType())\r\n",
							"\r\n",
							" \r\n",
							"\r\n",
							"##Create Spark Readstream\r\n",
							"\r\n",
							"clickstreamTestDf = (\r\n",
							"  spark\r\n",
							"  .readStream\r\n",
							"  .format(\"kafka\")\r\n",
							"  .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\r\n",
							"  .option(\"kafka.security.protocol\", \"SASL_SSL\")\r\n",
							"  .option(\"kafka.sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(confluentApiKey, confluentSecret))\r\n",
							"  .option(\"kafka.sasl.mechanism\", \"PLAIN\")\r\n",
							"  .option(\"subscribe\", confluentTopicName)\r\n",
							"  .option(\"startingOffsets\", \"earliest\")\r\n",
							"  .option(\"failOnDataLoss\", \"false\")\r\n",
							"  .load()\r\n",
							"  # .select(fn.col(\"value\").cast(StringType()).alias(\"value\"))\r\n",
							")\r\n",
							"\r\n",
							"clickstreamTestDf = (clickstreamTestDf\r\n",
							".withColumn('key', fn.col(\"key\").cast(StringType()))\r\n",
							"  .withColumn('fixedValue', fn.expr(\"substring(value, 6, length(value)-5)\"))\r\n",
							"  .withColumn('valueSchemaId', binary_to_string(fn.expr(\"substring(value, 2, 4)\")))\r\n",
							"  .select('topic', 'partition', 'offset', 'timestamp', 'timestampType', 'key', 'valueSchemaId','fixedValue')\r\n",
							")\r\n",
							"\r\n",
							"query = (clickstreamTestDf.writeStream\r\n",
							"  .option(\"checkpointLocation\", \"abfss://demo@datakickstartadls.dfs.core.windows.net/checkpoints/confluent6\")\r\n",
							"  .format(\"delta\")\r\n",
							"  .outputMode(\"append\")\r\n",
							"  .trigger(processingTime=\"5 seconds\")\r\n",
							"  .start(\"abfss://demo@datakickstartadls.dfs.core.windows.net/confluent_test6\")\r\n",
							")\r\n",
							"\r\n",
							"query.processAllAvailable()\r\n",
							"query.stop()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 20
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Confluent_v2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Streaming"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "confluent1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "525bb3d8-0bfb-49a4-90af-5e8f382a805b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/confluent1",
						"name": "confluent1",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/confluent1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"##Define Variables\r\n",
							"confluentBootstrapServers = \"pkc-57jzz.southcentralus.azure.confluent.cloud:9092\"\r\n",
							"confluentApiKey = \"57SIALDON7M7P3BM\"\r\n",
							"confluentSecret = \"meYS4nx3FtyZ2MHEb8vYE0QQxIjinhcROx+k7hBA38fEgwmTMqz3LtgeXenMtkTD\"\r\n",
							"confluentRegistryApiKey = \"K7ATMG3SO7XDORJB\"\r\n",
							"confluentRegistrySecret = \"mp/hWGvfrFI4lY1rKLzqqnbrbL1pIbf7sEbahShc8rQnV0qzaKY3NsIBb9NXu5SO\"\r\n",
							"confluentTopicName = \"product\"\r\n",
							"schemaRegistryUrl = \"https://psrc-pg3n2.westus2.azure.confluent.cloud\"\r\n",
							"adls_path = \"abfss://demo@datakickstartadls.dfs.core.windows.net\"\r\n",
							" \r\n",
							"##Define Schema Registry\r\n",
							"from confluent_kafka.schema_registry import SchemaRegistryClient\r\n",
							"schema_registry_conf = {\r\n",
							"    'url': schemaRegistryUrl,\r\n",
							"    'basic.auth.user.info': '{}:{}'.format(confluentRegistryApiKey, confluentRegistrySecret)}\r\n",
							"schema_registry_client = SchemaRegistryClient(schema_registry_conf)\r\n",
							" \r\n",
							"##Import Library\r\n",
							"import pyspark.sql.functions as fn\r\n",
							"from pyspark.sql.avro.functions import from_avro\r\n",
							"from pyspark.sql.types import StringType\r\n",
							"binary_to_string = fn.udf(lambda x: str(int.from_bytes(x, byteorder='big')), StringType())\r\n",
							" \r\n",
							"##Create Spark Readstream\r\n",
							"clickstreamTestDf = (\r\n",
							"  spark\r\n",
							"  .readStream\r\n",
							"  .format(\"kafka\")\r\n",
							"  .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\r\n",
							"  .option(\"kafka.security.protocol\", \"SASL_SSL\")\r\n",
							"  .option(\"kafka.sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(confluentApiKey, confluentSecret))\r\n",
							"  .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\r\n",
							"  .option(\"kafka.sasl.mechanism\", \"PLAIN\")\r\n",
							"  .option(\"subscribe\", confluentTopicName)\r\n",
							"  .option(\"startingOffsets\", \"earliest\")\r\n",
							"  .option(\"failOnDataLoss\", \"false\")\r\n",
							"  .load()\r\n",
							"  .withColumn('key', fn.col(\"key\").cast(StringType()))\r\n",
							"  .withColumn('fixedValue', fn.expr(\"substring(value, 6, length(value)-5)\"))\r\n",
							"  .withColumn('valueSchemaId', binary_to_string(fn.expr(\"substring(value, 2, 4)\")))\r\n",
							"  .select('topic', 'partition', 'offset', 'timestamp', 'timestampType', 'key', 'valueSchemaId','fixedValue')\r\n",
							")\r\n",
							"\r\n",
							"query = (clickstreamTestDf.writeStream\r\n",
							"    .option(\"checkpointLocation\", \"{}/checkpoints/confluent\".format(adls_path))\r\n",
							"    .format(\"delta\")\r\n",
							"    .outputMode(\"append\")\r\n",
							"    .trigger(processingTime=\"5 seconds\")\r\n",
							"    .start(\"{}/output\".format(adls_path))\r\n",
							"  )\r\n",
							"query.awaitTermination()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Kafka_Producer')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Streaming"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "tobogganCustom",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8fbfdf87-24f0-4920-b9f7-a498de785686"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/tobogganCustom",
						"name": "tobogganCustom",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/tobogganCustom",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import java.sql.Timestamp\r\n",
							"import java.time.Instant\r\n",
							"import java.util.Properties\r\n",
							"\r\n",
							"import scala.util.Random\r\n",
							"import org.apache.kafka.clients.producer._\r\n",
							"\r\n",
							"  val bootstrapServers = \"streaming-demo-eh.servicebus.windows.net:9093\"\r\n",
							"  val mode: String = \"eventhubs\"\r\n",
							"\r\n",
							"  val props = new Properties()\r\n",
							"  props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\r\n",
							"  props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\r\n",
							"\r\n",
							"  if (mode.toLowerCase == \"local\") {\r\n",
							"    println(\"Running producer in local mode\")\r\n",
							"    props.put(\"bootstrap.servers\", \"localhost:9092\")\r\n",
							"  } else if (mode.toLowerCase == \"confluent\") {\r\n",
							"    println(\"Running producer in Confluent Cloud mode\")\r\n",
							"    val kafkaAPIKey = \"\"\r\n",
							"    val kafkaAPISecret = \"\"\r\n",
							"    props.put(\"bootstrap.servers\", bootstrapServers)\r\n",
							"    props.put(\"security.protocol\", \"SASL_SSL\")\r\n",
							"    props.put(\"sasl.jaas.config\", s\"org.apache.kafka.common.security.plain.PlainLoginModule  required username='$kafkaAPIKey'   password='$kafkaAPISecret';\")\r\n",
							"    props.put(\"sasl.mechanism\", \"PLAIN\")\r\n",
							"    // Required for correctness in Apache Kafka clients prior to 2.6\r\n",
							"    props.put(\"client.dns.lookup\", \"use_all_dns_ips\")\r\n",
							"    // Best practice for Kafka producer to prevent data loss\r\n",
							"    props.put(\"acks\", \"all\")\r\n",
							"  } else if (mode.toLowerCase==\"eventhubs\") {\r\n",
							"    println(\"Running producer to Eventhubs\")\r\n",
							"    val saslPwd = \"Endpoint=sb://streaming-demo-eh.servicebus.windows.net/;SharedAccessKeyName=tobogganPolicy;SharedAccessKey=bNaWYEtNNtgEy5JbSBvPsgkPzjgEFX+T6KhcFs2uGJM=;EntityPath=stream-demo-1\"\r\n",
							"    props.put(\"bootstrap.servers\", bootstrapServers)\r\n",
							"    props.put(\"security.protocol\", \"SASL_SSL\")\r\n",
							"    props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule  required username='$ConnectionString'  \" + s\" password='$saslPwd';\")\r\n",
							"    props.put(\"sasl.mechanism\", \"PLAIN\")\r\n",
							"  }\r\n",
							"\r\n",
							"def produceRecord(producer: KafkaProducer[String, String], record: ProducerRecord[String, String]) = {\r\n",
							"    val metadata = producer.send(record)\r\n",
							"    printf(s\"sent to topic %s: record(key=%s value=%s) \" +\r\n",
							"      \"meta(partition=%d, offset=%d)\\n\",\r\n",
							"      record.topic(), record.key(), record.value(),\r\n",
							"      metadata.get().partition(),\r\n",
							"      metadata.get().offset()\r\n",
							"    )\r\n",
							"  }\r\n",
							" \r\n",
							"  def produceSingleVideoUsage(user: Int, usageId: Int): Unit = {\r\n",
							"    val usageTopic = \"stream_demo_1\"\r\n",
							"\r\n",
							"    val producer = new KafkaProducer[String, String](props)\r\n",
							"\r\n",
							"    val randomCompleted = () => { if (Random.nextInt(2) == 1) true else false }\r\n",
							"    val randomDuration= () => { Random.nextInt(360) }\r\n",
							"    val timestampNow = () => Timestamp.from(Instant.now)\r\n",
							"\r\n",
							"    try {\r\n",
							"      val usageInfo = s\"\"\"{\"usageId\": $usageId, \"user\": \"user${user}\", \"completed\": ${randomCompleted()}, \"durationSeconds\": ${randomDuration()}, \"eventTimestamp\": \"${timestampNow()}\"}\"\"\"\r\n",
							"      val record = new ProducerRecord[String, String](usageTopic, usageId.toString, usageInfo)\r\n",
							"\r\n",
							"      //println(usageInfo)\r\n",
							"      produceRecord(producer, record)\r\n",
							"\r\n",
							"    }catch{\r\n",
							"      case e:Exception => e.printStackTrace()\r\n",
							"    }finally {\r\n",
							"      producer.close()\r\n",
							"    }\r\n",
							"  }\r\n",
							"\r\n",
							" def writeUsageWithPlan(): Unit = {\r\n",
							"    val usageTopic = \"stream-demo-1\"\r\n",
							"\r\n",
							"    val producer = new KafkaProducer[String, String](props)\r\n",
							"\r\n",
							"    val randomCompleted = () => {\r\n",
							"      if (Random.nextInt(2) == 1) true else false\r\n",
							"    }\r\n",
							"    val randomUser = () => {\r\n",
							"      Random.nextInt(100)\r\n",
							"    }\r\n",
							"    val randomDuration = () => {\r\n",
							"      Random.nextInt(360)\r\n",
							"    }\r\n",
							"    val timestampNow = () => Timestamp.from(Instant.now)\r\n",
							"    val startInstant = Instant.now\r\n",
							"\r\n",
							"    val maxRecordId = 10000\r\n",
							"    val sleepMilliseconds = 500\r\n",
							"    val randomizeSleep = () => Random.nextInt(9) + 1\r\n",
							"    val pauseThreshold = 0 // at which record should pause start being enforced, 0 means all records\r\n",
							"\r\n",
							"    try {\r\n",
							"      for (i <- 0 to maxRecordId) {\r\n",
							"        val usageInfo = s\"\"\"{\"usageId\": $i, \"user\": \"user${randomUser()}\", \"completed\": ${randomCompleted()}, \"durationSeconds\": ${randomDuration()}, \"eventTimestamp\": \"${timestampNow()}\"}\"\"\"\r\n",
							"        val record = new ProducerRecord[String, String](usageTopic, i.toString, usageInfo)\r\n",
							"\r\n",
							"        println(usageInfo)\r\n",
							"        produceRecord(producer, record)\r\n",
							"\r\n",
							"        if (i > pauseThreshold) {\r\n",
							"          \r\n",
							"          Thread.sleep(sleepMilliseconds / randomizeSleep())\r\n",
							"        }\r\n",
							"      }\r\n",
							"    } catch {\r\n",
							"      case e: Exception => e.printStackTrace()\r\n",
							"    } finally {\r\n",
							"      producer.close()\r\n",
							"    }\r\n",
							"  }\r\n",
							"\r\n",
							"  writeUsageWithPlan()"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark_Kafka_Streaming')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Streaming"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "tobogganCustom",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e543ad6a-4476-4c31-8844-39ccd84418c0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/tobogganCustom",
						"name": "tobogganCustom",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/tobogganCustom",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# results = spark.read.format(\"delta\").load(f\"abfss://demo@datakickstartadls.dfs.core.windows.net/{directory}\")\r\n",
							"# results.show()"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"##Define Variables\r\n",
							"\r\n",
							"confluentBootstrapServers = \"pkc-41973.westus2.azure.confluent.cloud:9092\"  # mssparkutils.credentials.getSecret(\"dvtrainingkv\", \"confluent-cloud-brokers\")\r\n",
							"\r\n",
							"confluentApiKey = \"HBZAZ3BKC57KW5XI\"\r\n",
							"\r\n",
							"confluentSecret = \"tvEWW4vI+X1M81Y0BU7knPqlgllBthCs+sU7Zsiv27CyJlII2qRTF208kVkf0q4/\"\r\n",
							"\r\n",
							"confluentRegistryApiKey = \"PYSZJLGMTTVXUFNH\"\r\n",
							"\r\n",
							"confluentRegistrySecret = \"DVUSBKSeXQO533EDfowOkHL1ncmkuNHee48NOK82D1rY/0NqFqofxgyMpZb1pA0Y\"\r\n",
							"\r\n",
							"confluentTopicName = \"video_usage\"\r\n",
							"\r\n",
							"\r\n",
							"##Import Library\r\n",
							"\r\n",
							"import pyspark.sql.functions as fn\r\n",
							"from pyspark.sql.types import StringType\r\n",
							"directory = \"confluent_test19\"\r\n",
							"\r\n",
							"##Create Spark Readstream\r\n",
							"clickstreamTestDf = (\r\n",
							"  spark\r\n",
							"  .readStream\r\n",
							"  .format(\"kafka\")\r\n",
							"  .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\r\n",
							"  .option(\"kafka.security.protocol\", \"SASL_SSL\")\r\n",
							"  .option(\"kafka.sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(confluentApiKey, confluentSecret))\r\n",
							"  .option(\"kafka.sasl.mechanism\", \"PLAIN\")\r\n",
							"  .option(\"subscribe\", confluentTopicName)\r\n",
							"  .option(\"startingOffsets\", \"earliest\")\r\n",
							"  .option(\"failOnDataLoss\", \"false\")\r\n",
							"  .load()\r\n",
							"  # .select(fn.col(\"value\").cast(StringType()).alias(\"value\"))\r\n",
							")\r\n",
							"\r\n",
							"clickstreamTestDf = (clickstreamTestDf\r\n",
							".withColumn('key', fn.col(\"key\").cast(StringType()))\r\n",
							"  .select('topic', 'partition', 'offset', 'timestamp', 'timestampType', 'key')\r\n",
							")\r\n",
							"\r\n",
							"query = (clickstreamTestDf.writeStream\r\n",
							"  .option(\"checkpointLocation\", f\"abfss://demo@datakickstartadls.dfs.core.windows.net/checkpoints/{directory}\")\r\n",
							"  .format(\"delta\")\r\n",
							"  .outputMode(\"append\")\r\n",
							"  .trigger(once=True)  # .trigger(processingTime=\"5 seconds\")\r\n",
							"  .start(f\"abfss://demo@datakickstartadls.dfs.core.windows.net/{directory}\")\r\n",
							")\r\n",
							"\r\n",
							"query.processAllAvailable()\r\n",
							"query.stop()\r\n",
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark_Kafka_Streaming_BATCH')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Streaming"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "confluent1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "f5a0422d-b014-4a14-861f-65bfb87c00fb"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/confluent1",
						"name": "confluent1",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/confluent1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 10
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import pkg_resources\r\n",
							"# for d in pkg_resources.working_set:\r\n",
							"#      print(d)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %%configure -f\r\n",
							"# {\r\n",
							"#     \"conf\": {\r\n",
							"#         \"spark.jars\": \"abfss://demo@datakickstartadls.dfs.core.windows.net/jars/*\",\r\n",
							"\r\n",
							"#     }\r\n",
							"# }"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"source": [
							"##Define Variables\r\n",
							"\r\n",
							"confluentBootstrapServers = \"pkc-41973.westus2.azure.confluent.cloud:9092\"\r\n",
							"\r\n",
							"confluentApiKey = \"HBZAZ3BKC57KW5XI\"\r\n",
							"\r\n",
							"confluentSecret = \"tvEWW4vI+X1M81Y0BU7knPqlgllBthCs+sU7Zsiv27CyJlII2qRTF208kVkf0q4/\"\r\n",
							"\r\n",
							"confluentRegistryApiKey = \"PYSZJLGMTTVXUFNH\"\r\n",
							"\r\n",
							"confluentRegistrySecret = \"DVUSBKSeXQO533EDfowOkHL1ncmkuNHee48NOK82D1rY/0NqFqofxgyMpZb1pA0Y\"\r\n",
							"\r\n",
							"confluentTopicName = \"video_usage\"\r\n",
							"\r\n",
							"\r\n",
							"##Import Library\r\n",
							"\r\n",
							"import pyspark.sql.functions as fn\r\n",
							"from pyspark.sql.types import StringType\r\n",
							"\r\n",
							"##Create Spark Readstream\r\n",
							"\r\n",
							"clickstreamTestDf = (\r\n",
							"  spark\r\n",
							"  .read\r\n",
							"  .format(\"kafka\")\r\n",
							"  .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\r\n",
							"  .option(\"kafka.security.protocol\", \"SASL_SSL\")\r\n",
							"  .option(\"kafka.sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(confluentApiKey, confluentSecret))\r\n",
							"  .option(\"kafka.sasl.mechanism\", \"PLAIN\")\r\n",
							"  .option(\"subscribe\", confluentTopicName)\r\n",
							"  .option(\"startingOffsets\", \"earliest\")\r\n",
							"  .option(\"failOnDataLoss\", \"false\")\r\n",
							"  .load()\r\n",
							"  # .select(fn.col(\"value\").cast(StringType()).alias(\"value\"))\r\n",
							")\r\n",
							"\r\n",
							"clickstreamTestDf = (clickstreamTestDf\r\n",
							".withColumn('key', fn.col(\"key\").cast(StringType()))\r\n",
							"  .select('topic', 'partition', 'offset', 'timestamp', 'timestampType', 'key')\r\n",
							")\r\n",
							"\r\n",
							"# query = (clickstreamTestDf.writeStream\r\n",
							"#   .option(\"checkpointLocation\", \"abfss://demo@datakickstartadls.dfs.core.windows.net/checkpoints/confluent9\")\r\n",
							"#   .format(\"delta\")\r\n",
							"#   .outputMode(\"append\")\r\n",
							"#   .trigger(once=True)\r\n",
							"#   .start(\"abfss://demo@datakickstartadls.dfs.core.windows.net/confluent_test9\")\r\n",
							"# )\r\n",
							"\r\n",
							"\r\n",
							"(clickstreamTestDf.write\r\n",
							"  .format(\"delta\")\r\n",
							"  .mode(\"append\")\r\n",
							"  .save(\"abfss://demo@datakickstartadls.dfs.core.windows.net/confluent_test12\")\r\n",
							")\r\n",
							"# print(query.status)\r\n",
							"# print(query.recentProgress)\r\n",
							"# query.awaitTermination()\r\n",
							"# import time\r\n",
							"# time.sleep(300)\r\n",
							"# query.stop()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark_Kickstart_ReadPatterns')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Spark Kickstart"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark32",
					"type": "BigDataPoolReference"
				},
				"targetSparkConfiguration": {
					"referenceName": "sparkConfigurationSmall",
					"type": "SparkConfigurationReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "2ffacdc8-93be-4c93-98f7-7c973d6eb53c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/spark32",
						"name": "spark32",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 10,
					"targetSparkConfiguration": "sparkConfigurationSmall"
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### "
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Read with JDBC\r\n",
							"\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"database = \"StackOverflow2010\"\r\n",
							"db_host_name = \"sandbox-2-sqlserver.database.windows.net\"\r\n",
							"db_url = f\"jdbc:sqlserver://{db_host_name};databaseName={database}\"\r\n",
							"db_user = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"sql-user-stackoverflow\")\r\n",
							"db_password = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"sql-pwd-stackoverflow\")\r\n",
							"\r\n",
							"table = \"Users\"\r\n",
							"\r\n",
							"df = (\r\n",
							"    spark.read\r\n",
							"    .format(\"jdbc\")\r\n",
							"    .option(\"url\", db_url)\r\n",
							"    .option(\"dbtable\", table)\r\n",
							"    .option(\"user\", db_user)\r\n",
							"    .option(\"password\", db_password)\r\n",
							"    .load()\r\n",
							")\r\n",
							"\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Read from SQL Server (special driver)"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"database = \"StackOverflow2010\"\r\n",
							"db_host_name = \"sandbox-2-sqlserver.database.windows.net\"\r\n",
							"db_url = f\"jdbc:sqlserver://{db_host_name};databaseName={database}\"\r\n",
							"db_user = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"sql-user-stackoverflow\")\r\n",
							"db_password = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"sql-pwd-stackoverflow\")\r\n",
							"  \r\n",
							"table = \"Users\"\r\n",
							"\r\n",
							"df = (\r\n",
							"    spark.read\r\n",
							"    .format(\"com.microsoft.sqlserver.jdbc.spark\")\r\n",
							"    .option(\"url\", db_url)\r\n",
							"    .option(\"dbtable\", table)\r\n",
							"    .option(\"user\", db_user)\r\n",
							"    .option(\"password\", db_password)\r\n",
							"    .load()\r\n",
							")\r\n",
							"\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sparkConfigurations/sparkConfigurationSmall')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark_Kickstart_WritePatterns')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Spark Kickstart"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cf21ba94-2f12-46ce-a561-fec48950d266"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/spark32",
						"name": "spark32",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"sample_column_names = [\"ItemName\", \"Value\"]\r\n",
							"sample_data = [[\"Item1\", 1.0], [\"Item2\", 2.0], [\"Item3\", 3.0], [\"Item4\", 4.0]]\r\n",
							"\r\n",
							"df = spark.createDataFrame(sample_data, sample_column_names)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Write to ADLS\r\n",
							"In Databricks it is easiest to mount the ADLS location. You can write to local filesystem or specially mounted folders depending on the environment you are in. Many different protocols that may require their own Spark extensions to be installed.\r\n",
							"\r\n",
							"Common protocols:\r\n",
							"\r\n",
							"file:// Local filesystem\r\n",
							"hdfs:// Hadoop filesystem\r\n",
							"abfss:// Azure Data Lake Storage (Gen2)\r\n",
							"wasbs:// Azure Blob Storage\r\n",
							"s3:// AWS S3 (several variations of this)\r\n",
							"gs:// Google Cloud Storage"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# CSV\r\n",
							"csv_path = \"abfss://demo@datakickstartadls.dfs.core.windows.net/write_examples/items_csv\"\r\n",
							"(\r\n",
							"  df.write\r\n",
							"    .mode(\"overwrite\")\r\n",
							"    .option(\"header\",\"true\")\r\n",
							"    .csv(csv_path) \r\n",
							" )"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Parquet\r\n",
							"parquet_path = \"abfss://demo@datakickstartadls.dfs.core.windows.net/write_examples/items_parquet\"\r\n",
							"\r\n",
							"(\r\n",
							"  df.write\r\n",
							"    .mode(\"overwrite\")\r\n",
							"    .parquet(parquet_path)\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Delta\r\n",
							"delta_path = \"abfss://demo@datakickstartadls.dfs.core.windows.net/write_examples/items_delta\"\r\n",
							"\r\n",
							"(\r\n",
							"  df.write\r\n",
							"    .format(\"delta\")\r\n",
							"    .save(delta_path)\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Write to JDBC"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"database = \"StackOverflow2010\"\r\n",
							"db_host_name = \"sandbox-2-sqlserver.database.windows.net\"\r\n",
							"db_url = f\"jdbc:sqlserver://{db_host_name};databaseName={database}\"\r\n",
							"db_user = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"sql-user-stackoverflow\")\r\n",
							"db_password = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"sql-pwd-stackoverflow\")\r\n",
							"\r\n",
							"table = \"items_test\"\r\n",
							"\r\n",
							"(\r\n",
							"  df.write\r\n",
							"    .mode(\"overwrite\")\r\n",
							"    .format(\"jdbc\")\r\n",
							"    .option(\"url\", db_url)\r\n",
							"    .option(\"dbtable\", table)\r\n",
							"    .option(\"user\", db_user)\r\n",
							"    .option(\"password\", db_password)\r\n",
							"    .save()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Write to SQL Server (special driver)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"database = \"StackOverflow2010\"\r\n",
							"db_host_name = \"sandbox-2-sqlserver.database.windows.net\"\r\n",
							"db_url = f\"jdbc:sqlserver://{db_host_name};databaseName={database}\"\r\n",
							"db_user = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"sql-user-stackoverflow\")\r\n",
							"db_password = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"sql-pwd-stackoverflow\")\r\n",
							"\r\n",
							"table = \"items_test2\"\r\n",
							"\r\n",
							"(\r\n",
							"    df.write\r\n",
							"    .mode(\"overwrite\")\r\n",
							"    .format(\"com.microsoft.sqlserver.jdbc.spark\")\r\n",
							"    .option(\"url\", db_url)\r\n",
							"    .option(\"dbtable\", table)\r\n",
							"    .option(\"user\", db_user)\r\n",
							"    .option(\"password\", db_password)\r\n",
							"    .save()\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/confluent_synapse_scala')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Streaming"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "confluent1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "Synapse Spark"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/confluent1",
						"name": "confluent1",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/confluent1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"// ##Define Variables\r\n",
							"\r\n",
							"// val confluentBootstrapServers = \"pkc-57jzz.southcentralus.azure.confluent.cloud:9092\"\r\n",
							"\r\n",
							"// val confluentApiKey = \"57SIALDON7M7P3BM\"\r\n",
							"\r\n",
							"// val confluentSecret = \"meYS4nx3FtyZ2MHEb8vYE0QQxIjinhcROx+k7hBA38fEgwmTMqz3LtgeXenMtkTD\"\r\n",
							"\r\n",
							"// val confluentRegistryApiKey = \"K7ATMG3SO7XDORJB\"\r\n",
							"\r\n",
							"// val confluentRegistrySecret = \"mp/hWGvfrFI4lY1rKLzqqnbrbL1pIbf7sEbahShc8rQnV0qzaKY3NsIBb9NXu5SO\"\r\n",
							"\r\n",
							"// val confluentTopicName = \"product\"\r\n",
							"\r\n",
							"// val schemaRegistryUrl = \"https://psrc-pg3n2.westus2.azure.confluent.cloud\"\r\n",
							"\r\n",
							"val confluentBootstrapServers = \"pkc-41973.westus2.azure.confluent.cloud:9092\"\r\n",
							"\r\n",
							"val confluentApiKey = \"HBZAZ3BKC57KW5XI\"\r\n",
							"\r\n",
							"val confluentSecret = \"tvEWW4vI+X1M81Y0BU7knPqlgllBthCs+sU7Zsiv27CyJlII2qRTF208kVkf0q4/\"\r\n",
							"\r\n",
							"// val confluentRegistryApiKey = \"K7ATMG3SO7XDORJB\"\r\n",
							"\r\n",
							"// val confluentRegistrySecret = \"mp/hWGvfrFI4lY1rKLzqqnbrbL1pIbf7sEbahShc8rQnV0qzaKY3NsIBb9NXu5SO\"\r\n",
							"\r\n",
							"// val confluentTopicName = \"product\"\r\n",
							"\r\n",
							"// val schemaRegistryUrl = \"https://psrc-pg3n2.westus2.azure.confluent.cloud\"\r\n",
							"\r\n",
							"// ##Define Schema Registry\r\n",
							"\r\n",
							"// from confluent_kafka.schema_registry import SchemaRegistryClient\r\n",
							"\r\n",
							"// schema_registry_conf = {\r\n",
							"\r\n",
							"//     'url': schemaRegistryUrl,\r\n",
							"\r\n",
							"//     'basic.auth.user.info': '{}:{}'.format(confluentRegistryApiKey, confluentRegistrySecret)}\r\n",
							"\r\n",
							"// schema_registry_client = SchemaRegistryClient(schema_registry_conf)\r\n",
							"\r\n",
							" \r\n",
							"\r\n",
							"// ##Import Library\r\n",
							"\r\n",
							"import spark.sql.functions\r\n",
							"\r\n",
							"import spark.sql.avro.functions.from_avro\r\n",
							"\r\n",
							"import spark.sql.types.StringType\r\n",
							"\r\n",
							"// binary_to_string = fn.udf(lambda x: str(int.from_bytes(x, byteorder='big')), StringType())\r\n",
							"\r\n",
							" \r\n",
							"\r\n",
							"// ##Create Spark Readstream\r\n",
							"\r\n",
							"// val clickstreamTestDf =spark.readStream.format(\"kafka\")\r\n",
							"\r\n",
							"val clickstreamTestDf = (\r\n",
							"  spark\r\n",
							"  .readStream\r\n",
							"  .format(\"kafka\")\r\n",
							"  .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\r\n",
							"  .option(\"kafka.security.protocol\", \"SASL_SSL\")\r\n",
							"  .option(\"kafka.sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(confluentApiKey, confluentSecret))\r\n",
							"  .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\r\n",
							"  .option(\"kafka.sasl.mechanism\", \"PLAIN\")\r\n",
							"  .option(\"subscribe\", confluentTopicName)\r\n",
							"  .option(\"startingOffsets\", \"earliest\")\r\n",
							"  .option(\"failOnDataLoss\", \"false\")\r\n",
							"  .load()\r\n",
							"  .select(\"topic\", \"partition\", \"offset\", \"timestamp\", \"timestampType\")\r\n",
							")\r\n",
							"\r\n",
							"// #Data within the Spark stream can be displayed dynamically from within the notebook.\r\n",
							"\r\n",
							"// ##Display \r\n",
							"\r\n",
							"// # display(clickstreamTestDf)"
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/data_lake_load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Data Lake Demo"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "demo31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "371f896b-dd93-4a98-8d13-a26e5d0782b2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/demo31",
						"name": "demo31",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/demo31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 60
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"location = 'dev'"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import col, desc, regexp_replace, substring, to_date, from_json, explode, expr\n",
							"from pyspark.sql.types import StructType, StringType\n",
							"\n",
							"print(location)\n",
							"\n",
							"yellow_source_path = \"wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow/puYear=2018/puMonth=*/*.parquet\"\n",
							"taxi_zone_source_path = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_zone_lookup.csv\"\n",
							"\n",
							"taxi_zone_path = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_zone\"\n",
							"taxi_rate_path = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_rate_code\"\n",
							"yellow_delta_path = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/tripdata/yellow_delta\"\n",
							"\n",
							"date_format = \"yyyy-MM-dd HH:mm:ss\"\n",
							"\n",
							"# Define a schema that Spark understands. This is one of several ways to do it.\n",
							"taxi_zone_schema = (\n",
							"  StructType()\n",
							"    .add('LocationID', 'integer')\n",
							"    .add('Borough', 'string')\n",
							"    .add('Zone', 'string')\n",
							"    .add('ServiceZone', 'string')\n",
							")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"zone_df = (\n",
							"  spark.read\n",
							"    .option(\"header\",\"true\")\n",
							"    .schema(taxi_zone_schema)\n",
							"    .csv(taxi_zone_source_path) \n",
							"  )\n",
							"\n",
							"zone_df.write.format(\"delta\").mode(\"overwrite\").save(taxi_zone_path)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"input_df = spark.read.parquet(yellow_source_path)\n",
							"\n",
							"# Take your pick on how to transform, withColumn or SQL Expressions. Only one of these is needed.\n",
							"\n",
							"# Option A\n",
							"# transformed_df = (\n",
							"#   input_df\n",
							"#     .withColumn(\"yearMonth\", regexp_replace(substring(\"tpepPickupDatetime\",1,7), '-', '_'))\n",
							"#     .withColumn(\"pickupDt\", to_date(\"tpepPickupDatetime\", date_format)) \n",
							"#     .withColumn(\"dropoffDt\", to_date(\"tpepDropoffDatetime\", date_format))\n",
							"#     .withColumn(\"tipPct\", col(\"tipAmount\") / col(\"totalAmount\"))\n",
							"# )\n",
							"  \n",
							"# Option B\n",
							"transformed_df = input_df.selectExpr(\n",
							"                  \"*\",\n",
							"                  \"replace(left(tpepPickupDatetime, 7),'-','_') as yearMonth\",\n",
							"                  f\"to_date(tpepPickupDatetime, '{date_format}') as pickupDt\",\n",
							"                  f\"to_date(tpepDropoffDatetime, '{date_format}') as dropoffDt\",\n",
							"                  f\"tipAmount/totalAmount as tipPct\")\n",
							"\n",
							"zone_df = spark.read.format(\"delta\").load(taxi_zone_path)\n",
							"\n",
							"# Join to bring in Taxi Zone data\n",
							"trip_df = (\n",
							"   transformed_df\n",
							"     .join(zone_df, transformed_df[\"PULocationID\"] == zone_df[\"LocationID\"], how=\"left\").drop(\"LocationID\")\n",
							"     .withColumnRenamed(\"Burough\", \"PickupBurrough\")\n",
							"     .withColumnRenamed(\"Zone\", \"PickupZone\")\n",
							"     .withColumnRenamed(\"ServiceZone\", \"PickupServiceZone\")\n",
							")\n",
							"\n",
							"trip_df.write.mode(\"overwrite\").partitionBy(\"yearMonth\").format(\"delta\").save(yellow_delta_path)\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/data_lake_load_dotnet')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Data Lake Demo"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "demo1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "csharp"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/demo1",
						"name": "demo1",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/demo1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "2.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## .NET Spark (C#): Load to Data Lake\n",
							"\n",
							"A good reference for additional syntax examples: https://github.com/dotnet/spark/blob/master/examples/Microsoft.Spark.CSharp.Examples/Sql/Batch/Basic.cs\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"using Microsoft.Spark.Sql;\n",
							"using Microsoft.Spark.Sql.Types;\n",
							"using static Microsoft.Spark.Sql.Functions;\n",
							"\n",
							"var yellowSourcePath = \"wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow/puYear=2018/puMonth=*/*.parquet\";\n",
							"var taxiZoneSourcePath = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_zone_lookup.csv\";\n",
							"\n",
							"var taxiZonePath = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_zone\";\n",
							"var taxiRatePath = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_rate_code\";\n",
							"var yellowDeltaPath = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/tripdata/yellow_delta\";\n",
							"\n",
							"var dateFormat = \"yyyy-MM-dd HH:mm:ss\";\n",
							"\n",
							"// Define a schema that Spark understands. This is one of several ways to do it.\n",
							"var taxiZoneSchema = new StructType(new[]\n",
							"{\n",
							"    new StructField(\"LocationID\", new IntegerType()),\n",
							"    new StructField(\"Borough\", new StringType()),\n",
							"    new StructField(\"Zone\", new StringType()),\n",
							"    new StructField(\"ServiceZone\", new StringType()),\n",
							"});"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"source": [
							"DataFrame zoneDF = spark.Read()\n",
							"    .Option(\"header\",\"true\")\n",
							"    .Schema(taxiZoneSchema)\n",
							"    .Csv(taxiZoneSourcePath); \n",
							"\n",
							"zoneDF.Write().Format(\"delta\").Mode(\"overwrite\").Save(taxiZonePath);\n",
							"\n",
							"zoneDF.Show();"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"source": [
							"DataFrame inputDF = spark.Read()\n",
							"    .Option(\"inferSchema\", \"true\")\n",
							"    .Parquet(yellowSourcePath);\n",
							"\n",
							"// Take your pick on how to transform, withColumn or SQL Expressions. Only one of these is needed.\n",
							"// Option A\n",
							"// var transformedDF = inputDF\n",
							"//     .WithColumn(\"yearMonth\", RegexpReplace(Substring(Col(\"tpepPickupDatetime\"),1,7), \"-\", \"_\"))\n",
							"//     .WithColumn(\"pickupDt\", ToDate(Col(\"tpepPickupDatetime\"), dateFormat)) \n",
							"//     .WithColumn(\"dropoffDt\", ToDate(Col(\"tpepDropoffDatetime\"), dateFormat))\n",
							"//     .WithColumn(\"tipPct\", Col(\"tipAmount\") / Col(\"totalAmount\"));\n",
							"  \n",
							"// Option B\n",
							"var transformedDF = inputDF.SelectExpr(\n",
							"                  \"*\",\n",
							"                  \"replace(left(tpepPickupDatetime, 7),\\\"-\\\",\\\"_\\\") as yearMonth\",\n",
							"                  $\"to_date(tpepPickupDatetime, \\\"{dateFormat}\\\") as pickupDt\",\n",
							"                  $\"to_date(tpepDropoffDatetime, \\\"{dateFormat}\\\") as dropoffDt\",\n",
							"                  $\"tipAmount/totalAmount as tipPct\");\n",
							"\n",
							"DataFrame zoneDF = spark.Read().Format(\"delta\").Load(taxiZonePath);\n",
							"\n",
							"// Join to bring in Taxi Zone data\n",
							"var tripDF = transformedDF\n",
							"     .Join(zoneDF, transformedDF[\"PULocationID\"] == zoneDF[\"LocationID\"], \"left\").Drop(\"LocationID\")\n",
							"     .WithColumnRenamed(\"Burough\", \"PickupBurrough\")\n",
							"     .WithColumnRenamed(\"Zone\", \"PickupZone\")\n",
							"     .WithColumnRenamed(\"ServiceZone\", \"PickupServiceZone\");\n",
							"\n",
							"tripDF.Write().Mode(\"overwrite\").PartitionBy(\"yearMonth\").Format(\"delta\").Save(yellowDeltaPath);"
						],
						"outputs": []
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Test read\n",
							"Simple test read of the delta formatted data that was just saved.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"var testDF = spark.Read().Format(\"delta\").Load(yellowDeltaPath).Limit(20);\n",
							"testDF.Select(\"VendorID\", \"tpepPickupDatetime\", \"tpepDropoffDatetime\", \"passengerCount\").Show();"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/data_lake_load_scala')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Data Lake Demo"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "demo2",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/demo2",
						"name": "demo2",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/demo2",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "2.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## Scala Spark: Load to Data Lake"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import org.apache.spark.sql.functions._\n",
							"import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}\n",
							"\n",
							"import spark.implicits._\n",
							"\n",
							"val yellowSourcePath = \"wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow/puYear=2018/puMonth=*/*.parquet\"\n",
							"val taxiZoneSourcePath = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_zone_lookup.csv\"\n",
							"\n",
							"val taxiZonePath = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_zone\"\n",
							"val taxiRatePath = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/lookups/taxi_rate_code\"\n",
							"val yellowDeltaPath = \"abfss://demo@datakickstartadls.dfs.core.windows.net/nyctaxi/tripdata/yellow_delta\"\n",
							"\n",
							"val dateFormat = \"yyyy-MM-dd HH:mm:ss\"\n",
							"\n",
							"// Define a schema that Spark understands. This is one of several ways to do it.\n",
							"val taxiZoneSchema = StructType(Seq(\n",
							"    StructField(\"LocationID\", IntegerType),\n",
							"    StructField(\"Borough\", StringType),\n",
							"    StructField(\"Zone\", StringType),\n",
							"    StructField(\"ServiceZone\", StringType)\n",
							"))"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"val zoneDF = spark.read.option(\"header\",\"true\").schema(taxiZoneSchema).csv(taxiZoneSourcePath) \n",
							"\n",
							"zoneDF.write.format(\"delta\").mode(\"overwrite\").save(taxiZonePath)\n",
							"\n",
							"zoneDF.show()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"val inputDF = spark.read.parquet(yellowSourcePath)\n",
							"\n",
							"// Take your pick on how to transform, withColumn or SQL Expressions. Only one of these is needed.\n",
							"\n",
							"// Option A\n",
							"// val transformedDF = {\n",
							"//     inputDF\n",
							"//      .withColumn(\"yearMonth\", regexp_replace(substring(\"tpepPickupDatetime\",1,7), '-', '_'))\n",
							"//      .withColumn(\"pickupDt\", to_date(\"tpepPickupDatetime\", dateFormat)) \n",
							"//      .withColumn(\"dropoffDt\", to_date(\"tpepDropoffDatetime\", dateFormat))\n",
							"//      .withColumn(\"tipPct\", col(\"tipAmount\") / col(\"totalAmount\"))\n",
							"// }\n",
							"\n",
							"// Option B\n",
							"val transformedDF = inputDF.selectExpr(\n",
							"                  \"*\",\n",
							"                  \"replace(left(tpepPickupDatetime, 7),'-','_') as yearMonth\",\n",
							"                  s\"to_date(tpepPickupDatetime, '$dateFormat') as pickupDt\",\n",
							"                  s\"to_date(tpepDropoffDatetime, '$dateFormat') as dropoffDt\",\n",
							"                  \"tipAmount/totalAmount as tipPct\")\n",
							"\n",
							"val zoneDF = spark.read.format(\"delta\").load(taxiZonePath)\n",
							"\n",
							"// Join to bring in Taxi Zone data\n",
							"val tripDF = {\n",
							"    transformedDF.as(\"t\")\n",
							"        .join(zoneDF.as(\"z\"), expr(\"t.PULocationID == z.LocationID\"), joinType=\"left\").drop(\"LocationID\")\n",
							"        .withColumnRenamed(\"Burough\", \"PickupBurrough\")\n",
							"        .withColumnRenamed(\"Zone\", \"PickupZone\")\n",
							"        .withColumnRenamed(\"ServiceZone\", \"PickupServiceZone\")\n",
							"}\n",
							"\n",
							"tripDF.write.mode(\"overwrite\").partitionBy(\"yearMonth\").format(\"delta\").save(yellowDeltaPath)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Test read\n",
							"Simple test read of the delta formatted data that was just saved."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"val testDF = spark.read.format(\"delta\").load(yellowDeltaPath).limit(20)\n",
							"testDF.select(\"VendorID\", \"tpepPickupDatetime\", \"tpepDropoffDatetime\", \"passengerCount\").show()"
						],
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/eventhubs_spark_scala')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Streaming"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "tobogganCustom",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a88408f1-76c1-4378-8df7-8755f467dc23"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/tobogganCustom",
						"name": "tobogganCustom",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/tobogganCustom",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\r\n",
							"// import org.apache.spark.sql.internal.SQLConf\r\n",
							"// var s = SQLConf.get.useDeprecatedKafkaOffsetFetching"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\r\n",
							"import org.apache.spark.eventhubs._\r\n",
							"import org.apache.spark.sql.SparkSession\r\n",
							"import org.apache.spark.sql.streaming.Trigger\r\n",
							"\r\n",
							"val conString = \"Endpoint=sb://streaming-demo-eh.servicebus.windows.net/;SharedAccessKeyName=dv_laptop_policy;SharedAccessKey=XfiEYwsSy/BnfxEDaJyxfjg/7b80+PeVSCx2RjI8zgw=;EntityPath=stream-demo-1\"\r\n",
							"val eventHubsConf = EventHubsConf(conString).setStartingPosition(EventPosition.fromEndOfStream)\r\n",
							"\r\n",
							"// Create a stream that reads data from the specified Event Hub.\r\n",
							"val spark = SparkSession.builder.appName(\"SimpleStream\").getOrCreate()\r\n",
							"val eventHubStream = {spark.readStream\r\n",
							"   .format(\"eventhubs\")\r\n",
							"   .options(eventHubsConf.toMap)\r\n",
							"   .load()\r\n",
							"   }\r\n",
							"\r\n",
							"val query = {\r\n",
							"   eventHubStream.writeStream\r\n",
							"   .option(\"checkpointLocation\", \"abfss://demo@datakickstartadls.dfs.core.windows.net/checkpoints/confluent13\")\r\n",
							"  .format(\"delta\")\r\n",
							"  .outputMode(\"append\")\r\n",
							"  .trigger(Trigger.ProcessingTime(\"10 seconds\"))\r\n",
							"  .start(\"abfss://demo@datakickstartadls.dfs.core.windows.net/confluent_test13\")\r\n",
							"}\r\n",
							"\r\n",
							"query.processAllAvailable()\r\n",
							"query.stop()\r\n",
							"// query.awaitTermination()"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/logging_utils')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "utils"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "demo31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "61c6e826-27e6-470b-9fe4-87cd7b81049e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/demo31",
						"name": "demo31",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/demo31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"# Parameters:\r\n",
							"# Set test_run to True only when running tests. Keep it as False (default) for normal use\r\n",
							"test_run= True #False"
						],
						"outputs": [],
						"execution_count": 103
					},
					{
						"cell_type": "code",
						"source": [
							"from dataclasses import dataclass, asdict\r\n",
							"import time\r\n",
							"\r\n",
							"@dataclass\r\n",
							"class LogMessage():\r\n",
							"    message: str\r\n",
							"    job_name: str\r\n",
							"    destination_database_name: str = \"\"\r\n",
							"    destination_table_name: str = \"\"\r\n",
							"    source_table_name: str =\"\"\r\n",
							"    duration: str = \"0\"\r\n",
							"    job_start_time: str = \"\"\r\n",
							"    job_end_time: str = \"\"\r\n",
							"\r\n",
							"base_message = None\r\n",
							"\r\n",
							"def print_message(start=False, stop=False):\r\n",
							"    msg_str = '{\"message\": \"' + base_message.message + '\", \"job_name\": \"' + base_message.job_name + '\"'\r\n",
							"    if start or stop:\r\n",
							"        msg_str += ', \"destination\": \"' + base_message.destination_database_name + '.' + base_message.destination_table_name + '\"'\r\n",
							"    if stop:\r\n",
							"         msg_str += ', \"duration\": \"' + base_message.duration + '\"'\r\n",
							"    msg_str += '}'\r\n",
							"    return msg_str\r\n",
							"\r\n",
							"spark_log4j = sc._jvm.org.apache.log4j\r\n",
							"logger = spark_log4j.LogManager.getLogger(\"com.datakickstart.datakickstart_synapse\")"
						],
						"outputs": [],
						"execution_count": 112
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def start_logging(job_name, destination_database_name, destination_table_name, source_table_name):\r\n",
							"    job_start_time = time.perf_counter()\r\n",
							"    \r\n",
							"    global base_message\r\n",
							"    base_message = LogMessage(message=\"\", job_name=job_name, destination_database_name=destination_database_name, destination_table_name=destination_table_name, job_start_time=job_start_time)\r\n",
							"\r\n",
							"    base_message.message=f\"Starting logger for {job_name}\"\r\n",
							"    logger.info(print_message(start=True))"
						],
						"outputs": [],
						"execution_count": 105
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def stop_logging(job_name):\r\n",
							"    base_message.job_end_time = time.perf_counter()\r\n",
							"    base_message.duration = str(base_message.job_end_time - base_message.job_start_time)\r\n",
							"    base_message.message = f\"Stopping logger for {job_name}\"\r\n",
							"    logger.info(print_message(stop=True))"
						],
						"outputs": [],
						"execution_count": 106
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def log_informational_message(message):\r\n",
							"    base_message.message = message\r\n",
							"    logger.info(print_message())"
						],
						"outputs": [],
						"execution_count": 107
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def log_warning_message(message):\r\n",
							"    base_message.message = message\r\n",
							"    logger.warn(print_message())"
						],
						"outputs": [],
						"execution_count": 108
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def log_error_message(message):\r\n",
							"    base_message.message = message\r\n",
							"    logger.error(print_message())"
						],
						"outputs": [],
						"execution_count": 109
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"if test_run:\r\n",
							"    start_logging('log_test_job', 'log_test_destination_db', 'log_test_destination_table', 'log_test_source_table')\r\n",
							"    log_informational_message('Log test informational message')\r\n",
							"    stop_logging('log_test_job')"
						],
						"outputs": [],
						"execution_count": 113
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/logging_utils_json')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "utils"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "demo31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "a7f8d64d-d20c-4782-8d8e-b6d438704666"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/demo31",
						"name": "demo31",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/demo31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from dataclasses import dataclass\r\n",
							"from dataclasses_json import dataclass_json\r\n",
							"import time\r\n",
							"\r\n",
							"@dataclass_json\r\n",
							"@dataclass\r\n",
							"class LogMessage():\r\n",
							"    message: str\r\n",
							"    job_name: str\r\n",
							"    destination_database_name: str = \"\"\r\n",
							"    destination_table_name: str = \"\"\r\n",
							"    source_table_name: str =\"\"\r\n",
							"    duration: str = \"0\"\r\n",
							"    job_start_time: str = \"\"\r\n",
							"    job_end_time: str = \"\"\r\n",
							"\r\n",
							"base_message = None\r\n",
							"spark_log4j = sc._jvm.org.apache.log4j\r\n",
							"logger = spark_log4j.LogManager.getLogger(\"synapse_dataplatform\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def start_logging(job_name, destination_database_name, destination_table_name, source_table_name):\r\n",
							"    job_start_time = time.perf_counter()\r\n",
							"    \r\n",
							"    global base_message\r\n",
							"    base_message = LogMessage(message=\"\", job_name=job_name, destination_database_name=destination_database_name, destination_table_name=destinatoin_table_name)\r\n",
							"\r\n",
							"    base_message.message=f\"Starting logger for {job_name}\"\r\n",
							"    logger.info(base_message.to_json())"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def stop_logging(job_name):\r\n",
							"    base_message.job_end_time = time.perf_counter()\r\n",
							"    base_message.duration = base_message.job_end_time - base_message.job_start_time\r\n",
							"    base_message.message = f\"Stopping logger for {job_name}\"\r\n",
							"    logger.info(base_message.to_json())"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def log_informational_message(message):\r\n",
							"    base_message.message = message\r\n",
							"    logger.info(base_message.to_json())"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def log_warning_message(message):\r\n",
							"    base_message.message = message\r\n",
							"    logger.warn(base_message.to_json())"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def log_error_message(message):\r\n",
							"    base_message.message = message\r\n",
							"    logger.error(base_message.to_json())"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_consumerindex_raw')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "best_of_class_recruiting"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "demo31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "2e9f1e7d-b67a-4abd-bd03-dae57a3890f7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/demo31",
						"name": "demo31",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/demo31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Ingest CU data to raw\r\n",
							"The latsest consumer price index is provided publicly via the [Bureau of Labor Statistics website](https://www.bls.gov/cpi/data.htm). This script is to download and reload raw tables with the latest files. Currently there is no check on if the data has changed at the source.\r\n",
							"\r\n",
							"\r\n",
							"## Steps\r\n",
							"1. Create raw_cu database (if does not exist) \r\n",
							"2. Download all \"enabled\" files from website\r\n",
							"3. Save as parquet tables in raw \r\n",
							"4. Log timing for each step"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%%configure -f\r\n",
							"{\r\n",
							"    \"driverMemory\": \"28g\",\r\n",
							"    \"driverCores\": 4,\r\n",
							"    \"executorMemory\": \"28g\",\r\n",
							"    \"executorCores\": 4,\r\n",
							"    \"conf\":\r\n",
							"    {\r\n",
							"        \"spark.driver.maxResultSize\": \"10g\",\r\n",
							"        \"spark.sql.execution.arrow.pyspark.enabled\": \"false\",\r\n",
							"        \"spark.sql.shuffle.partitions\": 12  \r\n",
							"    }\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"keyvault_ls = \"demokv\""
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"\r\n",
							"raw_storage_base_path = mssparkutils.credentials.getSecretWithLS(keyvault_ls, 'raw-datalake-path')\r\n",
							"refined_storage_base_path =  mssparkutils.credentials.getSecretWithLS(keyvault_ls, 'refined-datalake-path')\r\n",
							"curated_storage_base_path =  mssparkutils.credentials.getSecretWithLS(keyvault_ls, 'curated-datalake-path')"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\r\n",
							"import requests\r\n",
							"import io\r\n",
							"\r\n",
							"\r\n",
							"def create_database(db_name, path, drop=False):\r\n",
							"    if drop:\r\n",
							"        spark.sql(f\"DROP DATABASE IF EXISTS {db_name} CASCADE;\")    \r\n",
							"    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION '{path}';\")\r\n",
							"\r\n",
							"def read_csv_from_web(url):\r\n",
							"    \"\"\"\r\n",
							"    Args:\r\n",
							"        url\r\n",
							"\r\n",
							"    Returns:\r\n",
							"        DataFrame, results of reading data with extra spaces stripped from column headers\r\n",
							"    \"\"\"\r\n",
							"    s = requests.get(url).content\r\n",
							"    c = pd.read_csv(url, delimiter='\\t', na_filter=False)\r\n",
							"\r\n",
							"    col_headers = []\r\n",
							"    for h in c.columns:\r\n",
							"        col_headers.append(h.strip())\r\n",
							"\r\n",
							"    return spark.createDataFrame(c, col_headers)\r\n",
							"\r\n",
							"def save_spark_table(df, table, format='parquet'):\r\n",
							"    #df.write.format('delta').mode('overwrite').save(raw_path)\r\n",
							"    # spark.sql(f\"DROP TABLE {table};\")\r\n",
							"    df.write.format(format).mode('overwrite').saveAsTable(table)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#raw_path = 'abfss://raw@datakickstartadls.dfs.core.windows.net/cu'\r\n",
							"raw_path = raw_storage_base_path + 'cu'\r\n",
							"\r\n",
							"create_database('raw_cu', raw_path)"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"file_map = {\r\n",
							"    'cu.series': 'series',\r\n",
							"    'cu.data.0.Current': 'current',\r\n",
							"    'cu.area': 'area',\r\n",
							"    # 'cu.base': 'base',\r\n",
							"    # 'cu.item': 'item',\r\n",
							"    # 'cu.period': 'period',    \r\n",
							"    # 'cu.periodicity': 'periodicity',\r\n",
							"    # 'cu.seasonal': 'seasonal',\r\n",
							"\r\n",
							"    # 'cu.data.1.AllItems': 'all_items',\r\n",
							"    # 'cu.data.2.Summaries': 'summaries',\r\n",
							"    # 'cu.data.3.AsizeNorthEast': 'asize_north_east',\r\n",
							"    # 'cu.data.4.AsizeNorthCentral': 'asize_north_central',\r\n",
							"    # 'cu.data.5.AsizeSouth': 'asize_south',\r\n",
							"    # 'cu.data.6.AsizeWest': 'asize_west',\r\n",
							"    # 'cu.data.7.OtherNorthEast': 'other_north_east',\r\n",
							"    # 'cu.data.8.OtherNorthCentral': 'other_north_central',\r\n",
							"    # 'cu.data.9.OtherSouth': 'other_south',\r\n",
							"    # 'cu.data.10.OtherWest': 'other_west',\r\n",
							"    # 'cu.data.11.USFoodBeverage': 'us_food_beverage',\r\n",
							"    # 'cu.data.12.USHousing': 'us_housing',\r\n",
							"    # 'cu.data.13.USApparel': 'us_apparel',\r\n",
							"    # 'cu.data.14.USTransportation': 'us_transportation',\r\n",
							"    # 'cu.data.15.USMedical': 'us_medical',\r\n",
							"    # 'cu.data.16.USRecreation': 'us_recreation',\r\n",
							"    # 'cu.data.17.USEducationAndCommunication': 'us_educational_and_communication',\r\n",
							"    # 'cu.data.18.USOtherGoodsAndServices': 'us_other_goods_and_services',\r\n",
							"    # 'cu.data.19.PopulationSize': 'population_size',\r\n",
							"    # 'cu.data.20.USCommoditiesServicesSpecial': 'us_commodities_services_special'\r\n",
							"    \r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# url = 'https://download.bls.gov/pub/time.series/cu/cu.data.0.Current'\r\n",
							"# destination_table = 'current'\r\n",
							"\r\n",
							"# source_df = read_csv_from_web(url)\r\n",
							"# display(source_df)"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for file, destination_table in file_map.items():\r\n",
							"    url = f'https://download.bls.gov/pub/time.series/cu/{file}'\r\n",
							"    source_df = read_csv_from_web(url)\r\n",
							"    \r\n",
							"    # FOR PROFILING AND EXPLORATION\r\n",
							"    print(destination_table)\r\n",
							"    # source_df.show()\r\n",
							"\r\n",
							"    save_spark_table(source_df,'raw_cu.'+destination_table)    "
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE OR REPLACE VIEW raw_cu.vw_current_full\r\n",
							"AS\r\n",
							"SELECT\r\n",
							"    c.year,\r\n",
							"    c.period,\r\n",
							"    s.*, \r\n",
							"    item.item_name,\r\n",
							"    area.area_name,\r\n",
							"    c.value\r\n",
							"FROM `current` c\r\n",
							"  JOIN series s\r\n",
							"    ON c.series_id = s.series_id\r\n",
							"  JOIN item\r\n",
							"    ON s.item_code = item.item_code\r\n",
							"  JOIN area\r\n",
							"    ON s.area_code = area.area_code"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT *\r\n",
							"FROM raw_cu.vw_current_full\r\n",
							"LIMIT 10"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"DESCRIBE FORMATTED raw_cu.area"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# from pyspark.sql.functions import input_file_name\r\n",
							"# current_df = spark.read.format('delta').load(raw_path).withColumn('path', input_file_name())\r\n",
							"# display(current_df)"
						],
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_jobposts_raw')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "best_of_class_recruiting"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "185512f4-fe0b-4296-a3ea-22ee785affc7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Raw streaming of Job Posts\r\n",
							"Do this one with Databricks."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nb_refined')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "best_of_class_recruiting"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4ea75375-7b78-417c-80c3-0cb9e6276331"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Delta refined and curated\r\n",
							"sql = 'CREATE DATABASE IF NOT EXISTS refined'"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nyctaxi_file_copy')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Data Lake Demo"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "demo31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "992a6507-8dec-47fa-b23b-a6e7cb73d301"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/demo31",
						"name": "demo31",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/demo31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.cp(\"wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow/puYear=2019\", \"/nyctaxi_raw/parquet/yellow/puYear=2019/\", recurse=True)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import col, desc, regexp_replace, substring, to_date, from_json, explode, expr\r\n",
							"from pyspark.sql.types import StructType, StringType\r\n",
							"\r\n",
							"yellow_source_path = \"wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/parquet/yellow/puYear=2019\"\r\n",
							"\r\n",
							"date_format = \"yyyy-MM-dd HH:mm:ss\"\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = spark.read.parquet(yellow_source_path)\r\n",
							"df.printSchema()"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.write.format(\"csv\").partitionBy(\"puMonth\").mode(\"overwrite\").option(\"compression\", \"gzip\").save(\"/nyctaxi_raw/csv/yellow/puYear=2019/\")"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/produce_stackoverflow_event')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "stackoverflow"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b0728121-cb96-4541-aa89-99c18987fef7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/spark32",
						"name": "spark32",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"source_base_path = \"abfss://demo@dvtrainingadls.dfs.core.windows.net/stackoverflow/\"\r\n",
							"raw_base_path = \"abfss://raw@datakickstartadls.dfs.core.windows.net/stackoverflow/\"\r\n",
							"refined_base_path = \"abfss://refined@datakickstartadls.dfs.core.windows.net/\"\r\n",
							"\r\n",
							"so_posts = spark.read.parquet(f\"{source_base_path}posts/CreationMonth=2022-06-01\")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"so_posts = so_posts.sort(col(\"_CreationDate\").asc())\r\n",
							"display(so_posts)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topic = \"stackoverflow_post\"\r\n",
							"GROUP_ID = \"so_v1\"\r\n",
							"\r\n",
							"def get_confluent_config(topic):\r\n",
							"    bootstrapServers = mssparkutils.credentials.getSecretWithLS('demokv', \"confluent-cloud-brokers\")\r\n",
							"    confluentApiKey =  mssparkutils.credentials.getSecretWithLS('demokv', 'confluent-cloud-user')\r\n",
							"    confluentSecret =  mssparkutils.credentials.getSecretWithLS('demokv', 'confluent-cloud-password')\r\n",
							"    confluentTopicName = \"stackoverflow_post\"\r\n",
							"\r\n",
							"    options = {\r\n",
							"        \"kafka.bootstrap.servers\": bootstrapServers,\r\n",
							"        \"kafka.security.protocol\": \"SASL_SSL\",\r\n",
							"        \"kafka.ssl.endpoint.identification.algorithm\": \"https\",\r\n",
							"        \"kafka.sasl.jaas.config\": \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(confluentApiKey, confluentSecret),\r\n",
							"        \"kafka.sasl.mechanism\": \"PLAIN\",\r\n",
							"        \"topic\": confluentTopicName\r\n",
							"    }\r\n",
							"\r\n",
							"    return options"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topic = 'stackoverflow-post'\r\n",
							"config = get_confluent_config(topic)\r\n",
							"\r\n",
							"min_dt = None\r\n",
							"\r\n",
							"for i in range(20):\r\n",
							"    if min_dt:\r\n",
							"        df = so_posts.where(col(\"_CreationDate\")< min_dt).limit(10)\r\n",
							"    else:\r\n",
							"        df = so_posts.limit(10)\r\n",
							"    min_dt = df.selectExpr(\"min(_CreationDate) as minCreationDate\").first().minCreationDate\r\n",
							"    df.selectExpr(\"cast(_Id as String) as key\", \"to_json(struct(*)) as value\").write.format(\"kafka\").options(**config).save()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pyspark_logging')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Demo Miscellaneous"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkLogging2",
					"type": "BigDataPoolReference"
				},
				"targetSparkConfiguration": {
					"referenceName": "sparkLogging_default",
					"type": "SparkConfigurationReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1f88be1e-6fc4-480d-82d9-b1c4214c30ad"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/sparkLogging2",
						"name": "sparkLogging2",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkLogging2",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30,
					"targetSparkConfiguration": "sparkLogging_default"
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"spark_log4j = sc._jvm.org.apache.log4j\r\n",
							"logger = spark_log4j.LogManager.getLogger(\"com.datakickstart.datakickstart_synapse\")\r\n",
							"\r\n",
							"# logger = spark_log4j.LogManager.getLogger(\"com.microsoft.azure.synapse.datakickstart\")"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"logger.info(\"Test simple log message\")"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import json\r\n",
							"msg = {\r\n",
							"    \"message\":\"Test simple log message\", \r\n",
							"    \"notebook\": \"pyspark_logging\", \r\n",
							"    \"source_data\": \"StackOverflow\", \r\n",
							"    \"destination_data\": \"raw_stackoverflow\"\r\n",
							"}\r\n",
							"logger.info(json.dumps(msg))"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df3 = spark.createDataFrame([[\"test1\"],[\"test5\"]])"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df3.show()"
						],
						"outputs": [],
						"execution_count": 23
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sparkConfigurations/sparkLogging_default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/python_beautifulsoup')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Demo Miscellaneous"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "pythondefault",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e943ec1b-0e32-4486-a4c8-022411ea4f0e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/pythondefault",
						"name": "pythondefault",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pythondefault",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import requests\r\n",
							"import pandas as pd\r\n",
							"import openpyxl\r\n",
							"import json\r\n",
							"\r\n",
							"import requests\r\n",
							"# import pytest\r\n",
							"from bs4 import BeautifulSoup"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# https://realpython.com/beautiful-soup-web-scraper-python/\r\n",
							"\r\n",
							"URL = \"https://realpython.github.io/fake-jobs/\"\r\n",
							"page = requests.get(URL)\r\n",
							"\r\n",
							"soup = BeautifulSoup(page.content, \"html.parser\")\r\n",
							"results = soup.find(id=\"ResultsContainer\")\r\n",
							"# print(results.prettify())\r\n",
							"job_elements = results.find_all(\"div\", class_=\"card-content\")\r\n",
							"\r\n",
							"for job_element in job_elements:\r\n",
							"    title_element = job_element.find(\"h2\", class_=\"title\")\r\n",
							"    company_element = job_element.find(\"h3\", class_=\"company\")\r\n",
							"    location_element = job_element.find(\"p\", class_=\"location\")\r\n",
							"    print(title_element.text.strip())\r\n",
							"    print(company_element.text.strip())\r\n",
							"    print(location_element.text.strip())\r\n",
							"    print()"
						],
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/python_custom')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Demo Miscellaneous"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "demo2022",
					"type": "BigDataPoolReference"
				},
				"targetSparkConfiguration": {
					"referenceName": "sparkLogging",
					"type": "SparkConfigurationReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "41cf2d47-d4d3-4097-a717-de8efcd30d79"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/demo2022",
						"name": "demo2022",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/demo2022",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30,
					"targetSparkConfiguration": "sparkLogging"
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"def dataframe_summary(df, value_col):\r\n",
							"    try:\r\n",
							"        summary = df.selectExpr(\r\n",
							"            f\"count({value_col}) as row_count\",\r\n",
							"            f\"min({value_col}) as min_value\",\r\n",
							"            f\"max({value_col}) as max_value\")\r\n",
							"        result = summary.first()\r\n",
							"    except AnalysisException as err:\r\n",
							"        if str(err).find(f\"cannot resolve '`{value_col}`' given input columns\") >= 0:\r\n",
							"            raise KeyError(f\"Column {value_col} does not exist in the DataFrame.\")\r\n",
							"        else:\r\n",
							"            raise err\r\n",
							"    return (result.row_count, result.min_value, result.max_value)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pytest\r\n",
							"import ipytest\r\n",
							"\r\n",
							"def sample_df(vals):\r\n",
							"    col_names = [\"name\", \"val\"]\r\n",
							"    data = [[\"test \" + str(v), v] for v in vals]\r\n",
							"    df = spark.createDataFrame(data, col_names)\r\n",
							"    return df\r\n",
							"\r\n",
							"@pytest.fixture(scope=\"session\")\r\n",
							"def input_df():\r\n",
							"    return sample_df([1, 2])\r\n",
							"\r\n",
							"def test_dataframe_summary_valid_column(input_df):\r\n",
							"    # input_df = sample_df([1, 2])\r\n",
							"    result = (2, 1, 2)\r\n",
							"    print(input_df, result)\r\n",
							"    assert result == dataframe_summary(input_df, \"val\")\r\n",
							"\r\n",
							"def test_dataframe_summary_invalid_column(input_df):\r\n",
							"    # input_df = sample_df([1, 2])\r\n",
							"    result = (2, 1, 2)\r\n",
							"    print(input_df, result)\r\n",
							"    with pytest.raises(KeyError):\r\n",
							"        result == dataframe_summary(input_df, \"fake_col\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ipytest.autoconfig()\r\n",
							"\r\n",
							"ipytest.run()"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from datakickstart_utils import pyspark_utils\r\n",
							"logger = pyspark_utils.get_log4j_logger(sc)\r\n",
							"logger.info(\"Test log message\")"
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sparkConfigurations/sparkLogging')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/scala_tests')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Demo Miscellaneous"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "tobogganCustom",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7d05053b-63a8-401d-882c-e951548643cc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/tobogganCustom",
						"name": "tobogganCustom",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/tobogganCustom",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"t = mssparkutils.credentials.getSecret(\"dvtrainingkv\", \"confluent-cloud-brokers\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"val t = mssparkutils.credentials.getSecret(\"dvtrainingkv\", \"confluent-cloud-brokers\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/stackoverflow_analysis')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "stackoverflow"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkLogging",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "924fda26-32ea-46a2-bbf8-1731e01207e7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/sparkLogging",
						"name": "sparkLogging",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkLogging",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 10
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"WITH post_comments (\r\n",
							"  Select \r\n",
							"    _PostId,\r\n",
							"    count(1) CommentsCount,\r\n",
							"    avg(_Score) AvgCommentScore\r\n",
							"  From raw_stackoverflow.comments\r\n",
							"  group by _PostId\r\n",
							")\r\n",
							"Select\r\n",
							"    posts._PostTypeId\r\n",
							"    ,users._Reputation\r\n",
							"    ,min(posts._CreationDate) FirstCreationDate\r\n",
							"    ,max(_LastActivityDate) LastActivityDate\r\n",
							"    ,max(_ClosedDate) LastClosedDate\r\n",
							"    ,count(_ClosedDate) ClosedCount\r\n",
							"    ,sum(_FavoriteCount) FavoriteCount\r\n",
							"    ,sum(_CommentCount) CommentCount\r\n",
							"    ,sum(_AnswerCount) AnswerCount\r\n",
							"    ,sum(_Score) ScoreTotal\r\n",
							"    ,sum(_ViewCount) ViewCount\r\n",
							"    ,count(1) Posts\r\n",
							"    ,sum(CommentsCount) CommentsCount\r\n",
							"    ,max(AvgCommentScore) AvgCommentScore\r\n",
							"FROM raw_stackoverflow.posts\r\n",
							"  JOIN raw_stackoverflow.users\r\n",
							"    ON posts._OwnerUserId = users._Id\r\n",
							"  JOIN post_comments\r\n",
							"    ON posts._Id = post_comments._PostId\r\n",
							"GROUP BY _PostTypeId, users._Reputation"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/stackoverflow_curated')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "stackoverflow"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "demo2022",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0bd4189e-84d4-4293-a74d-9d9da476849d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/demo2022",
						"name": "demo2022",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/demo2022",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## StackOverflow curated example\r\n",
							"This notebook will create a curated table in our replicated lake database.\r\n",
							"\r\n",
							"Steps:\r\n",
							"1. Query refined using Spark sql.\r\n",
							"2. Replace data in destination table"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import current_timestamp\r\n",
							"curated_base_path = \"abfss://curated@datakickstartadls.dfs.core.windows.net/\""
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run utils/logging_utils"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"job_name = \"stackoverflow_curated\"\r\n",
							"destination_database_name = \"curated\"\r\n",
							"destination_table_name = \"dim_user\"\r\n",
							"source_table_name = \"stackoverflow_refined\"\r\n",
							"start_logging(job_name, destination_database_name, destination_table_name, source_table_name)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(f\"DROP DATABASE IF EXISTS curated\")\r\n",
							"spark.sql(f\"CREATE DATABASE IF NOT EXISTS curated LOCATION '{curated_base_path}/spark_tables'\")"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = spark.sql(\"\"\"\r\n",
							"select\r\n",
							"  _Id as id,\r\n",
							"  ifnull(_AccountId,0) as account_id,\r\n",
							"  _DisplayName as display_name,\r\n",
							"  _Location as location,\r\n",
							"  _WebsiteUrl as website_url,\r\n",
							"  _CreationDate as creation_date,\r\n",
							"  _LastAccessDate as last_access_date\r\n",
							"from refined.users\r\n",
							"where _LastAccessDate >= '2021-01-01 00:00:00'\r\n",
							"\"\"\").cache()\r\n",
							"\r\n",
							"print(df.count())"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"curated.stackoverflow_user\")"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stop_logging(job_name)"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/stackoverflow_db_ingest_parallel')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "stackoverflow"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "pythonsession",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "122f94ab-8cf1-4f75-8035-dbc8aaf360eb"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/pythonsession",
						"name": "pythonsession",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pythonsession",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 10
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"database = \"StackOverflow2010\"\r\n",
							"db_host_name = \"sandbox-2-sqlserver.database.windows.net\"\r\n",
							"db_url = f\"jdbc:sqlserver://{db_host_name};databaseName={database}\"\r\n",
							"db_user = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"sql-user-stackoverflow\")\r\n",
							"db_password = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"sql-pwd-stackoverflow\")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(f\"CREATE DATABASE IF NOT EXISTS raw_stackoverflow LOCATION '/demo/raw_stackoverflow'\")\r\n",
							"table_list = [\"Badges\", \"Comments\", \"LinkTypes\", \"PostLinks\", \"Posts\", \"PostTypes\", \"Users\", \"Votes\", \"VoteTypes\"]\r\n",
							"\r\n",
							"def load_table(table):\r\n",
							"    print(table)\r\n",
							"    destination_table = \"raw_stackoverflow.\" + table\r\n",
							"\r\n",
							"    df = (\r\n",
							"        spark.read\r\n",
							"        .format(\"com.microsoft.sqlserver.jdbc.spark\")\r\n",
							"        .option(\"url\", db_url)\r\n",
							"        .option(\"dbtable\", table)\r\n",
							"        .option(\"user\", db_user)\r\n",
							"        .option(\"password\", db_password)\r\n",
							"        .load()\r\n",
							"    )\r\n",
							"\r\n",
							"    df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(destination_table)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"from threading import Thread\r\n",
							"from queue import Queue\r\n",
							"\r\n",
							"q = Queue()\r\n",
							"\r\n",
							"worker_count = 2\r\n",
							"\r\n",
							"def run_tasks(function, q):\r\n",
							"    while not q.empty():\r\n",
							"        value = q.get()\r\n",
							"        function(value)\r\n",
							"        q.task_done()\r\n",
							"\r\n",
							"\r\n",
							"print(table_list)\r\n",
							"\r\n",
							"for table in table_list:\r\n",
							"    q.put(table)\r\n",
							"\r\n",
							"for i in range(worker_count):\r\n",
							"    t=Thread(target=run_tasks, args=(load_table, q))\r\n",
							"    t.daemon = True\r\n",
							"    t.start()\r\n",
							"\r\n",
							"print(\"Running load\")\r\n",
							"q.join()\r\n",
							"print(\"Load completed\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/stackoverflow_ingest_adls')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "stackoverflow"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark33",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c04fc7e6-00ae-4f9f-b192-174a1105fee3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/spark33",
						"name": "spark33",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark33",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## StackOverflow file ingest to Raw and Refined\r\n",
							"This notebook will ingest data from the external ADLS environment to our data lake.\r\n",
							"\r\n",
							"Steps:\r\n",
							"1. Read all tables from source (external) ADLS path.\r\n",
							"2. Save in raw form as parquet tables in raw_stackoverflow database.\r\n",
							"3. Enrich, transform, and save as delta tables in the refined database."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import current_timestamp\r\n",
							"\r\n",
							"source_base_path = \"abfss://demo@dvtrainingadls.dfs.core.windows.net/stackoverflow/\"\r\n",
							"raw_base_path = \"abfss://raw@datakickstartadls.dfs.core.windows.net/stackoverflow/\"\r\n",
							"refined_base_path = \"abfss://refined@datakickstartadls.dfs.core.windows.net/\""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run utils/logging_utils"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"job_name = \"stackoverflow_ingest\"\r\n",
							"destination_database_name = \"raw\"\r\n",
							"destination_table_name = \"\"\r\n",
							"source_table_name = \"stackoverflow_parquet_files\"\r\n",
							"start_logging(job_name, destination_database_name, destination_table_name, source_table_name)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# This is the simple Apache Spark code that represents what we are doing\r\n",
							"# df = spark.read.parquet('abfss://demo@dvtrainingadls.dfs.core.windows.net/stackoverflow/badges')\r\n",
							"# df.saveAsTable('raw_stackoverflow.badges')"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"def get_table_paths(base_path):\r\n",
							"    \"\"\"Return list of items for file path and folder/table.\r\n",
							"\r\n",
							"    Args:\r\n",
							"        base_path (str): Azure path to get data directories (must be authenticated), \r\n",
							"            formatted abfss://<container>@<storage_account>.dfs.core.windows.net/[root_directory]\r\n",
							"\r\n",
							"    Returns:\r\n",
							"        list[(path, table_name)]: List of tuples, each item has full path in ADLS and the folder name (usually used as table name)\r\n",
							"    \r\n",
							"    \"\"\"\r\n",
							"    table_paths = mssparkutils.fs.ls(base_path)\r\n",
							"    return [(file_info.path, file_info.name) for file_info in table_paths]\r\n",
							"\r\n",
							"\r\n",
							"def save_table(df, table, partition_str=None, format=\"delta\"):\r\n",
							"    \"\"\"Save Spark table using format provided (parquet or delta).\r\n",
							"    \r\n",
							"    Args:\r\n",
							"        df (DataFrame): Data to save as Spark table.\r\n",
							"        table (str): Name of destination table\r\n",
							"        partition_str (str, optional): Column names used to partition folders as comma delimited string, for example 'colA,colB'. No partitions created if not provided.\r\n",
							"        format (str, optional): Defaults to 'delta'\r\n",
							"\r\n",
							"    Returns:\r\n",
							"        None\r\n",
							"\r\n",
							"    \"\"\"\r\n",
							"    options = {\r\n",
							"        \"mergeSchema\": \"true\"\r\n",
							"    }\r\n",
							"    writer = df.write.mode(\"overwrite\").options(**options).format(format)\r\n",
							"    if partition_str:\r\n",
							"        writer = writer.partitionBy(partition_str)\r\n",
							"    writer.saveAsTable(table)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(f\"CREATE DATABASE IF NOT EXISTS raw_stackoverflow LOCATION '{raw_base_path}'\")\r\n",
							"\r\n",
							"table_list = get_table_paths(source_base_path)\r\n",
							"\r\n",
							"print(table_list)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for path, table in table_list:  \r\n",
							"    df = spark.read.parquet(path)\r\n",
							"    save_table(df, \"raw_stackoverflow.\" + table, format=\"parquet\")\r\n",
							"    log_informational_message(f\"Saved raw table {table}\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"CREATE DATABASE IF NOT EXISTS refined LOCATION 'abfss://refined@datakickstartadls.dfs.core.windows.net/refined'\")\r\n",
							"\r\n",
							"table_list = get_table_paths(raw_base_path)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# for path, table in table_list:  \r\n",
							"#     df = spark.read.parquet(path)\r\n",
							"#     df = df.withColumn(\"refresh_date\", current_timestamp())\r\n",
							"#     save_table(df, \"refined.\" + table, format=\"delta\")\r\n",
							"#     log_informational_message(f\"Saved refined table {table}\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stop_logging(job_name)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/stackoverflow_ingest_api')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "stackoverflow"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark32",
					"type": "BigDataPoolReference"
				},
				"targetSparkConfiguration": {
					"referenceName": "sparkConfigurationSmall",
					"type": "SparkConfigurationReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "12dd8ab1-abec-4e1f-bfce-e0a78fb1689d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/spark32",
						"name": "spark32",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 10,
					"targetSparkConfiguration": "sparkConfigurationSmall"
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "7f90409f-a685-48d8-9fb5-2b6838a2e7d4"
							}
						},
						"source": [
							"import json\n",
							"import sys\n",
							"import time\n",
							"import requests\n",
							"\n",
							"source_base_path = \"abfss://demo@dvtrainingadls.dfs.core.windows.net/stackoverflow/\"\n",
							"raw_base_path = \"abfss://raw@datakickstartadls.dfs.core.windows.net/stackoverflow/\"\n",
							"refined_base_path = \"abfss://refined@datakickstartadls.dfs.core.windows.net/\""
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run utils/logging_utils"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"job_name = 'stackoverflow_ingest_api'\r\n",
							"destination_database = 'raw_stackoverflow'\r\n",
							"start_logging(job_name, destination_database, \"questions_with_user\", \"stackoverflow_api\")\r\n",
							"\r\n",
							"base_url = \"https://api.stackexchange.com/2.2\"\r\n",
							"question_endpoint = base_url + \"/questions\"\r\n",
							"users_endpoint = base_url + \"/users\""
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "823a1bee-4de9-4dfc-a979-48e498666f23"
							}
						},
						"source": [
							"def get_questions(endpoint, fromdate, todate):\n",
							"    tags = \"pyspark\" # post must match all tags, consider separate calls for each of these: pyspark; apache-spark-sql; apache-spark\n",
							"    query_string = f\"?fromdate={str(fromdate)}&todate={str(todate)}&order=desc&min=1&sort=votes&tagged={tags}&filter=default&site=stackoverflow&run=true\" #tagged=python&site=stackoverflow\"\n",
							"    r = requests.get(f\"{endpoint}{query_string}\")\n",
							"    if r.status_code == 200:\n",
							"        return r.text\n",
							"    else:\n",
							"        return r\n",
							"    \n",
							"def get_users(endpoint, id_list):\n",
							"    ids_string = ';'.join(list(id_list))\n",
							"    logger.debug(\"User ids:\" + ids_string)\n",
							"    ids = \"/\" + ids_string\n",
							"    query_string = \"?site=stackoverflow\"\n",
							"    r = requests.get(f\"{endpoint}{ids}{query_string}\")\n",
							"    if r.status_code == 200:\n",
							"        return r.text\n",
							"    else:\n",
							"        return r"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "cd7f7999-ad3b-4e68-8b0e-6f2a154ce41e"
							}
						},
						"source": [
							"# Get Questions\n",
							"todate = int(time.time()) # - 18000  # test value: \"1619178204\"\n",
							"fromdate = todate - 300000 #100000  # test value: \"1617108204\"\n",
							"\n",
							"response = get_questions(question_endpoint, fromdate, todate)\n",
							"response_json = json.loads(response)\n",
							"question_list = response_json[\"items\"]\n",
							"\n",
							"# Remove items and print response metadata\n",
							"del response_json[\"items\"]\n",
							"metadata = response_json\n",
							"logger.info(f\"API metadata: {metadata}\")"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "f4aa906f-e98d-48dc-9f2f-2ec080819fe3"
							}
						},
						"source": [
							"print(\"Question count:\", len(question_list))\n",
							"print(question_list[0])"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "fbd4a304-a1bc-47ba-b167-50dedfcedb53"
							},
							"collapsed": false
						},
						"source": [
							"question_df = spark.createDataFrame(question_list)\n",
							"display(question_df)"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "f66909b3-38b0-45af-98bd-e75594eabb24"
							}
						},
						"source": [
							"# Build unique set of users\n",
							"question_rows = question_df.collect()\n",
							"user_ids = set()\n",
							"for row in question_rows:\n",
							"    user_ids.add(str(row['owner']['user_id']))\n",
							"\n",
							"print(user_ids)"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "72b5c1e8-6559-48a2-9958-a54aa49c98f5"
							}
						},
						"source": [
							"# Get Users\n",
							"users_response = get_users(users_endpoint, user_ids)\n",
							"users_json = json.loads(users_response)\n",
							"users_list = users_json[\"items\"]\n",
							"\n",
							"print(users_list[0])"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "4fd26baf-95dc-4961-b8d9-4aa15a033565"
							},
							"collapsed": false
						},
						"source": [
							"# Create Users DataFrame\n",
							"user_df = spark.createDataFrame(users_list).withColumnRenamed(\"creation_date\", \"user_creation_date\").withColumnRenamed(\"link\", \"user_link\")\n",
							"user_df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").save(f\"{refined_base_path}stackoverflow_new_users\")\n",
							"display(user_df)"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "16e75e73-0550-46e5-b0c4-fc0c36e7daf2"
							},
							"collapsed": false
						},
						"source": [
							"#Join\n",
							"combined_df = question_df.join(user_df, question_df[\"owner.user_id\"] == user_df[\"user_id\"], how=\"left\")\n",
							"combined_df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").save(f\"{refined_base_path}questions_with_user\")\n",
							"display(combined_df)"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "acf5d186-eec2-47b5-9c60-8aae0c240591"
							},
							"collapsed": false
						},
						"source": [
							"test_df = spark.read.format(\"delta\").load(f\"{refined_base_path}questions_with_user\").limit(30)\n",
							"display(test_df)"
						],
						"outputs": [],
						"execution_count": 37
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sparkConfigurations/sparkConfigurationSmall')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/stackoverflow_refined')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "stackoverflow"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark33",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cf96ee4e-2828-4c76-9247-102c772e50c9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/spark33",
						"name": "spark33",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark33",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run utils/logging_utils"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import lit, col\r\n",
							"from datetime import datetime\r\n",
							"\r\n",
							"load_time = datetime.now()\r\n",
							"raw_base_path = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"raw-datalake-path\") + \"stackoverflow\"\r\n",
							"refined_base_path = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"refined-datalake-path\") + \"stackoverflow\"\r\n",
							"raw_format = \"parquet\"\r\n",
							"refined_format = \"delta\"\r\n",
							"job_name = \"stackoverflow_refined\"\r\n",
							"\r\n",
							"# logger = start_logging(job_name, destination_database_name, destination_table_name, source_table_name"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# def create_database(db_name, path, drop=False):\r\n",
							"#     if drop:\r\n",
							"#         spark.sql(f\"DROP DATABASE IF EXISTS {db_name} CASCADE;\")    \r\n",
							"#     spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION '{path}'\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Load refined tables concurrently"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def load_table(args):\r\n",
							"    status = mssparkutils.notebook.run(\"stackoverflow_refined_table_load\", 1800, arguments=args)\r\n",
							"    if status != 'success':\r\n",
							"        raise Exception(f\"Failed to load refined database. Status: {str(status)}\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from threading import Thread\r\n",
							"from queue import Queue\r\n",
							"\r\n",
							"q = Queue()\r\n",
							"worker_count = 3\r\n",
							"errors = {}\r\n",
							"\r\n",
							"def run_tasks(function, q):\r\n",
							"    while not q.empty():\r\n",
							"        try:\r\n",
							"            value = q.get()\r\n",
							"            function(value)\r\n",
							"        except Exception as e:\r\n",
							"            table = value.get(\"table\", \"UNKNOWN TABLE\")\r\n",
							"            msg = f\"Error processing table {table}: {str(e)}\"\r\n",
							"            errors[value] = e\r\n",
							"            log_error_message(msg)\r\n",
							"        finally:\r\n",
							"            q.task_done()\r\n",
							"\r\n",
							"\r\n",
							"table_list = [\r\n",
							"    {\"table\": \"badges\", \"id\": \"id\", \r\n",
							"     \"columns\": \"_Class as class,_Date as event_date,_Id as id,_Name as name,_UserId as userid\"\r\n",
							"    },\r\n",
							"    {\"table\": \"comments\", \"id\": \"id\", \r\n",
							"     \"columns\": \"_ContentLicense as content_license,_CreationDate as creation_date,_Id as id,_PostId as post_id,_Score as score,_Text as text,_UserDisplayName as user_display_name,_UserId as user_id\"\r\n",
							"    }\r\n",
							"]\r\n",
							"\r\n",
							"print(table_list)\r\n",
							"\r\n",
							"for table_args in table_list:\r\n",
							"    q.put(table_args)\r\n",
							"\r\n",
							"for i in range(worker_count):\r\n",
							"    t=Thread(target=run_tasks, args=(load_table, q))\r\n",
							"    t.daemon = True\r\n",
							"    t.start()\r\n",
							"\r\n",
							"q.join()\r\n",
							"\r\n",
							"if len(errors) == 0:\r\n",
							"    log_informational_message(\"All tasks completed successfully.\")\r\n",
							"elif len(errors) > 0:\r\n",
							"    msg = f\"Errors during tasks {list(errors.keys())} -> \\n {str(errors)}\"\r\n",
							"    raise Exception(msg)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stop_logging(job_name)"
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/stackoverflow_refined_table_load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "stackoverflow"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark33",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "763c498f-1053-4b6a-ab65-451339e16d83"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/spark33",
						"name": "spark33",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark33",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"table = \"badges\"\r\n",
							"id = \"id\"\r\n",
							"columns = \"_Class as class,_Date as event_date,_Id as id,_Name as name,_UserId as userid\""
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run utils/logging_utils"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import lit, col\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"from datetime import datetime\r\n",
							"\r\n",
							"load_time = datetime.now()\r\n",
							"raw_base_path = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"raw-datalake-path\") + \"stackoverflow\"\r\n",
							"refined_base_path = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"refined-datalake-path\") + \"stackoverflow\"\r\n",
							"raw_format = \"parquet\"\r\n",
							"refined_format = \"delta\"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SET spark.databricks.delta.properties.defaults.enableChangeDataFeed = true"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Prep source and target tables"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from delta.tables import *\r\n",
							"\r\n",
							"column_list = columns.split(',')\r\n",
							"print(column_list)\r\n",
							"\r\n",
							"raw_df = spark.read.format(raw_format).load(f\"{raw_base_path}/{table}\")\r\n",
							"df_source = raw_df.selectExpr(*column_list).withColumn(\"is_deleted\", lit(False))\r\n",
							"\r\n",
							"try:\r\n",
							"    delta_target = DeltaTable.forPath(spark, f\"{refined_base_path}/{table}\")\r\n",
							"    df_target = delta_target.toDF()\r\n",
							"except AnalysisException as e:\r\n",
							"    if str(e).find(f\"Path does not exist\") > -1 or str(e).find(f\"is not a Delta table\") > -1:\r\n",
							"        print(f\"Delta table {table} doesn't exist yet, creating table and exiting notebook early.\")\r\n",
							"        df_source.write.format(\"delta\").save(f\"{refined_base_path}/{table}\")\r\n",
							"        mssparkutils.notebook.exit(\"success\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Delete if not in source"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"t = df_target.filter(\"is_deleted == False\")\r\n",
							"df_source_ids = df_source.select(id)\r\n",
							"df_deleted = t.join(df_source_ids, t[id] == df_source_ids[id], \"left_anti\").withColumn(\"is_deleted\", lit(True))\r\n",
							"display(df_deleted)"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Upsert if any column changed"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Upsert to delta target table\r\n",
							"update_dct = {f\"{c}\": f\"s.{c}\" for c in df_target.columns if c != id}\r\n",
							"condition_str = ' or '.join(f\"t.{k} != {v}\" for k,v in update_dct.items())\r\n",
							"\r\n",
							"df_source = df_source.union(df_deleted)\r\n",
							"\r\n",
							"print(condition_str)\r\n",
							"\r\n",
							"delta_target.alias('t') \\\r\n",
							".merge(df_source.alias('s'), f\"t.{id} = s.{id}\") \\\r\n",
							".whenMatchedUpdate(condition=f\"t.{id} = s.{id} and ({condition_str})\", set=update_dct) \\\r\n",
							".whenNotMatchedInsertAll() \\\r\n",
							".execute()"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.notebook.exit(\"success\")"
						],
						"outputs": [],
						"execution_count": 52
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/stackoverflow_streaming')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "stackoverflow"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark32",
					"type": "BigDataPoolReference"
				},
				"targetSparkConfiguration": {
					"referenceName": "sparkConfigurationSmall",
					"type": "SparkConfigurationReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6ec1a5a8-87eb-46b5-923f-7b7e9a2e2ef8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/spark32",
						"name": "spark32",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 10,
					"targetSparkConfiguration": "sparkConfigurationSmall"
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import StructType, StringType\r\n",
							"\r\n",
							"GROUP_ID = \"so_v2\"\r\n",
							"\r\n",
							"source_base_path = \"abfss://demo@dvtrainingadls.dfs.core.windows.net/stackoverflow/\"\r\n",
							"raw_base_path = \"abfss://raw@datakickstartadls.dfs.core.windows.net/stackoverflow/\"\r\n",
							"refined_base_path = \"abfss://refined@datakickstartadls.dfs.core.windows.net/\"\r\n",
							"\r\n",
							"ckpt_path = f\"{raw_base_path}checkpoints/stackoverflow_streaming_{GROUP_ID}\"\r\n",
							"dest_path = f\"{raw_base_path}stack_overflow_streaming/posts_{GROUP_ID}\""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run utils/logging_utils"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"job_name = \"stackoverflow_stream\"\r\n",
							"destination_database_name = \"raw\"\r\n",
							"destination_table_name = \"stackoverflow_streaming\"\r\n",
							"source_table_name = \"stackoverflow_kafka_stream\"\r\n",
							"start_logging(job_name, destination_database_name, destination_table_name, source_table_name)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"post_schema = (\r\n",
							"  StructType()\r\n",
							"    .add('_Id','long')\r\n",
							"    .add('_ParentId','long')\r\n",
							"    .add('_PostTypeId','long')\r\n",
							"    .add('_Score','long')\r\n",
							"    .add('_Tags','string')\r\n",
							"    .add('_Title','string')\r\n",
							"    .add('_ViewCount','long')  \r\n",
							"    .add('_LastActivityDate','timestamp')\r\n",
							"    .add('_LastEditDate','timestamp')\r\n",
							"    .add('_LastEditorDisplayName','string')\r\n",
							"    .add('_LastEditorUserId','long')\r\n",
							"    .add('_OwnerDisplayName','string')\r\n",
							"    .add('_OwnerUserId','long')\r\n",
							"    .add('_AcceptedAnswerId','long')\r\n",
							"    .add('_AnswerCount','long')\r\n",
							"    .add('_Body','string')\r\n",
							"    .add('_ClosedDate','timestamp')\r\n",
							"    .add('_CommentCount','long')\r\n",
							"    .add('_CommunityOwnedDate','timestamp')\r\n",
							"    .add('_ContentLicense','string')\r\n",
							"    .add('_CreationDate','timestamp')\r\n",
							"    .add('_FavoriteCount','long')\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"##Define Variables\r\n",
							"confluentBootstrapServers = \"pkc-41973.westus2.azure.confluent.cloud:9092\"\r\n",
							"confluentApiKey =  mssparkutils.credentials.getSecretWithLS('demokv', 'confluent-cloud-user')\r\n",
							"confluentSecret = mssparkutils.credentials.getSecretWithLS('demokv', 'confluent-cloud-password')\r\n",
							"confluentTopicName = \"stackoverflow_post\"\r\n",
							"\r\n",
							"##Create Spark Readstream\r\n",
							"post_raw_df = (\r\n",
							"  spark\r\n",
							"  .readStream\r\n",
							"  .format(\"kafka\")\r\n",
							"  .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\r\n",
							"  .option(\"kafka.security.protocol\", \"SASL_SSL\")\r\n",
							"  .option(\"kafka.sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(confluentApiKey, confluentSecret))\r\n",
							"  .option(\"kafka.sasl.mechanism\", \"PLAIN\")\r\n",
							"  .option(\"subscribe\", confluentTopicName)\r\n",
							"  .option(\"startingOffsets\", \"earliest\")\r\n",
							"  .option(\"failOnDataLoss\", \"false\")\r\n",
							"  .load()\r\n",
							")\r\n",
							"\r\n",
							"post_cast_df = post_raw_df.select(\r\n",
							"    col('key').cast('string').alias('key'),\r\n",
							"    from_json(col('value').cast('string'), post_schema).alias(\"json\"))\r\n",
							"\r\n",
							"post_df = post_cast_df.selectExpr(\"json.*\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"new_users = spark.read.format(\"delta\").load(f\"{refined_base_path}stackoverflow_new_users\").select(\r\n",
							"    \"user_id\",\r\n",
							"    \"account_id\",\r\n",
							"    \"display_name\",\r\n",
							"    \"website_url\"\r\n",
							")\r\n",
							"\r\n",
							"old_users = spark.read.parquet(f\"{raw_base_path}users\").selectExpr(\r\n",
							"    \"_ID as user_id\",\r\n",
							"    \"_AccountId as account_id\",\r\n",
							"    \"_DisplayName as display_name\",\r\n",
							"    \"_WebsiteUrl as website_url\"\r\n",
							"    )\r\n",
							"\r\n",
							"all_users = old_users.union(new_users)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"combined_df = post_df.join(all_users, post_df[\"_OwnerUserId\"] == all_users[\"user_id\"], how=\"left\")\r\n",
							"log_informational_message(\"Starting stream for combined so posts to delta file.\")\r\n",
							"\r\n",
							"q = combined_df.writeStream.format(\"delta\").option(\"checkpointLocation\",ckpt_path) \\\r\n",
							"    .trigger(processingTime='30 seconds').outputMode(\"append\") \\\r\n",
							"    .start(dest_path)\r\n",
							"\r\n",
							"q.processAllAvailable()\r\n",
							"q.stop()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"test_df = spark.read.format(\"delta\").load(dest_path)\r\n",
							"display(test_df)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"stop_logging(job_name)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sparkConfigurations/sparkConfigurationSmall')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test_python_custom_library')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "test"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "pythoncustom",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "f75b94f4-e8f7-4222-9b4a-3d1f6251b55f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/pythoncustom",
						"name": "pythoncustom",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pythoncustom",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import requests\r\n",
							"import pandas as pd\r\n",
							"import openpyxl\r\n",
							"import json\r\n",
							"\r\n",
							"import requests\r\n",
							"import pytest\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def dataframe_summary(df, col1, col2):\r\n",
							"    summary = df.select(\r\n",
							"        \"count(1) as row_count\",\r\n",
							"        \"min(col2) as min_value\",\r\n",
							"        \"max(col2) as max_value\")\r\n",
							"    result = summary.first()\r\n",
							"    return (result.row_count, result.min_value, result.max_value)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def sample_df(vals):\r\n",
							"    col_names = [\"name\", \"val\"]\r\n",
							"    data = [[\"test \" + str(v), v] for v in vals]\r\n",
							"    df = spark.createDataFrame(data, col_names)\r\n",
							"\r\n",
							"@pytest.mark.parameterize('input_df, result',\r\n",
							"[ ( sample_df([1, 2]), (2, 1, 2) ),\r\n",
							"  ( sample_df([3, 4, 5]), (3, 3, 5))\r\n",
							"])\r\n",
							"def test_dataframe_summary(input_df, result):\r\n",
							"    print(input_df, result)\r\n",
							"    assert result == dataframe_summary(input_df, \"val\")"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# test_dataframe_summary()"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# pytest.main([\"-v\",\"test_dataframe_summary\"])"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# print(__main__.__file__)"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/video_view_producer')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Streaming"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "kafka303",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b9077cdd-cd66-4af9-b23e-42db455fc929"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b99affbe-2256-409d-a682-c20a3963070b/resourceGroups/datakickstart-synapse-rg/providers/Microsoft.Synapse/workspaces/datakickstart-synapse/bigDataPools/kafka303",
						"name": "kafka303",
						"type": "Spark",
						"endpoint": "https://datakickstart-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/kafka303",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import org.apache.kafka.clients.producer._\r\n",
							"import java.sql.Timestamp\r\n",
							"import java.time.Instant\r\n",
							"import java.util.Properties\r\n",
							"\r\n",
							"import scala.util.Random\r\n",
							"import org.apache.kafka.clients.producer._"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"val bootstrapServers = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"confluent-cloud-brokers\")\r\n",
							"val props = new Properties()\r\n",
							"props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\r\n",
							"props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\r\n",
							"\r\n",
							"val kafkaAPIKey = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"confluent-cloud-user\")\r\n",
							"val kafkaAPISecret = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"confluent-cloud-password\")\r\n",
							"props.put(\"bootstrap.servers\", bootstrapServers)\r\n",
							"props.put(\"security.protocol\", \"SASL_SSL\")\r\n",
							"props.put(\"sasl.jaas.config\", s\"org.apache.kafka.common.security.plain.PlainLoginModule  required username='$kafkaAPIKey'   password='$kafkaAPISecret';\")\r\n",
							"props.put(\"sasl.mechanism\", \"PLAIN\")\r\n",
							"// Required for correctness in Apache Kafka clients prior to 2.6\r\n",
							"props.put(\"client.dns.lookup\", \"use_all_dns_ips\")\r\n",
							"// Best practice for Kafka producer to prevent data loss\r\n",
							"props.put(\"acks\", \"all\")\r\n",
							"\r\n",
							"val usageTopic = \"video_usage\"\r\n",
							"val producer = new KafkaProducer[String, String](props)\r\n",
							"\r\n",
							"val randomCompleted = () => { if (Random.nextInt(2) == 1) true else false }\r\n",
							"val randomDuration= () => { Random.nextInt(360) }\r\n",
							"val timestampNow = () => Timestamp.from(Instant.now)\r\n",
							"\r\n",
							"\r\n",
							"// val usageInfo = s\"\"\"{\"usageId\": $usageId, \"user\": \"user${user}\", \"completed\": ${randomCompleted()}, \"durationSeconds\": ${randomDuration()}, \"eventTimestamp\": \"${timestampNow()}\"}\"\"\"\r\n",
							"// val record = new ProducerRecord[String, String](usageTopic, usageId.toString, usageInfo)\r\n",
							"\r\n",
							"// //println(usageInfo)\r\n",
							"// produceRecord(producer, record)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"\r\n",
							"val bootstrapServers = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"confluent-cloud-brokers\")\r\n",
							"val mode: String = \"confluent\"\r\n",
							"\r\n",
							"val props = new Properties()\r\n",
							"props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\r\n",
							"props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\r\n",
							"\r\n",
							"if (mode.toLowerCase == \"local\") {\r\n",
							"    println(\"Running producer in local mode\")\r\n",
							"    props.put(\"bootstrap.servers\", \"localhost:9092\")\r\n",
							"} else {\r\n",
							"    println(\"Running producer in Confluent Cloud mode\")\r\n",
							"    // If using Confluent\r\n",
							"    val kafkaAPIKey = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"confluent-cloud-user\")\r\n",
							"    val kafkaAPISecret = mssparkutils.credentials.getSecretWithLS(\"demokv\", \"confluent-cloud-password\")\r\n",
							"    props.put(\"bootstrap.servers\", bootstrapServers)\r\n",
							"    props.put(\"security.protocol\", \"SASL_SSL\")\r\n",
							"    props.put(\"sasl.jaas.config\", s\"org.apache.kafka.common.security.plain.PlainLoginModule  required username='$kafkaAPIKey'   password='$kafkaAPISecret';\")\r\n",
							"    props.put(\"sasl.mechanism\", \"PLAIN\")\r\n",
							"    // Required for correctness in Apache Kafka clients prior to 2.6\r\n",
							"    props.put(\"client.dns.lookup\", \"use_all_dns_ips\")\r\n",
							"    // Best practice for Kafka producer to prevent data loss\r\n",
							"    props.put(\"acks\", \"all\")\r\n",
							"}\r\n",
							"\r\n",
							"\r\n",
							"def produceSingleVideoUsage(user: Int, usageId: Int): Unit = {\r\n",
							"    val usageTopic = \"video_usage\"\r\n",
							"\r\n",
							"    val producer = new KafkaProducer[String, String](props)\r\n",
							"\r\n",
							"    val randomCompleted = () => { if (Random.nextInt(2) == 1) true else false }\r\n",
							"    val randomDuration= () => { Random.nextInt(360) }\r\n",
							"    val timestampNow = () => Timestamp.from(Instant.now)\r\n",
							"\r\n",
							"    try {\r\n",
							"        val usageInfo = s\"\"\"{\"usageId\": $usageId, \"user\": \"user${user}\", \"completed\": ${randomCompleted()}, \"durationSeconds\": ${randomDuration()}, \"eventTimestamp\": \"${timestampNow()}\"}\"\"\"\r\n",
							"        val record = new ProducerRecord[String, String](usageTopic, usageId.toString, usageInfo)\r\n",
							"\r\n",
							"        //println(usageInfo)\r\n",
							"        produceRecord(producer, record)\r\n",
							"\r\n",
							"    }catch{\r\n",
							"        case e:Exception => e.printStackTrace()\r\n",
							"    }finally {\r\n",
							"        producer.close()\r\n",
							"    }\r\n",
							"}\r\n",
							""
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def produceRecord(producer: KafkaProducer[String, String], record: ProducerRecord[String, String]) = {\r\n",
							"    val metadata = producer.send(record)\r\n",
							"    printf(s\"sent to topic %s: record(key=%s value=%s) \" +\r\n",
							"        \"meta(partition=%d, offset=%d)\\n\",\r\n",
							"        record.topic(), record.key(), record.value(),\r\n",
							"        metadata.get().partition(),\r\n",
							"        metadata.get().offset()\r\n",
							"    )\r\n",
							"}"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def writeUsageWithPlan(): Unit = {\r\n",
							"    val usageTopic = \"video_usage\"\r\n",
							"\r\n",
							"    val producer = new KafkaProducer[String, String](props)\r\n",
							"\r\n",
							"    val randomCompleted = () => {\r\n",
							"        if (Random.nextInt(2) == 1) true else false\r\n",
							"    }\r\n",
							"    val randomUser = () => {\r\n",
							"        Random.nextInt(100)\r\n",
							"    }\r\n",
							"    val randomDuration = () => {\r\n",
							"        Random.nextInt(360)\r\n",
							"    }\r\n",
							"    val timestampNow = () => Timestamp.from(Instant.now)\r\n",
							"    val startInstant = Instant.now\r\n",
							"\r\n",
							"    val maxRecordId = 10000\r\n",
							"    val sleepMilliseconds = 500\r\n",
							"    val randomizeSleep = () => Random.nextInt(9) + 1\r\n",
							"    val pauseThreshold = 0 // at which record should pause start being enforced, 0 means all records\r\n",
							"\r\n",
							"    try {\r\n",
							"        for (i <- 0 to maxRecordId) {\r\n",
							"        val usageInfo = s\"\"\"{\"usageId\": $i, \"user\": \"user${randomUser()}\", \"completed\": ${randomCompleted()}, \"durationSeconds\": ${randomDuration()}, \"eventTimestamp\": \"${timestampNow()}\"}\"\"\"\r\n",
							"        val record = new ProducerRecord[String, String](usageTopic, i.toString, usageInfo)\r\n",
							"\r\n",
							"        println(usageInfo)\r\n",
							"        produceRecord(producer, record)\r\n",
							"\r\n",
							"        if (i > pauseThreshold) {\r\n",
							"            \r\n",
							"            Thread.sleep(sleepMilliseconds / randomizeSleep())\r\n",
							"        }\r\n",
							"        }\r\n",
							"    } catch {\r\n",
							"        case e: Exception => e.printStackTrace()\r\n",
							"    } finally {\r\n",
							"        producer.close()\r\n",
							"    }\r\n",
							"}\r\n",
							"\r\n",
							"\r\n",
							"writeUsageWithPlan()"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkConfigurationSmall')]",
			"type": "Microsoft.Synapse/workspaces/sparkConfigurations",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"configs": {
					"spark.sql.shuffle.partitions": "36"
				},
				"created": "2022-07-28T23:09:53.49Z",
				"createdBy": "training@dustinvannoy.com",
				"annotations": [],
				"configMergeRule": {
					"artifact.currentOperation.spark.sql.shuffle.partitions": "replace"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkLogging')]",
			"type": "Microsoft.Synapse/workspaces/sparkConfigurations",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"configs": {
					"spark.synapse.logAnalytics.enabled": "true",
					"spark.synapse.logAnalytics.workspaceId": "041a7d31-3f9e-4b27-989c-114d98731f26",
					"spark.synapse.logAnalytics.keyVault.name": "dvtrainingkv",
					"spark.synapse.logAnalytics.keyVault.key.secret": "log-synapse-workspace-key",
					"spark.synapse.logAnalytics.keyVault.linkedServiceName": "demokv",
					"spark.synapse.logAnalytics.filter.loggerName.match": "com.datakickstart.datakickstart_synapse,metrics,org.apache.spark.SparkContext,org.apache.spark.sql,com.microsoft.azure.synapse.notebookUtils,com.microsoft.azure.synapse.diagnosticLogger"
				},
				"created": "2022-12-06T09:07:50.8010000-08:00",
				"createdBy": "training@dustinvannoy.com",
				"annotations": [],
				"configMergeRule": {
					"artifact.currentOperation.spark.synapse.logAnalytics.enabled": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.workspaceId": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.keyVault.name": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.keyVault.key.secret": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.keyVault.linkedServiceName": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.filter.loggerName.match": "replace"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkLogging_default')]",
			"type": "Microsoft.Synapse/workspaces/sparkConfigurations",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"configs": {
					"spark.synapse.logAnalytics.enabled": "true",
					"spark.synapse.logAnalytics.workspaceId": "041a7d31-3f9e-4b27-989c-114d98731f26",
					"spark.synapse.logAnalytics.keyVault.name": "dvtrainingkv",
					"spark.synapse.logAnalytics.keyVault.linkedServiceName": "demokv",
					"spark.synapse.logAnalytics.keyVault.key.secret": "log-synapse-workspace-key"
				},
				"created": "2022-12-06T09:00:21.2180000-08:00",
				"createdBy": "training@dustinvannoy.com",
				"annotations": [],
				"configMergeRule": {
					"artifact.currentOperation.spark.synapse.logAnalytics.enabled": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.workspaceId": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.keyVault.name": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.keyVault.linkedServiceName": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.keyVault.key.secret": "replace"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/curated_lakedb')]",
			"type": "Microsoft.Synapse/workspaces/databases",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"Ddls": [
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "curated_lakedb",
							"EntityType": "DATABASE",
							"Origin": {
								"Type": "SPARK"
							},
							"Properties": {
								"IsSyMSCDMDatabase": true
							},
							"Source": {
								"Provider": "ADLS",
								"Location": "abfss://curated@datakickstartadls.dfs.core.windows.net/curated_lakedb",
								"Properties": {
									"FormatType": "parquet",
									"LinkedServiceName": "datakickstart-synapse-WorkspaceDefaultStorage"
								}
							}
						},
						"Source": {
							"Type": "SPARK"
						}
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/demo1')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 0,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "2.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/demo2')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 0,
					"minNodeCount": 0
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "2.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/confluent1')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 5
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"customLibraries": [
					{
						"name": "kafka-clients-3.0.0.jar",
						"path": "datakickstart-synapse/libraries/kafka-clients-3.0.0.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					},
					{
						"name": "commons-pool2-2.11.1.jar",
						"path": "datakickstart-synapse/libraries/commons-pool2-2.11.1.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					},
					{
						"name": "spark-sql-kafka-0-10_2.12-3.0.3.jar",
						"path": "datakickstart-synapse/libraries/spark-sql-kafka-0-10_2.12-3.0.3.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					},
					{
						"name": "spark-token-provider-kafka-0-10_2.12-3.0.3.jar",
						"path": "datakickstart-synapse/libraries/spark-token-provider-kafka-0-10_2.12-3.0.3.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					}
				],
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tobogganCustom')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 5
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"customLibraries": [
					{
						"name": "spark-sql-kafka-0-10_2.12-3.1.2_custom.jar",
						"path": "datakickstart-synapse/libraries/spark-sql-kafka-0-10_2.12-3.1.2_custom.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					},
					{
						"name": "kafka-clients-3.0.0.jar",
						"path": "datakickstart-synapse/libraries/kafka-clients-3.0.0.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					},
					{
						"name": "commons-pool2-2.11.1.jar",
						"path": "datakickstart-synapse/libraries/commons-pool2-2.11.1.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					}
				],
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kafkatest')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 5
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 0,
					"minNodeCount": 0
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"customLibraries": [
					{
						"name": "kafka-clients-3.0.0.jar",
						"path": "datakickstart-synapse/libraries/kafka-clients-3.0.0.jar",
						"containerName": "prep",
						"uploadedTimestamp": "2021-10-08T20:48:20.480877+00:00",
						"type": "jar"
					},
					{
						"name": "commons-pool2-2.11.1.jar",
						"path": "datakickstart-synapse/libraries/commons-pool2-2.11.1.jar",
						"containerName": "prep",
						"uploadedTimestamp": "2021-10-09T23:06:03.9116454+00:00",
						"type": "jar"
					},
					{
						"name": "spark-sql-kafka-0-10_2.12-3.0.3.jar",
						"path": "datakickstart-synapse/libraries/spark-sql-kafka-0-10_2.12-3.0.3.jar",
						"containerName": "prep",
						"uploadedTimestamp": "2021-11-18T06:08:23.3129749+00:00",
						"type": "jar"
					},
					{
						"name": "spark-token-provider-kafka-0-10_2.12-3.0.3.jar",
						"path": "datakickstart-synapse/libraries/spark-token-provider-kafka-0-10_2.12-3.0.3.jar",
						"containerName": "prep",
						"uploadedTimestamp": "2021-11-18T06:23:35.5836051+00:00",
						"type": "jar"
					}
				],
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kafkatest2')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 5
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"customLibraries": [
					{
						"name": "kafka-clients-3.0.0.jar",
						"path": "datakickstart-synapse/libraries/kafka-clients-3.0.0.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					},
					{
						"name": "spark-token-provider-kafka-0-10_2.12-3.0.3.jar",
						"path": "datakickstart-synapse/libraries/spark-token-provider-kafka-0-10_2.12-3.0.3.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					},
					{
						"name": "commons-pool2-2.11.1.jar",
						"path": "datakickstart-synapse/libraries/commons-pool2-2.11.1.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					}
				],
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kafka312')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 10
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"customLibraries": [
					{
						"name": "kafka-clients-3.0.0.jar",
						"path": "datakickstart-synapse/libraries/kafka-clients-3.0.0.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					},
					{
						"name": "commons-pool2-2.11.1.jar",
						"path": "datakickstart-synapse/libraries/commons-pool2-2.11.1.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					},
					{
						"name": "spark-token-provider-kafka-0-10_2.12-3.1.2.jar",
						"path": "datakickstart-synapse/libraries/spark-token-provider-kafka-0-10_2.12-3.1.2.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					},
					{
						"name": "spark-sql-kafka-0-10_2.12-3.1.2.jar",
						"path": "datakickstart-synapse/libraries/spark-sql-kafka-0-10_2.12-3.1.2.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					}
				],
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kafka303')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 5,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pythoncustom')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"libraryRequirements": {
					"content": "name: synapsedemo\r\nchannels:\r\n- defaults\r\ndependencies:\r\n- pip:\r\n  - pytest\r\n  - ipytest\r\n",
					"filename": "synapsedemo.yml",
					"time": "2022-05-13T03:46:00.2250778Z"
				},
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"customLibraries": [],
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pythondefault')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 5
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pythonsession')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 5
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 5,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/demo31')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 5
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 8,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spark31')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sparkConfigProperties": {
					"configurationType": "Artifact",
					"filename": "sparkConfigurationSmall",
					"content": "{\"name\":\"sparkConfigurationSmall\",\"properties\":{\"configs\":{\"spark.sql.shuffle.partitions\":\"36\"},\"annotations\":[],\"type\":\"Microsoft.Synapse/workspaces/sparkconfigurations\",\"description\":\"\",\"notes\":\"\",\"created\":\"2022-07-28T16:09:53.4900000-07:00\",\"createdBy\":\"training@dustinvannoy.com\",\"configMergeRule\":{\"admin.currentOperation.spark.sql.shuffle.partitions\":\"replace\"}}}",
					"time": "2022-07-28T23:11:03.7716492Z"
				},
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spark32')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 10
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 6,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/demo2022')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 6,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sparkConfigProperties": {
					"configurationType": "Artifact",
					"filename": "sparkLogging",
					"content": "{\"name\":\"sparkLogging\",\"properties\":{\"configs\":{\"spark.synapse.logAnalytics.enabled\":\"true\",\"spark.synapse.logAnalytics.secret\":\"vk3dQI1NJU8LCe2NMxdNvuEYWUq38ecVMl0ZHg66NJE/8aliA5FKhmBTzGrrRTOfUDAhcXb/l0HsM70INZ4D+A==\",\"spark.synapse.logAnalytics.workspaceId\":\"041a7d31-3f9e-4b27-989c-114d98731f26\"},\"annotations\":[],\"type\":\"Microsoft.Synapse/workspaces/sparkconfigurations\",\"description\":\"\",\"notes\":\"\",\"created\":\"2022-12-05T09:02:19.1850000-08:00\",\"createdBy\":\"training@dustinvannoy.com\",\"configMergeRule\":{\"admin.currentOperation.spark.synapse.logAnalytics.enabled\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.secret\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.workspaceId\":\"replace\"}}}",
					"time": "2022-12-05T17:04:11.1767221Z"
				},
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkLogging')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sparkConfigProperties": {
					"configurationType": "Artifact",
					"filename": "sparkLogging",
					"content": "{\"name\":\"sparkLogging\",\"properties\":{\"configs\":{\"spark.synapse.logAnalytics.enabled\":\"true\",\"spark.synapse.logAnalytics.workspaceId\":\"041a7d31-3f9e-4b27-989c-114d98731f26\",\"spark.synapse.logAnalytics.keyVault.name\":\"dvtrainingkv\",\"spark.synapse.logAnalytics.keyVault.key.secret\":\"log-synapse-workspace-key\",\"spark.synapse.logAnalytics.keyVault.linkedServiceName\":\"demokv\"},\"annotations\":[],\"type\":\"Microsoft.Synapse/workspaces/sparkconfigurations\",\"description\":\"\",\"notes\":\"\",\"created\":\"2022-12-05T13:40:52.5680000-08:00\",\"createdBy\":\"training@dustinvannoy.com\",\"configMergeRule\":{\"admin.currentOperation.spark.synapse.logAnalytics.enabled\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.workspaceId\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.keyVault.name\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.keyVault.key.secret\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.keyVault.linkedServiceName\":\"replace\"}}}",
					"time": "2022-12-05T21:44:47.6082389Z"
				},
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkLogging2')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 114,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sparkConfigProperties": {
					"configurationType": "Artifact",
					"filename": "sparkLogging_default",
					"content": "{\"name\":\"sparkLogging_default\",\"properties\":{\"configs\":{\"spark.synapse.logAnalytics.enabled\":\"true\",\"spark.synapse.logAnalytics.workspaceId\":\"041a7d31-3f9e-4b27-989c-114d98731f26\",\"spark.synapse.logAnalytics.keyVault.name\":\"dvtrainingkv\",\"spark.synapse.logAnalytics.keyVault.linkedServiceName\":\"demokv\",\"spark.synapse.logAnalytics.keyVault.key.secret\":\"log-synapse-workspace-key\"},\"annotations\":[],\"type\":\"Microsoft.Synapse/workspaces/sparkconfigurations\",\"description\":\"\",\"notes\":\"\",\"created\":\"2022-12-06T09:00:21.2180000-08:00\",\"createdBy\":\"training@dustinvannoy.com\",\"configMergeRule\":{\"admin.currentOperation.spark.synapse.logAnalytics.enabled\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.workspaceId\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.keyVault.name\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.keyVault.linkedServiceName\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.keyVault.key.secret\":\"replace\"}}}",
					"time": "2022-12-06T18:41:58.2658545Z"
				},
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spark33')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 10
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 6,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sparkConfigProperties": {
					"configurationType": "Artifact",
					"filename": "sparkLogging_default",
					"content": "{\"name\":\"sparkLogging_default\",\"properties\":{\"configs\":{\"spark.synapse.logAnalytics.enabled\":\"true\",\"spark.synapse.logAnalytics.workspaceId\":\"041a7d31-3f9e-4b27-989c-114d98731f26\",\"spark.synapse.logAnalytics.keyVault.name\":\"dvtrainingkv\",\"spark.synapse.logAnalytics.keyVault.linkedServiceName\":\"demokv\",\"spark.synapse.logAnalytics.keyVault.key.secret\":\"log-synapse-workspace-key\"},\"annotations\":[],\"type\":\"Microsoft.Synapse/workspaces/sparkconfigurations\",\"description\":\"\",\"notes\":\"\",\"created\":\"2022-12-06T09:00:21.2180000-08:00\",\"createdBy\":\"training@dustinvannoy.com\",\"configMergeRule\":{\"admin.currentOperation.spark.synapse.logAnalytics.enabled\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.workspaceId\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.keyVault.name\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.keyVault.linkedServiceName\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.keyVault.key.secret\":\"replace\"}}}",
					"time": "2023-01-03T22:55:55.4545477Z"
				},
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sql01')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		}
	]
}